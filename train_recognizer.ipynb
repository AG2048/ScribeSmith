{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/aps360/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return -output + 1\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "# preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "320 10\n",
      "images\n",
      "320 10\n",
      "both\n",
      "1000 10\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(1000))\n",
    "line_dataset_val = Subset(line_dataset_val, range(10))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([66, 61,  1,  9, 17,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'to 19 .')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    recognizer_optimizer = optim.SGD(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.CTCLoss(zero_infinity=True)\n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = []\n",
    "    recognizer_train_accuracies = []\n",
    "    recognizer_val_losses = []\n",
    "    recognizer_val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "        recognizer_train_accuracy = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            print(\"\\t\",test)\n",
    "            print(line_text_batch.shape)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            recognizer_outputs = recognizer(line_image_batch)\n",
    "\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            target = torch.cat(line_text_batch_pad_remove)\n",
    "            input_lengths = torch.full(size=(cur_batch_size,), fill_value=500)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                recognizer_outputs.log_softmax(2),\n",
    "                target,\n",
    "                input_lengths,\n",
    "                target_lengths\n",
    "            )\n",
    "            test2 = recognizer_outputs[:,0,:]\n",
    "#             print(test2)\n",
    "            test2 = torch.softmax(test2, dim=1)\n",
    "#             print(test2)\n",
    "            test2 = torch.argmax(test2, dim=1)\n",
    "#             print(test2.shape)\n",
    "            test2 = test2[test2.nonzero()]\n",
    "#             print(test2.shape)\n",
    "#             test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "            print(f\"_{test2}_\")\n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            print(recognizer_loss)\n",
    "            # recognizer_train_loss += recognizer_loss\n",
    "        \n",
    "        recognizer_val_loss = 9999\n",
    "\n",
    "        print(f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0634,  0.0393, -0.0383,  ..., -0.0321,  0.0528,  0.0113],\n",
       "         [-0.0735,  0.0304, -0.0225,  ..., -0.0644,  0.0662, -0.0230],\n",
       "         [-0.0442,  0.0100, -0.0688,  ..., -0.0404,  0.0456, -0.0019],\n",
       "         ...,\n",
       "         [-0.0489,  0.0226, -0.0125,  ..., -0.0618,  0.0552, -0.0498],\n",
       "         [-0.0400,  0.0247, -0.0554,  ..., -0.0598,  0.0721, -0.0155],\n",
       "         [-0.0345, -0.0007, -0.0204,  ..., -0.0154,  0.0455, -0.0134]],\n",
       "\n",
       "        [[-0.0671,  0.0271, -0.0180,  ..., -0.0320,  0.0501, -0.0078],\n",
       "         [-0.0554,  0.0157,  0.0024,  ..., -0.0561,  0.0592, -0.0064],\n",
       "         [-0.0723,  0.0098, -0.0688,  ..., -0.0438,  0.0542, -0.0177],\n",
       "         ...,\n",
       "         [-0.0213,  0.0278, -0.0138,  ..., -0.0721,  0.0626, -0.0658],\n",
       "         [-0.0355,  0.0366, -0.0459,  ..., -0.0873,  0.0642, -0.0079],\n",
       "         [-0.0188,  0.0007, -0.0301,  ..., -0.0190,  0.0467, -0.0223]],\n",
       "\n",
       "        [[-0.0908,  0.0462, -0.0127,  ..., -0.0337,  0.0644, -0.0199],\n",
       "         [-0.0429,  0.0279,  0.0045,  ..., -0.0795,  0.0647, -0.0204],\n",
       "         [-0.0648,  0.0028, -0.0683,  ..., -0.0376,  0.0370, -0.0391],\n",
       "         ...,\n",
       "         [-0.0139, -0.0012, -0.0168,  ..., -0.0895,  0.0491, -0.0426],\n",
       "         [-0.0018,  0.0168, -0.0456,  ..., -0.0741,  0.0462, -0.0413],\n",
       "         [-0.0253, -0.0003, -0.0502,  ..., -0.0175,  0.0348, -0.0332]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0362,  0.0230, -0.0423,  ..., -0.0286,  0.0565, -0.0234],\n",
       "         [-0.0817,  0.0468, -0.0054,  ..., -0.0553,  0.0755, -0.0230],\n",
       "         [-0.0508,  0.0125, -0.0712,  ..., -0.0474,  0.0683, -0.0742],\n",
       "         ...,\n",
       "         [-0.0436, -0.0034, -0.0321,  ..., -0.0609,  0.0454, -0.0629],\n",
       "         [-0.0454,  0.0350, -0.0454,  ..., -0.0719,  0.0567, -0.0184],\n",
       "         [-0.0614, -0.0168, -0.0500,  ..., -0.0348,  0.0469, -0.0170]],\n",
       "\n",
       "        [[-0.0591,  0.0416, -0.0472,  ..., -0.0239,  0.0644, -0.0229],\n",
       "         [-0.0735,  0.0354, -0.0166,  ..., -0.0502,  0.0659, -0.0125],\n",
       "         [-0.0281,  0.0043, -0.0596,  ..., -0.0666,  0.0666, -0.0673],\n",
       "         ...,\n",
       "         [-0.0544,  0.0065, -0.0428,  ..., -0.0448,  0.0437, -0.0240],\n",
       "         [-0.0419,  0.0111, -0.0470,  ..., -0.0515,  0.0611, -0.0280],\n",
       "         [-0.0325,  0.0010, -0.0510,  ..., -0.0450,  0.0482, -0.0257]],\n",
       "\n",
       "        [[-0.0676,  0.0560, -0.0353,  ..., -0.0326,  0.0692, -0.0463],\n",
       "         [-0.0654,  0.0287, -0.0281,  ..., -0.0414,  0.0637, -0.0111],\n",
       "         [-0.0341,  0.0124, -0.0477,  ..., -0.0732,  0.0835, -0.0220],\n",
       "         ...,\n",
       "         [-0.0649,  0.0098, -0.0392,  ..., -0.0523,  0.0369, -0.0445],\n",
       "         [-0.0395,  0.0135, -0.0364,  ..., -0.0589,  0.0814, -0.0221],\n",
       "         [-0.0451, -0.0038, -0.0461,  ..., -0.0326,  0.0667, -0.0369]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        img, _ = self.lstm(img)\n",
    "        img = self.dense(img)\n",
    "        img = img.permute(1, 0, 2)\n",
    "        # print(img.shape)\n",
    "        return img\n",
    "        print(img.shape)\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "        \n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((32, 1, 32, 512), dtype=torch.float32))\n",
    "# print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\t Let us consider first what would\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aa_\n",
      "tensor(51.7976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t There have been many conflicting tendencies in\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aa_\n",
      "tensor(49.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t season moves into top gear .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aa_\n",
      "tensor(52.2450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t of \" Abou Ben Adhem . \"\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aaa_\n",
      "tensor(54.2269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t a nostalgic affection for them .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aaa_\n",
      "tensor(59.8412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t construction in lines 9 , 23-25 of the Moabite Stone ,\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aaaaa_\n",
      "tensor(50.4582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t But he said discussions \" on a higher\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,,,,aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_\n",
      "tensor(50.2417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t been attending some kind of fancy dress do\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,,aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_\n",
      "tensor(60.6039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t John eyed him steadily . \" It 's the Parcifal\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "_,aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_\n",
      "tensor(56.5201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t the frame with panel pins to complete\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(59.4916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t school was held .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(58.7954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t These support costs are a big drain on America's\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(50.8806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t laid his hand on hers . Gay smiled at him\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(45.6505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t whatever you say . \" \" I was afraid of\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(49.1707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t We must learn all we can about\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(46.7545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t and has made it live . The shabby streets\n",
      "torch.Size([40, 82])\n",
      "torch.Size([500, 40, 73])\n",
      "__\n",
      "tensor(50.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 0\n",
      "\t logical Office .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(49.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t whatever you say . \" \" I was afraid of\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(45.8902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t in Canadian leadership , or the practical\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(43.9918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t forty and had written Hamlet two years\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(40.3593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t made public .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(42.6345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t I 'd marry you myself . \" Gay laughed , Doc was\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(39.2315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t year or two later as ' more like a God\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(47.3825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t the neuro-physiologists .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(39.7424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t ciency and , in the case of Mr. Richards , with\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(31.8894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t Ultratoryism , that the Commons House upon\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(29.4300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t Mr. Macleod went on with the conference at\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(29.0458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t will return .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(25.8908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t Anglesey a strong , energetic man with a\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(25.3003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t sunshine , still burning like a half-cooled iron ,\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(20.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t chief aide , Mr. Julius Greenfield , telephoned\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(18.6745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t studied under its founder , the aged\n",
      "torch.Size([40, 82])\n",
      "torch.Size([500, 40, 73])\n",
      "__\n",
      "tensor(13.7077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 1\n",
      "\t the dissections of living animals , Harvey noted that\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(11.7853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t tied . He dare not precipitate\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n",
      "__\n",
      "tensor(9.2196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\t I followed , seeing how the sun gilded\n",
      "torch.Size([64, 82])\n",
      "torch.Size([500, 64, 73])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m recognizer \u001b[39m=\u001b[39m Recognizer()\n\u001b[1;32m      2\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(recognizer\u001b[39m=\u001b[39mrecognizer, \n\u001b[1;32m      6\u001b[0m               train_line_dataset\u001b[39m=\u001b[39mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39mline_dataset_val, \n\u001b[1;32m      7\u001b[0m               batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, recognizer_lr\u001b[39m=\u001b[39m\u001b[39m0.0004\u001b[39m,\n\u001b[1;32m      8\u001b[0m               betas\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0.999\u001b[39m), num_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, loss_balancing_alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(recognizer, train_line_dataset, val_line_dataset, batch_size, recognizer_lr, betas, num_epochs, loss_balancing_alpha)\u001b[0m\n\u001b[1;32m     44\u001b[0m             recognizer_outputs \u001b[39m=\u001b[39m recognizer(line_image_batch)\n\u001b[1;32m     46\u001b[0m \u001b[39m#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m#             Refer to CTC documentation\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m             line_text_batch_pad_remove \u001b[39m=\u001b[39m [line_text[line_text\u001b[39m.\u001b[39mnonzero()\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)] \u001b[39mfor\u001b[39;00m line_text \u001b[39min\u001b[39;00m line_text_batch]  \u001b[39m# Array of tensors\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             target_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mlen\u001b[39m(line_text_pad_remove) \u001b[39mfor\u001b[39;00m line_text_pad_remove \u001b[39min\u001b[39;00m line_text_batch_pad_remove])\n\u001b[1;32m     51\u001b[0m             target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(line_text_batch_pad_remove)\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m             recognizer_outputs \u001b[39m=\u001b[39m recognizer(line_image_batch)\n\u001b[1;32m     46\u001b[0m \u001b[39m#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m#             Refer to CTC documentation\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m             line_text_batch_pad_remove \u001b[39m=\u001b[39m [line_text[line_text\u001b[39m.\u001b[39mnonzero()\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)] \u001b[39mfor\u001b[39;00m line_text \u001b[39min\u001b[39;00m line_text_batch]  \u001b[39m# Array of tensors\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             target_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mlen\u001b[39m(line_text_pad_remove) \u001b[39mfor\u001b[39;00m line_text_pad_remove \u001b[39min\u001b[39;00m line_text_batch_pad_remove])\n\u001b[1;32m     51\u001b[0m             target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(line_text_batch_pad_remove)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=64, recognizer_lr=0.0004,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
