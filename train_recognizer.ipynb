{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 7135\n"
     ]
    }
   ],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "320 10\n",
      "images\n",
      "320 10\n",
      "both\n",
      "128 128\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(64*2))\n",
    "line_dataset_val = Subset(line_dataset_val, range(64*2))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([66, 61,  1,  9, 17,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'to 19 .')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    #print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #print(len(train_line_dataset_loader))\n",
    "\n",
    "    recognizer_optimizer = optim.SGD(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.MSELoss()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = []\n",
    "    recognizer_train_accuracies = []\n",
    "    recognizer_val_losses = []\n",
    "    recognizer_val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "        recognizer_train_accuracy = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            #test = line_text_batch[0]\n",
    "            #test = test[test.nonzero()]\n",
    "            #test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            #print(\"\\t\",test)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            #print(line_text_batch, line_text_batch.shape)\n",
    "            recognizer_outputs = recognizer(line_image_batch)\n",
    "   \n",
    "            print(recognizer_outputs, recognizer_outputs.shape)\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            #line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            #target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            #target = torch.cat(line_text_batch_pad_remove)\n",
    "            #print(target, target.shape)\n",
    "            #input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\n",
    "                line_text_batch\n",
    "            )\n",
    "            test2 = recognizer_outputs[:,0,:]\n",
    "#             print(test2)\n",
    "            test2 = torch.softmax(test2, dim=1)\n",
    "#             print(test2)\n",
    "            test2 = torch.argmax(test2, dim=1)\n",
    "#             print(test2.shape)\n",
    "            test2 = test2[test2.nonzero()]\n",
    "#             print(test2.shape)\n",
    "#             test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "            print(f\"_{test2}_\")\n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            print(recognizer_loss)\n",
    "            # recognizer_train_loss += recognizer_loss\n",
    "        \n",
    "        recognizer_val_loss = 9999\n",
    "\n",
    "        print(f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizer(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(64, 128, kernel_size=(4, 2), stride=(1, 1))\n",
      "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dense): Linear(in_features=256, out_features=73, bias=True)\n",
      "  (dense2): Linear(in_features=248, out_features=82, bias=True)\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7299e-02, -1.6552e-02, -1.6864e-02,  ...,  5.4516e-02,\n",
       "          -5.5020e-02, -6.0381e-02],\n",
       "         [ 4.3431e-02, -8.4052e-03, -6.1646e-03,  ...,  5.2120e-02,\n",
       "          -1.3030e-02, -5.7027e-02],\n",
       "         [ 4.8663e-02, -1.5826e-02, -1.4545e-02,  ...,  5.1659e-02,\n",
       "          -5.5724e-02, -5.7337e-02],\n",
       "         ...,\n",
       "         [ 4.8484e-02, -1.5902e-02, -1.4652e-02,  ...,  5.1532e-02,\n",
       "          -5.5847e-02, -5.7430e-02],\n",
       "         [ 3.8555e-02, -1.9690e-02,  1.7753e-03,  ...,  4.6241e-02,\n",
       "           2.3985e-02, -3.0803e-02],\n",
       "         [ 4.8922e-02, -1.4866e-02, -1.4678e-02,  ...,  5.1658e-02,\n",
       "          -5.5828e-02, -5.6138e-02]],\n",
       "\n",
       "        [[ 4.1466e-02, -1.8527e-02, -1.3138e-02,  ...,  4.9486e-02,\n",
       "          -6.1614e-02, -6.0632e-02],\n",
       "         [ 4.5429e-02, -1.4695e-02, -7.1653e-03,  ...,  5.2039e-02,\n",
       "          -4.5475e-02, -5.8978e-02],\n",
       "         [ 4.8719e-02, -1.5666e-02, -1.4372e-02,  ...,  5.1740e-02,\n",
       "          -5.6043e-02, -5.7363e-02],\n",
       "         ...,\n",
       "         [ 4.8583e-02, -1.5916e-02, -1.4512e-02,  ...,  5.1663e-02,\n",
       "          -5.6273e-02, -5.7552e-02],\n",
       "         [ 4.6232e-02, -8.9491e-03,  1.9446e-03,  ...,  3.3650e-02,\n",
       "          -2.3020e-03, -5.3859e-02],\n",
       "         [ 4.5378e-02, -1.4774e-02, -1.2948e-02,  ...,  5.1230e-02,\n",
       "          -4.9421e-02, -5.8019e-02]],\n",
       "\n",
       "        [[ 4.7844e-02, -1.6388e-02, -1.5313e-02,  ...,  5.2042e-02,\n",
       "          -5.4806e-02, -5.7134e-02],\n",
       "         [ 5.4776e-02, -2.7921e-02, -9.7144e-03,  ...,  4.4038e-02,\n",
       "          -4.1952e-02, -5.5123e-02],\n",
       "         [ 4.8424e-02, -1.5391e-02, -1.4228e-02,  ...,  5.2892e-02,\n",
       "          -5.7857e-02, -5.5282e-02],\n",
       "         ...,\n",
       "         [ 4.3557e-02, -1.4552e-02, -9.0113e-03,  ...,  5.1684e-02,\n",
       "          -5.1869e-02, -5.5476e-02],\n",
       "         [ 5.3902e-02, -1.2266e-02, -5.5187e-03,  ...,  5.5164e-02,\n",
       "          -1.9624e-03, -4.7492e-02],\n",
       "         [ 5.1182e-02, -2.2503e-02, -1.8720e-02,  ...,  5.5191e-02,\n",
       "          -6.4259e-02, -6.1419e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.8962e-02, -1.4842e-02, -1.5530e-02,  ...,  5.1277e-02,\n",
       "          -5.4955e-02, -5.7939e-02],\n",
       "         [ 5.5221e-02, -1.6592e-02, -1.0114e-02,  ...,  5.2793e-02,\n",
       "          -3.3167e-02, -4.9768e-02],\n",
       "         [ 4.8796e-02, -1.5663e-02, -1.4833e-02,  ...,  5.1772e-02,\n",
       "          -5.6141e-02, -5.7288e-02],\n",
       "         ...,\n",
       "         [ 5.1108e-02, -1.3498e-02, -1.4016e-02,  ...,  5.1654e-02,\n",
       "          -5.6507e-02, -5.7127e-02],\n",
       "         [ 4.3098e-02, -1.7536e-02, -1.7829e-05,  ...,  3.5166e-02,\n",
       "           3.9755e-02, -5.0345e-02],\n",
       "         [ 4.8204e-02, -1.5447e-02, -1.2573e-02,  ...,  5.4238e-02,\n",
       "          -5.6133e-02, -5.5672e-02]],\n",
       "\n",
       "        [[ 4.6641e-02, -1.7584e-02, -1.3238e-02,  ...,  4.8954e-02,\n",
       "          -4.9186e-02, -5.8683e-02],\n",
       "         [ 3.0776e-02, -1.7616e-02, -2.6451e-02,  ...,  5.1751e-02,\n",
       "          -4.8031e-02, -6.1029e-02],\n",
       "         [ 4.8028e-02, -1.8510e-02, -1.3577e-02,  ...,  5.1479e-02,\n",
       "          -5.5691e-02, -6.2749e-02],\n",
       "         ...,\n",
       "         [ 4.8710e-02, -1.5952e-02, -1.4627e-02,  ...,  5.1476e-02,\n",
       "          -5.6049e-02, -5.7320e-02],\n",
       "         [ 4.8268e-02, -1.7734e-02, -1.3445e-02,  ...,  4.0936e-02,\n",
       "          -4.0052e-02, -4.9431e-02],\n",
       "         [ 4.7426e-02, -1.3417e-02, -1.4871e-02,  ...,  5.3142e-02,\n",
       "          -5.2682e-02, -5.7792e-02]],\n",
       "\n",
       "        [[ 4.8716e-02, -1.5952e-02, -1.4563e-02,  ...,  5.1565e-02,\n",
       "          -5.6151e-02, -5.7122e-02],\n",
       "         [ 4.2027e-02, -1.0925e-02, -1.2598e-03,  ...,  3.6326e-02,\n",
       "          -2.2553e-02, -5.7450e-02],\n",
       "         [ 4.8825e-02, -1.6142e-02, -1.4504e-02,  ...,  5.1789e-02,\n",
       "          -5.5612e-02, -5.7765e-02],\n",
       "         ...,\n",
       "         [ 4.8568e-02, -1.5927e-02, -1.4643e-02,  ...,  5.1540e-02,\n",
       "          -5.6214e-02, -5.7456e-02],\n",
       "         [ 5.7975e-02,  6.6090e-03,  6.3978e-03,  ...,  5.6654e-02,\n",
       "          -7.1119e-03, -5.1305e-02],\n",
       "         [ 5.9136e-02, -1.9967e-02, -1.6769e-02,  ...,  4.7600e-02,\n",
       "          -5.6354e-02, -5.1503e-02]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(248, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        #print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.dense2(img).squeeze(0)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "        \n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((32, 1, 32, 512), dtype=torch.float32))\n",
    "print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0455, -0.0350,  0.0592,  ..., -0.0254,  0.0453, -0.0394],\n",
      "         [ 0.0434, -0.0559,  0.0716,  ..., -0.0700,  0.0534, -0.0680],\n",
      "         [ 0.0450, -0.0385,  0.0574,  ..., -0.0250,  0.0310, -0.0329],\n",
      "         ...,\n",
      "         [ 0.0516, -0.0337,  0.0787,  ..., -0.0386,  0.0470, -0.0454],\n",
      "         [ 0.0449, -0.0388,  0.0566,  ..., -0.0264,  0.0313, -0.0330],\n",
      "         [ 0.0399, -0.0379,  0.0615,  ..., -0.0302,  0.0323, -0.0409]],\n",
      "\n",
      "        [[ 0.0238, -0.0398,  0.0640,  ..., -0.0688,  0.0356, -0.0702],\n",
      "         [ 0.0501, -0.0505,  0.0517,  ..., -0.0575,  0.0267, -0.0372],\n",
      "         [ 0.0450, -0.0385,  0.0578,  ..., -0.0256,  0.0308, -0.0333],\n",
      "         ...,\n",
      "         [ 0.0340, -0.0489,  0.0702,  ..., -0.0588,  0.0527, -0.0634],\n",
      "         [ 0.0445, -0.0391,  0.0572,  ..., -0.0258,  0.0316, -0.0338],\n",
      "         [ 0.0571, -0.0313,  0.0492,  ..., -0.0353,  0.0349, -0.0501]],\n",
      "\n",
      "        [[ 0.0390, -0.0530,  0.0835,  ..., -0.0867,  0.0377, -0.0479],\n",
      "         [ 0.0384, -0.0381,  0.0519,  ..., -0.0608,  0.0465, -0.0686],\n",
      "         [ 0.0449, -0.0383,  0.0575,  ..., -0.0256,  0.0311, -0.0330],\n",
      "         ...,\n",
      "         [ 0.0510, -0.0441,  0.0698,  ..., -0.0451,  0.0418, -0.0493],\n",
      "         [ 0.0439, -0.0394,  0.0566,  ..., -0.0249,  0.0339, -0.0337],\n",
      "         [ 0.0326, -0.0443,  0.0454,  ..., -0.0321,  0.0277, -0.0464]],\n",
      "\n",
      "        [[ 0.0551, -0.0648,  0.0832,  ..., -0.0699,  0.0490, -0.0759],\n",
      "         [ 0.0422, -0.0502,  0.0626,  ..., -0.0748,  0.0563, -0.0712],\n",
      "         [ 0.0450, -0.0386,  0.0574,  ..., -0.0258,  0.0313, -0.0334],\n",
      "         ...,\n",
      "         [ 0.0508, -0.0299,  0.0750,  ..., -0.0322,  0.0232, -0.0411],\n",
      "         [ 0.0439, -0.0366,  0.0584,  ..., -0.0271,  0.0325, -0.0318],\n",
      "         [ 0.0343, -0.0577,  0.0538,  ..., -0.0545,  0.0401, -0.0496]]],\n",
      "       grad_fn=<SqueezeBackward1>) torch.Size([4, 73, 82])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"mse_cpu\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m recognizer \u001b[39m=\u001b[39m Recognizer()\n\u001b[1;32m      2\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(recognizer\u001b[39m=\u001b[39;49mrecognizer, \n\u001b[1;32m      6\u001b[0m               train_line_dataset\u001b[39m=\u001b[39;49mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39;49mline_dataset_val, \n\u001b[1;32m      7\u001b[0m               batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, recognizer_lr\u001b[39m=\u001b[39;49m\u001b[39m5e-9\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m               betas\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0.999\u001b[39;49m), num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, loss_balancing_alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[163], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(recognizer, train_line_dataset, val_line_dataset, batch_size, recognizer_lr, betas, num_epochs, loss_balancing_alpha)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[39mprint\u001b[39m(recognizer_outputs, recognizer_outputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     50\u001b[0m \u001b[39m#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[39m#             Refer to CTC documentation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[39m#print(target, target.shape)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m             \u001b[39m#input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m             recognizer_loss \u001b[39m=\u001b[39m recognizer_loss_function(\n\u001b[1;32m     59\u001b[0m                 torch\u001b[39m.\u001b[39;49margmax(F\u001b[39m.\u001b[39;49mlog_softmax(recognizer_outputs, \u001b[39m2\u001b[39;49m), \u001b[39m1\u001b[39;49m),\n\u001b[1;32m     60\u001b[0m                 line_text_batch\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m             test2 \u001b[39m=\u001b[39m recognizer_outputs[:,\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     63\u001b[0m \u001b[39m#             print(test2)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3295\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m-> 3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"mse_cpu\" not implemented for 'Long'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=4, recognizer_lr=5e-9,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n",
      "tensor([[0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0139, 0.0137],\n",
      "        [0.0138, 0.0138, 0.0138,  ..., 0.0138, 0.0134, 0.0138],\n",
      "        [0.0137, 0.0138, 0.0137,  ..., 0.0137, 0.0135, 0.0137],\n",
      "        ...,\n",
      "        [0.0140, 0.0140, 0.0140,  ..., 0.0139, 0.0135, 0.0139],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0138, 0.0139, 0.0137],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0138, 0.0137]],\n",
      "       grad_fn=<SoftmaxBackward0>) torch.Size([82, 73])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "\n",
    "print(torch.softmax(recognizer(image.unsqueeze(0)), 1), torch.softmax(recognizer(image.unsqueeze(0)), 1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
