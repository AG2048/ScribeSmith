{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 7135\n"
     ]
    }
   ],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "320 10\n",
      "images\n",
      "320 10\n",
      "both\n",
      "1 10\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(1))\n",
    "line_dataset_val = Subset(line_dataset_val, range(10))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([66, 61,  1,  9, 17,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'to 19 .')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    recognizer_optimizer = optim.SGD(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.CTCLoss(zero_infinity=True)\n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = []\n",
    "    recognizer_train_accuracies = []\n",
    "    recognizer_val_losses = []\n",
    "    recognizer_val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "        recognizer_train_accuracy = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            print(\"\\t\",test)\n",
    "            print(line_text_batch.shape)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            recognizer_outputs = recognizer(line_image_batch)\n",
    "\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            target = torch.cat(line_text_batch_pad_remove)\n",
    "            input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                recognizer_outputs.log_softmax(2),\n",
    "                target,\n",
    "                input_lengths,\n",
    "                target_lengths\n",
    "            )\n",
    "            test2 = recognizer_outputs[:,0,:]\n",
    "#             print(test2)\n",
    "            test2 = torch.softmax(test2, dim=1)\n",
    "#             print(test2)\n",
    "            test2 = torch.argmax(test2, dim=1)\n",
    "#             print(test2.shape)\n",
    "            test2 = test2[test2.nonzero()]\n",
    "#             print(test2.shape)\n",
    "#             test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "            print(f\"_{test2}_\")\n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            print(recognizer_loss)\n",
    "            # recognizer_train_loss += recognizer_loss\n",
    "        \n",
    "        recognizer_val_loss = 9999\n",
    "\n",
    "        print(f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 15, 255])\n",
      "torch.Size([32, 16, 13, 253])\n",
      "torch.Size([32, 32, 11, 251])\n",
      "torch.Size([32, 64, 9, 249])\n",
      "torch.Size([32, 128, 6, 248])\n",
      "torch.Size([32, 128, 248])\n",
      "torch.Size([32, 248, 128])\n",
      "torch.Size([32, 248, 256])\n",
      "torch.Size([32, 248, 73])\n",
      "torch.Size([248, 32, 73])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0634,  0.0393, -0.0383,  ..., -0.0321,  0.0528,  0.0113],\n",
       "         [-0.0735,  0.0304, -0.0225,  ..., -0.0644,  0.0662, -0.0230],\n",
       "         [-0.0442,  0.0100, -0.0688,  ..., -0.0404,  0.0456, -0.0019],\n",
       "         ...,\n",
       "         [-0.0489,  0.0226, -0.0125,  ..., -0.0618,  0.0552, -0.0498],\n",
       "         [-0.0400,  0.0247, -0.0554,  ..., -0.0598,  0.0721, -0.0155],\n",
       "         [-0.0345, -0.0007, -0.0204,  ..., -0.0154,  0.0455, -0.0134]],\n",
       "\n",
       "        [[-0.0671,  0.0271, -0.0180,  ..., -0.0320,  0.0501, -0.0078],\n",
       "         [-0.0554,  0.0157,  0.0024,  ..., -0.0561,  0.0592, -0.0064],\n",
       "         [-0.0723,  0.0098, -0.0688,  ..., -0.0438,  0.0542, -0.0177],\n",
       "         ...,\n",
       "         [-0.0213,  0.0278, -0.0138,  ..., -0.0721,  0.0626, -0.0658],\n",
       "         [-0.0355,  0.0366, -0.0459,  ..., -0.0873,  0.0642, -0.0079],\n",
       "         [-0.0188,  0.0007, -0.0301,  ..., -0.0190,  0.0467, -0.0223]],\n",
       "\n",
       "        [[-0.0908,  0.0462, -0.0127,  ..., -0.0337,  0.0644, -0.0199],\n",
       "         [-0.0429,  0.0279,  0.0045,  ..., -0.0795,  0.0647, -0.0204],\n",
       "         [-0.0648,  0.0028, -0.0683,  ..., -0.0376,  0.0370, -0.0391],\n",
       "         ...,\n",
       "         [-0.0139, -0.0012, -0.0168,  ..., -0.0895,  0.0491, -0.0426],\n",
       "         [-0.0018,  0.0168, -0.0456,  ..., -0.0741,  0.0462, -0.0413],\n",
       "         [-0.0253, -0.0003, -0.0502,  ..., -0.0175,  0.0348, -0.0332]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0362,  0.0230, -0.0423,  ..., -0.0286,  0.0565, -0.0234],\n",
       "         [-0.0817,  0.0468, -0.0054,  ..., -0.0553,  0.0755, -0.0230],\n",
       "         [-0.0508,  0.0125, -0.0712,  ..., -0.0474,  0.0683, -0.0742],\n",
       "         ...,\n",
       "         [-0.0436, -0.0034, -0.0321,  ..., -0.0609,  0.0454, -0.0629],\n",
       "         [-0.0454,  0.0350, -0.0454,  ..., -0.0719,  0.0567, -0.0184],\n",
       "         [-0.0614, -0.0168, -0.0500,  ..., -0.0348,  0.0469, -0.0170]],\n",
       "\n",
       "        [[-0.0591,  0.0416, -0.0472,  ..., -0.0239,  0.0644, -0.0229],\n",
       "         [-0.0735,  0.0354, -0.0166,  ..., -0.0502,  0.0659, -0.0125],\n",
       "         [-0.0281,  0.0043, -0.0596,  ..., -0.0666,  0.0666, -0.0673],\n",
       "         ...,\n",
       "         [-0.0544,  0.0065, -0.0428,  ..., -0.0448,  0.0437, -0.0240],\n",
       "         [-0.0419,  0.0111, -0.0470,  ..., -0.0515,  0.0611, -0.0280],\n",
       "         [-0.0325,  0.0010, -0.0510,  ..., -0.0450,  0.0482, -0.0257]],\n",
       "\n",
       "        [[-0.0676,  0.0560, -0.0353,  ..., -0.0326,  0.0692, -0.0463],\n",
       "         [-0.0654,  0.0287, -0.0281,  ..., -0.0414,  0.0637, -0.0111],\n",
       "         [-0.0341,  0.0124, -0.0477,  ..., -0.0732,  0.0835, -0.0220],\n",
       "         ...,\n",
       "         [-0.0649,  0.0098, -0.0392,  ..., -0.0523,  0.0369, -0.0445],\n",
       "         [-0.0395,  0.0135, -0.0364,  ..., -0.0589,  0.0814, -0.0221],\n",
       "         [-0.0451, -0.0038, -0.0461,  ..., -0.0326,  0.0667, -0.0369]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        print(img.shape)\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        print(img.shape)\n",
    "        img = self.dense(img)\n",
    "        print(img.shape)\n",
    "        img = img.permute(1, 0, 2)\n",
    "        print(img.shape)\n",
    "        # print(img.shape)\n",
    "        return img\n",
    "        print(img.shape)\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "        \n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((32, 1, 32, 512), dtype=torch.float32))\n",
    "# print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_\"55555555\"\"o\"\"oooooooooooooI558\"\"o55\"\"\"oooooM\"\"\"\"\"oooo\"\"\"M\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"ooo\"\"\"\"\"\"\"ooo\"oo\"\"\"o\"\"\"\"\"\"\"\"\"\"\"M\"\"oo\"\"Moooooo\"o\"\"\"\"\"o\"\"\"\"\"o\"\"\"o\"o\"\"o\"\"\"\"\"\"\"o\"\"\"\"o\"\"\"\"\"o\"o\"\"\"\"\"\"oo\"\"\"\"\"oo\"\"M\"o\"Mo\"\"\"\"ooo\"oooo\"\"\"\"\"\"\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"\"\"o\"\"\"\"\"\"ooo\"\"\"\"_\n",
      "tensor(143.7606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 0\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_8ooo2ooo\"\"o\"\"\"\"\"\"\"\"\"\"ooo\"o8oooo\"\"\"L\"cooooo\"\"ooo\"\"o\"ooo\"\"\"\"oo\"oMMM\"oo\"oo\"\"\"\"\"\"\"\"\"oMo\"\"o\"o\"ooooM\"\"\"\"\"\"Moo\"\"\"\"o\"\"o\"\"\"\"\"\"\"\"\"o\"\"o\"\"\"o\"o\"\"\"\"\"oo\"\"oo\"\"\"\"M\"\"\"ooo\"\"\"oMoMM\"\"\"\"oM\"\"\"oo\"\"\"\"\"oooMoMoooo\"\"oooooo\"\"o\"o\"Mo\"\"oo\"\"\"\"\"o\"o\"\"\"\"\"oo\"\"\"\"\"\"oooo\"\"o\"\"\"\"\"\"\"Moooooo_\n",
      "tensor(143.7937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 1\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_5\"\"55\"\"o\"o\"o\"\"\"\"\"M\"ooo8\"88\"o555\"\"555\"\"\"\"\"\"\"Moo\"\"oooo\"\"o\"\"o\"\"\"\"\"\"ooMooo\"\"\"\"\"\"\"\"\"\"\"o\"oo\"\"oooooo\"o\"M\"o\"\"\"\"\"M\"\"\"\"ooo\"ooo\"ooooooooooo\"o\"\"\"\"ooo\"\"\"\"\"\"\"\"\"\"ooo\"o\"oo\"\"\"oooo\"oo\"oooooo\"\"\"\"\"\"\"\"\"oooooo\"oooooo\"\"\"\"\"\"oooooo\"o\"\"oooooo\"\"\"o\"\"oo\"\"\"\"\"oooooooo\"oo\"\"\"ooooo_\n",
      "tensor(143.7398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 2\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooo5joooooooo\"o\"\"\"\"\"oooooMoooo\"\"\"\"\"o\"\"\"\"o\"\"\"\"\"o\"\"\"\"\"\"o\"o\"M\"\"\"\"ooo\"o\"\"\"\"\"\"\"\"\"\"\"\"\"\"ooooo\"ooooooo\"\"o\"\"\"\"ooooo\"\"\"\"o\"o\"oooooooooooooooooo\"\"oooooooo\"\"\"\"ooo\"o\"o\"\"\"\"\"\"\"ooo\"o\"o\"\"oooooooo\"\"\"\"\"\"\"\"\"o\"\"\"\"\"ooooo\"o\"\"\"\"\"\"\"\"oo\"ooooo\"\"\"\"o\"\"o\"\"\"oo\"ooooooo\"\"\"o\"oo\"\"\"o_\n",
      "tensor(143.6387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 3\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooooooop:ooooooo\"oooooooooooooooooooooo5ooo\"o\"\"\"\"\"ooo\"\"o\"\"\"\"o\"\"o\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"o\"\"\"\"\"\"oo\"\"\"\"\"\"\"\"\"M\"oo\"\"\"\"\"ooo\"\"\"\"\"\"\"\"\"\"\"\"\"oooooo\"\"\"oo\"\"o\"\"\"\"\"\"\"\"\"\"\"\"\"oo\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"oooo\"\"\"\"\"\"\"ooooo\"\"\"\"\"\"Mo\"\"\"\"\"\"o\"\"\"\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"o\"\"\"\"o\"\"\"_\n",
      "tensor(143.6429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 4\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooo\"\"o\"\"\"o\"\"\"oo\"\"\"ooooooooM\"o\"\"\"oo\"\"\"\"\"o\"\"\"ooooo\"\"\"\"\"\"\"\"M\"\"\"\"o\"o\"\"\"\"\"\"\"oooo\"ooooooooo\"\"o\"\"\"\"\"\"\"\"o\"o\"oooooooo\"o\"ooooooooooooo\"\"\"\"\"\"\"\"\"o\"\"\"\"oo\"o\"\"\"\"\"\"o\"\"\"\"\"M\"M\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"\"oo\"\"\"\"\"\"o\"\"\"\"\"\"\"\"\"\"\"\"oooooooooooo_\n",
      "tensor(143.4335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 5\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_o6o\"oooooooooo\"ooooo\"\"\"\"\"\"\"o\"\"\"\"o\"5oooo\"\"\"\"\"oo\"ooo\"oooooooooooooo\"ooooooo\"Moooooo\"ooooooooooo\"oo\"o\"\"oooooooooooo\"oooooooo\"oo\"o\"\"oooooooooo\"ooooooooooo\"\"ooo\"\"oooooooo\"o\"o\"ooooooooooooooooooooooooooMoooo\"ooooooo\"o\"oooooooMoooooooo\"o\"\"\"ooooooooooooooo_\n",
      "tensor(143.3349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 6\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oo5o555555ooooooooooo:\":\"IIoo\"\"o\"\"\"oooooooooooooooooooooooooooooooooooooooooooooooooooooo\"ooooooooooooooooooooooooooooooooooooooooooo\"ooooooooooooooooooooooooooooooooooooooooooooo\"\"\"\"ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(143.0735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 7\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooo\"ooooooooooooooo\"\"oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\"\"\"oo\"oo\"oo\"ooooooooooooooooooooooooo\"oooooooooooooooooooooooooo_\n",
      "tensor(142.8036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 8\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooo5\"oooooo\"\"ooooooo\"ooooo5ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(142.5277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 9\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooo5oooLooooooooooooooooo\"\"oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(142.3449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 10\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(141.9742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 11\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(141.6051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 12\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(141.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 13\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(140.8141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 14\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(140.2935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 15\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(139.7671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 16\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(139.2548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 17\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(138.4650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 18\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo_\n",
      "tensor(137.7453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 19\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_ooooooooooooo_\n",
      "tensor(136.9783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 20\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_oooooooo_\n",
      "tensor(136.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 21\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "_o_\n",
      "tensor(135.2328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 22\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(134.0559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 23\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(133.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 24\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(131.8065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 25\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(130.6644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 26\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(129.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 27\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(127.8797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 28\n",
      "\t to 19 .\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 15, 255])\n",
      "torch.Size([1, 16, 13, 253])\n",
      "torch.Size([1, 32, 11, 251])\n",
      "torch.Size([1, 64, 9, 249])\n",
      "torch.Size([1, 128, 6, 248])\n",
      "torch.Size([1, 128, 248])\n",
      "torch.Size([1, 248, 128])\n",
      "torch.Size([1, 248, 256])\n",
      "torch.Size([1, 248, 73])\n",
      "torch.Size([248, 1, 73])\n",
      "__\n",
      "tensor(126.4555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Epoch 29\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAABPCAYAAAA9dhWEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAT7klEQVR4nO3dfVBU1/kH8O/dZVlestAI4WWFH0uiRAFDRzQEE2MQB2MjxtY0NjYNSmtrfBmMfYs4KZhkZp22004yE8lYG6cmdrAt2tjGmNAa0U7UhAXCi4pYRSGCW1RgQdiF5fn94bCTDWCAsHtBv5+ZO5M95+zdZ88T3GfuPfdeRUQERERERF6mUTsAIiIiujOxCCEiIiJVsAghIiIiVbAIISIiIlWwCCEiIiJVsAghIiIiVbAIISIiIlWwCCEiIiJVsAghIiIiVbAIISIiIlV4rAjZvn07YmNj4efnh+TkZBw7dsxTH0VEREQTkEeKkL1792Ljxo3YsmULysvLMXfuXCxatAiXLl3yxMcRERHRBKR44gF2KSkpmDlzJgoKClxt06dPx9KlS2E2m2/53r6+Ply+fBkGgwGKoox1aEREROQBIgKbzQaj0QiNZnjHOHzGOgiHwwGLxYIXX3zRrT0jIwMff/zxgPF2ux12u931+vPPP0d8fPxYh0VERERe0NDQgKioqGGNHfMipKWlBU6nE+Hh4W7t4eHhaG5uHjDebDZj69atA9obGhoQFBQ0oO1b3/oWVqxYgc2bN49t4B7U1dUFh8MBg8Ew7OoQAJxOJzo6OhAcHOzB6IiIiL6+9vZ2REdHw2AwDPs9Y16E9PvyqRQRGfT0yubNm7Fp0ybX6/4vERQUNKAIiYyMRFpaGsrLy6HT6eDv7++Z4MfYnj17sG/fPhQUFGDKlClfOV5E8Mknn8BsNsNisSAzMxMvv/wyQkNDvRAtERHR6I1kKcWYL0wNDQ2FVqsdcNTDarUOODoCAHq93lVwDFZ4fFlvby+uXLkCh8MxpnF7WmVlJd588020t7ffcpyIoKioCGlpaXj33XfR2NiIHTt2YO7cuSgoKIDT6fRSxERERJ415kWIr68vkpOTUVxc7NZeXFyMOXPmjMln2Gw2VFRUjMm+vOkvf/nLsIqQyspKdHV1QaPRICYmBiEhIThz5gx+/etf4/Dhw16KloiIyLM8conupk2bsHPnTrz11ls4ffo0XnjhBVy6dAlr1qz5WvsNCAhAeno6urq6UF9fPzbBepFWq73lmpDq6mpkZ2ejvr4eGo0G8+fPR2lpKf72t78BAJqamvDWW295K1wiIiKP8siakOXLl+Pq1at4+eWX0dTUhMTERBw8eBAxMTFfa7++vr5ISEiA0Wgco0i9q3/R7mBEBNu2bYOvry8KCwvxk5/8BGazGf7+/rh8+TKAmwtVr127hpaWFq4PISKiCc9jC1PXrl2LtWvXjuk+e3p6YLPZsGTJElit1jHdt9o6OzuxZ88emEwmZGdn45VXXkFwcDAcDodr/UtfXx/a29vR3t7OIoSIiCY8jxUhnnDt2jX86le/gqIouOeee9QOxyMaGhrw9NNPIyQkBMDN+66cPXsWwM0ipKOjA21tbWqGSERENCYm1APs9Ho9YmNjcfz4cTQ3N+O///2v2iGNiN1uR3l5OXp6egb0HTlyBMDN0zLV1dWu9p6eHpw4ccL1urW1FZ999pnHYyUiIvK0CVWE6HQ6TJkyBX19fWhtbZ0wp2Rmz56Nhx9+GFqtFvX19YOuCzlz5syg7+3t7cW5c+dcr7u6uvgMHiIiui1MqCJEURQEBASoHcaIlZaW4uTJkxARJCQkQKfTDRjz0EMPAbh5JOTTTz8dcl8GgwHTpk3zWKxERETeMqGKEL1ej+nTpwO4uVbiypUrKkc0PCKC3t5e6HQ6xMXFQavVDhiTmpqKtLQ0iAj+8Y9/4NChQxARXLt2DV1dXQAAjUaDiIgIzJw509tfgYiIaMxNqIWpX2S32yfM6Zh+DocDdXV1MBqNAwoRrVaL1157DQsWLIDVasWKFSuQk5ODc+fOuYotX19f3HfffcO69TsREdF4N6GOhHxRZ2fnhLthmaIo8PX1HbJ/xowZ2LFjBxITE3H9+nXk5+fjnXfecfWHhoYiMzPTG6ESERF53IQqQnx8fDB16lRMmzbNdev2a9euqR3WV7p48SLsdjt0Oh1iYmIGPR3T78knn8TJkyfx9ttv47nnnsO8efMA3DwKkpSUhPT0dG+FTURE5FETqghRFAV+fn4IDAyE0+lEU1OT22W6PT09KC8vx+rVqxEZGYmIiAj84he/wNmzZ1V98FtZWRlsNtuwxwcEBODZZ5/Fzp07XTd8CwgIwIMPPsiblBER0W1jQq0JERHYbDa0trYCuFl0nDlzxvVU3aNHj2Lv3r1uT/D9zW9+gw8++ABbtmxBfHw84uLibnlKhIiIiLxj3BYhS5YsQXBwsNvTckUEXV1daGlpAQDU1NTgueeeG3IfWq0WOp0O586dQ1ZWFnx8fGA2m5GdnT0hL/UlIiK6nYzbIqSkpGRY40JCQhAdHY3Gxka0tLRAo9EgIyMDmZmZiIuLw9SpU9HZ2Ynf//732Lt3L/Lz85GSkoLk5ORbPtF2PHA6nUPexIyIiGiiG7e/wj//+c/xzW9+0/Xax8cHSUlJ+MEPfoBZs2YBAIxGI/Ly8vDb3/4WYWFhAIDo6GisWrUKa9euxYIFCxATE4P4+HhkZWUhJCQEV69exdatW0e0RkMtWq2WNyYjIqLb1oiKELPZjNmzZ8NgMCAsLAxLly5FbW2t25iVK1dCURS3rf9uoCORm5uLhIQEGAwGpKWloa6uDhaLBa+//joWLlwI4OYVI0FBQbDb7ejo6AAAPPbYY0hJSRmwv4cffhirVq1CcHAw3nvvPRw7dkzVxapERER3uhEVISUlJVi3bh1OnDiB4uJi9Pb2IiMjA52dnW7jHn/8cTQ1Nbm2gwcPjjwwjQZvv/02PvjgA9hsNqSlpeH48eO4ceMGKisrh3xfeHj4oE/YVRQFubm5eP755xEYGIhTp06xCCEiIlLRiIqQQ4cOYeXKlUhISEBSUhJ27dqFS5cuwWKxuI3T6/WIiIhwbZMmTRpVcIqiIDU1Ffv370dKSgqysrJQW1uLOXPmuI279957XUdbnE7nkMWFj48Pli1bhrvuugtnzpzxehHicDhQW1s77M9VFAVarRaKong4MiIiIu/7WmtC2traAGBAkXHkyBGEhYUhLi4Oq1evvuXt1e12O9rb2922L5s8eTJeeeUVfOMb38Dy5ctRWFgIAOju7kZrayt8fHxcl93++9//RnFxMbq7u932YbPZUFpair/+9a/o7OzEo48+Ch8f767LdTgcOHXqFHp6eoY1XlEUhIaGIiAgAJ2dnbBYLOjt7fVwlERERN4x6l9hEcGmTZvwyCOPIDEx0dW+aNEifPe730VMTAwuXLiAl156CfPnz4fFYoFerx+wH7PZjK1bt97ysxRFwZQpU1BYWIicnBy8//77AACr1YqTJ09ixYoViIyMhKIoqKiowI9+9CNkZmbi/vvvx/nz55Geno6DBw9i//79CAgIwC9/+Us8/fTTgz7N1hNMJhP8/Pxw48aNEb9Xo9FAo9Ggp6cHjY2N6Ovr80CEREREKpBRWrt2rcTExEhDQ8Mtx12+fFl0Op0UFRUN2t/d3S1tbW2uraGhQQBIW1vbgLFOp1PKysokPj5eAIiiKLJs2TK5ceOGXLhwQVavXi06nU4ADNjCwsLkhRdekNLSUunp6Rnt1x6V7du3S1hYmACQZ599Vtrb24f9XqvVKiaTSQDIzJkzxW63ezBSIiKi0Wlraxvy93soozoSsmHDBhw4cABHjx5FVFTULcdGRkYiJiYGdXV1g/br9fpBj5AMRqPRYNq0afjZz36GH//4x3A6nejt7YXD4YDJZMKrr76KxMREvPHGGzh79ixMJhOmT5+Op556Cunp6YiKirrlc1s8Ze7cuQgKCoLVakVnZydEZFT7sdvtuHz5Mkwm05Bjrly5gurqaphMJphMJlW+LxER0XCMaE2IiGD9+vXYt28fDh8+jNjY2K98z9WrV9HQ0IDIyMhRB/lF/v7+WLhwIVatWoXY2FjMmjULwcHBAICwsDBs2LABpaWlaG5uxmeffYZ//vOfyM7O/soHx3lSfHw8Tpw4gebmZuzZswdBQUHDfq+/vz+eeuopAEBTUxN279495Nj//e9/eOmll7B48WI8+eSTbnebJSIiGm9GdCRk3bp1+POf/4x3330XBoPB9YyW4OBg+Pv7o6OjA/n5+Vi2bBkiIyNRX1+P3NxchIaG4tvf/vaYBW00GrFjx45B+xRFgcFggMFgGLPP+7o0Gg1CQkJG/X4/Pz8AQGdnJ06fPj3kOL1eD41Gg76+PtTU1OD69euj/kwiIiJPG1ERUlBQAODmDcG+aNeuXVi5ciW0Wi2qqqqwe/dutLa2IjIyEmlpadi7d++wi4L+UxWDXSVzJ+rr68Pjjz+Od955BwDwzDPPDDk3x48fR1VVFQICAvDDH/4QkydP5jwSEZFX9P/ejGTJgSKjXaDgIY2NjYiOjlY7DCIiIhqFhoaGr1wv2m/cFSF9fX2ora1FfHw8GhoaRrR+gsZOe3s7oqOjmQMVMQfqYw7UxflX30hyICKw2WwwGo3DfkDsuHuKrkajweTJkwEAQUFB/B9PZcyB+pgD9TEH6uL8q2+4Oei/UGS4xu1TdImIiOj2xiKEiIiIVDEuixC9Xo+8vLxh38SMxh5zoD7mQH3Mgbo4/+rzdA7G3cJUIiIiujOMyyMhREREdPtjEUJERESqYBFCREREqmARQkRERKpgEUJERESqGHdFyPbt2xEbGws/Pz8kJyfj2LFjaod02zh69CgyMzNhNBqhKAr+/ve/u/WLCPLz82E0GuHv74/HHnsMNTU1bmPsdjs2bNiA0NBQBAYGYsmSJWhsbPTit5i4zGYzZs+eDYPBgLCwMCxduhS1tbVuY5gDzyooKMADDzzguvtjamoq3n//fVc/59/7zGYzFEXBxo0bXW3Mg2fl5+dDURS3LSIiwtXv1fmXcaSwsFB0Op384Q9/kFOnTklOTo4EBgbKxYsX1Q7ttnDw4EHZsmWLFBUVCQDZv3+/W/+2bdvEYDBIUVGRVFVVyfLlyyUyMlLa29tdY9asWSOTJ0+W4uJiKSsrk7S0NElKSpLe3l4vf5uJZ+HChbJr1y6prq6WiooKeeKJJ+T//u//pKOjwzWGOfCsAwcOyHvvvSe1tbVSW1srubm5otPppLq6WkQ4/972ySefiMlkkgceeEBycnJc7cyDZ+Xl5UlCQoI0NTW5NqvV6ur35vyPqyLkwQcflDVr1ri1TZs2TV588UWVIrp9fbkI6evrk4iICNm2bZurrbu7W4KDg+XNN98UEZHW1lbR6XRSWFjoGvP555+LRqORQ4cOeS3224XVahUAUlJSIiLMgVruvvtu2blzJ+ffy2w2m0ydOlWKi4tl3rx5riKEefC8vLw8SUpKGrTP2/M/bk7HOBwOWCwWZGRkuLVnZGTg448/VimqO8eFCxfQ3NzsNv96vR7z5s1zzb/FYkFPT4/bGKPRiMTEROZoFNra2gAAkyZNAsAceJvT6URhYSE6OzuRmprK+feydevW4YknnsCCBQvc2pkH76irq4PRaERsbCy+973v4fz58wC8P//j5im6LS0tcDqdCA8Pd2sPDw9Hc3OzSlHdOfrneLD5v3jxomuMr68v7r777gFjmKORERFs2rQJjzzyCBITEwEwB95SVVWF1NRUdHd346677sL+/fsRHx/v+seT8+95hYWFKCsrw6effjqgj38HnpeSkoLdu3cjLi4OV65cwauvvoo5c+agpqbG6/M/boqQfoqiuL0WkQFt5DmjmX/maOTWr1+PyspK/Oc//xnQxxx41v3334+Kigq0traiqKgIWVlZKCkpcfVz/j2roaEBOTk5+PDDD+Hn5zfkOObBcxYtWuT67xkzZiA1NRX33Xcf/vSnP+Ghhx4C4L35HzenY0JDQ6HVagdUUVardUBFRmOvf2X0reY/IiICDocD169fH3IMfbUNGzbgwIED+OijjxAVFeVqZw68w9fXF1OmTMGsWbNgNpuRlJSE1157jfPvJRaLBVarFcnJyfDx8YGPjw9KSkrw+uuvw8fHxzWPzIP3BAYGYsaMGairq/P638G4KUJ8fX2RnJyM4uJit/bi4mLMmTNHpajuHLGxsYiIiHCbf4fDgZKSEtf8JycnQ6fTuY1pampCdXU1czQMIoL169dj3759OHz4MGJjY936mQN1iAjsdjvn30vS09NRVVWFiooK1zZr1ix8//vfR0VFBe69917mwcvsdjtOnz6NyMhI7/8djGgZq4f1X6L7xz/+UU6dOiUbN26UwMBAqa+vVzu024LNZpPy8nIpLy8XAPK73/1OysvLXZdAb9u2TYKDg2Xfvn1SVVUlzzzzzKCXZUVFRcm//vUvKSsrk/nz5/OyuGF6/vnnJTg4WI4cOeJ2adyNGzdcY5gDz9q8ebMcPXpULly4IJWVlZKbmysajUY+/PBDEeH8q+WLV8eIMA+e9tOf/lSOHDki58+flxMnTsjixYvFYDC4fmu9Of/jqggREXnjjTckJiZGfH19ZebMma7LF+nr++ijjwTAgC0rK0tEbl6alZeXJxEREaLX6+XRRx+Vqqoqt310dXXJ+vXrZdKkSeLv7y+LFy+WS5cuqfBtJp7B5h6A7Nq1yzWGOfCs7Oxs178v99xzj6Snp7sKEBHOv1q+XIQwD57Vf98PnU4nRqNRvvOd70hNTY2r35vzr4iIjPoYDhEREdEojZs1IURERHRnYRFCREREqmARQkRERKpgEUJERESqYBFCREREqmARQkRERKpgEUJERESqYBFCREREqmARQkRERKpgEUJERESqYBFCREREqvh/153YmwoVOOEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=64, recognizer_lr=0.00004,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
