{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/aps360/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "# preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "20 10\n",
      "images\n",
      "20 10\n",
      "both\n",
      "1000 10\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(20))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(20))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(1000))\n",
    "line_dataset_val = Subset(line_dataset_val, range(10))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    recognizer_optimizer = optim.Adam(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.CTCLoss(reduction='sum', zero_infinity=True)\n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = []\n",
    "    recognizer_train_accuracies = []\n",
    "    recognizer_val_losses = []\n",
    "    recognizer_val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "        recognizer_train_accuracy = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            print(\"\\t\",test)\n",
    "            print(line_text_batch.shape)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            recognizer_outputs = recognizer(line_image_batch)\n",
    "\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            target = torch.cat(line_text_batch_pad_remove)\n",
    "            input_lengths = torch.full(size=(cur_batch_size,), fill_value=2*82+1)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                recognizer_outputs.log_softmax(2),\n",
    "                target,\n",
    "                input_lengths,\n",
    "                target_lengths\n",
    "            )\n",
    "            test2 = recognizer_outputs[:,0,:]\n",
    "#             print(test2)\n",
    "            test2 = torch.softmax(test2, dim=1)\n",
    "#             print(test2)\n",
    "            test2 = torch.argmax(test2, dim=1)\n",
    "#             print(test2.shape)\n",
    "            test2 = test2[test2.nonzero()]\n",
    "#             print(test2.shape)\n",
    "#             test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "            print(f\"_{test2}_\")\n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            print(recognizer_loss)\n",
    "            # recognizer_train_loss += recognizer_loss\n",
    "        \n",
    "        recognizer_val_loss = 9999\n",
    "\n",
    "        print(f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 32, 73])\n"
     ]
    }
   ],
   "source": [
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=2*82+1, kernel_size=(3, 3)),\n",
    "            nn.BatchNorm2d(2*82+1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # Batch first is false to make it easier for CTC loss\n",
    "        self.rnn = nn.GRU(input_size=496, hidden_size=256, bidirectional=True, num_layers=5, batch_first=True)\n",
    "        self.final = nn.Linear(256*2, 73)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        '''\n",
    "        torch.Size([32, 1, 32, 512])\n",
    "        torch.Size([32, 256, 8, 128])\n",
    "        torch.Size([32, 256, 1024])\n",
    "        torch.Size([32, 165, 256])\n",
    "        torch.Size([32, 165, 256])\n",
    "        torch.Size([32, 165, 73])\n",
    "        torch.Size([165, 32, 73])\n",
    "        '''\n",
    "        \n",
    "        bs, _, _, _ = img.shape\n",
    "        # Encode it with CNNs\n",
    "        img = self.encoder(img)\n",
    "        # Next flatten but keep it in a form to be fed into the RNN\n",
    "        img = img.view(bs, img.size(1), -1)\n",
    "        # FC gets the 2*82+1 sequence length\n",
    "        img, _ = self.rnn(img)\n",
    "        # Pass it through a final FC layer\n",
    "        img = self.final(img)\n",
    "        # Change to (T, N, C)\n",
    "        img = img.permute(1, 0, 2)\n",
    "        print(img.shape)\n",
    "        # No sigmoid because CTC loss requires `log_softmax`\n",
    "        return img\n",
    "        \n",
    "        \n",
    "recog = Recognizer()\n",
    "recog(torch.randn((32, 1, 32, 512), dtype=torch.float32))\n",
    "None\n",
    "# print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "\t The details are harder still to see .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_YDDDUUUUUUUUUUUUUUUUwwwwwwwwwwUDDUUUUUUwwwwwwwwwwwwwwwUUUUUwwwwGGGGGGGGwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwUUUUwwwwwwwwwwwwwwwwwwwwwwwwwwwwww_\n",
      "tensor(37040.3203, grad_fn=<SumBackward0>)\n",
      "\t sunshine , still burning like a half-cooled iron ,\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_qqqqqqqqqqqqhhhhhhhhhqqqqqqqqGqqqqq;qqqqqqqqqDDGGGqqqwwqqqqqqqqqqGGGGGGGwwwqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqDDqqqqqqqqqqqqqqqqqqqqqqqqhhhqqqqqqqqqqqqqqqqqqqwwwwwwwwwww_\n",
      "tensor(36805.2852, grad_fn=<SumBackward0>)\n",
      "\t from any point of view .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_I2222GGGGGG22gg2222222;;22222222222222wwwwwww22qqqqqqqq2222222222wwwwwwwww22222wwwwww22222222222qqqq2222222222222222222cc2qqqqq222222222222222222222222222wGGwwwwwwww_\n",
      "tensor(36716.8672, grad_fn=<SumBackward0>)\n",
      "\t as ' more like a God upon earth than a human\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_bbbUUUwwwwwwwwwwwwwwwwwwqqqqqqqqqwwwwGGwwwwwzUUUUUUUUUUUUUqqqqqqqqqq22222qqqqqqqqwwwwwwwwwwGGGGGGwwwwwzzzqqzbbbbUUbbbbbbbbaaaaaaabbbbbbbbbbwwwwwwwwwqqqqqqwwwwwwwwwww_\n",
      "tensor(36527.2031, grad_fn=<SumBackward0>)\n",
      "\t forward in an ungainly shuffle , neck\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_Rwwwwwwwwwww6zzzzzzzwwww2222222222222wwwwwwwwwwwwwwwwwwwwwUUUUUUUUDDDGwwwwwwwwGGwwwwwwwwzzzqqq222zz22wwqqqqq22222222wwwUUwwYwwCCCCwww'''''''''AA222qqqqqqqwwwwwwwwwww_\n",
      "tensor(36247.3125, grad_fn=<SumBackward0>)\n",
      "\t cigarettes worth smoking , these . Most unhealthy , English\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_bYwwwwUUUUUwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwDDDDUUUUUUUUwwwwUwwwiiiiiwww_\n",
      "tensor(36183.2969, grad_fn=<SumBackward0>)\n",
      "\t the neuro-physiologists .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_IIIqqqw222GGGGGGGCC2222GGGGGGGGGGGGGGwwwGGGwwwwwwwwwwwwwwwGGGGGGGGwwwwwwwwwiiiiiiwGGGGGwwwwwwwwwwGiiwwGwwwwwwwwqqqieeeewwwwwwwwwwe2wwww_\n",
      "tensor(36052.3320, grad_fn=<SumBackward0>)\n",
      "\t Mr. Macmillan at Chequers .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_iGwwwqqqqqqq222qqqqqqqww_\n",
      "tensor(35869.2773, grad_fn=<SumBackward0>)\n",
      "\t search for an effective alleviation of his painful\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "_www_\n",
      "tensor(35340.1836, grad_fn=<SumBackward0>)\n",
      "\t a nostalgic affection for them .\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "__\n",
      "tensor(35400.4297, grad_fn=<SumBackward0>)\n",
      "\t performance of rare intelligence and restrained\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "__\n",
      "tensor(35129.6953, grad_fn=<SumBackward0>)\n",
      "\t advise Anglesey to give the system a trial , adding\n",
      "torch.Size([64, 82])\n",
      "torch.Size([165, 64, 73])\n",
      "__\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m recognizer \u001b[39m=\u001b[39m Recognizer()\n\u001b[1;32m      2\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(recognizer\u001b[39m=\u001b[39mrecognizer, \n\u001b[1;32m      6\u001b[0m               train_line_dataset\u001b[39m=\u001b[39mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39mline_dataset_val, \n\u001b[1;32m      7\u001b[0m               batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, recognizer_lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m,\n\u001b[1;32m      8\u001b[0m               betas\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0.999\u001b[39m), num_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, loss_balancing_alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(recognizer, train_line_dataset, val_line_dataset, batch_size, recognizer_lr, betas, num_epochs, loss_balancing_alpha)\u001b[0m\n\u001b[1;32m     68\u001b[0m test2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([int_to_char[\u001b[39mint\u001b[39m(i)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test2])\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mtest2\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m recognizer_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     73\u001b[0m recognizer_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     74\u001b[0m \u001b[39mprint\u001b[39m(recognizer_loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/aps360/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aps360/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=64, recognizer_lr=0.00001,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
