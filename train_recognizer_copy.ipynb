{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "c:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "from torchmetrics.text import CharErrorRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 20900\n"
     ]
    }
   ],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved_recognizer.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel, lambda x: x)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "\n",
    "            # RECOGNIZER EXCLUSIVE\n",
    "            # Some ways to augment images:\n",
    "            # - horizontally compress\n",
    "            # - horizontally stretch\n",
    "            # - vertically compress\n",
    "            # - vertically stretch\n",
    "            if len(transcription) > 60:\n",
    "                for i in range(int(len(transcription)/2)):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_compress)\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hc{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "            if len(transcription) < 30:\n",
    "                for i in range(10):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_stretch)\n",
    "                    # Just in case after stretching we have cut off boundary\n",
    "                    if resized_tensor is None:\n",
    "                        continue\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hs{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "\n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def horizontally_compress(cv_image):\n",
    "    \"\"\" by random percent \"\"\"\n",
    "    factor = 1 - random.random()/2\n",
    "    height, width = cv_image.shape\n",
    "    output = cv.resize(cv_image, (width, int(height*factor)), interpolation=cv.INTER_AREA)\n",
    "    return output\n",
    "    ...\n",
    "\n",
    "\n",
    "def horizontally_stretch(cv_image):\n",
    "    \"\"\" by random percent\"\"\"\n",
    "    factor = 1 + random.random()/2\n",
    "    height, width = cv_image.shape\n",
    "    output = cv.resize(cv_image, (width, int(height*factor)), interpolation=cv.INTER_LINEAR)\n",
    "    return output\n",
    "    ...\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel, additional_func=None):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # Apply additional filter\n",
    "    output = additional_func(output)\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "\n",
    "    if padding_to_add < 0:\n",
    "        return\n",
    "\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "320 10\n",
      "images\n",
      "320 10\n",
      "both\n",
      "16720 4180\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "# line_dataset_train = Subset(line_dataset_train, range(19000))\n",
    "# line_dataset_val = Subset(line_dataset_val, range(1000))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 440., 1177.,  990., 1287., 1430., 1683., 1926.,  615., 1144.,\n",
       "        1040., 1288.,  905.,  580.,  292., 1705., 2639.,  767.,  673.,\n",
       "         115.,  204.]),\n",
       " array([ 4. ,  7.9, 11.8, 15.7, 19.6, 23.5, 27.4, 31.3, 35.2, 39.1, 43. ,\n",
       "        46.9, 50.8, 54.7, 58.6, 62.5, 66.4, 70.3, 74.2, 78.1, 82. ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = line_dataset.lines_df[\"transcription_len\"].to_numpy()\n",
    "plt.hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([52, 61, 64,  1, 66, 51, 60,  1, 50, 47, 71, 65,  1, 47, 60, 50,  1, 65,\n",
       "         54, 61, 69, 51, 50,  1, 60, 61,  1, 47, 48, 60, 61, 64, 59, 47, 58, 55,\n",
       "         66, 55, 51, 65,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'for ten days and showed no abnormalities .')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[9995]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 73, 82])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4,2))\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        #self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4,2))\n",
    "        #xself.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        #self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,2))\n",
    "        #self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=4, bidirectional=True, batch_first=True)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(505, 82)\n",
    "        \n",
    "        #self.fc = nn.Linear(10, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.conv1(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        #img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        #print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.lrelu(self.dense2(img))\n",
    "        \n",
    "        #img = self.fc(img)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((1, 1, 32, 512), dtype=torch.float32))\n",
    "#print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function):\n",
    "    recognizer.eval() \n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(val_line_dataset_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device) \n",
    "        line_text_batch = line_text_batch.to(device)\n",
    "        recognizer_outputs = recognizer(line_image_batch)\n",
    "        recognizer_loss = recognizer_loss_function(F.log_softmax(recognizer_outputs, 1), line_text_batch)\n",
    "        \n",
    "        total_loss += recognizer_loss.item()\n",
    "        total_epoch += 1\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    \n",
    "    #print(recognizer_outputs, recognizer_outputs.shape)\n",
    "    #print(torch.argmax(recognizer_outputs, 1), torch.argmax(recognizer_outputs, 1).shape)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_recog_accuracy(preds, target):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the recognizer with character error rate\n",
    "    which is based on edit distance\n",
    "\n",
    "    Params:\n",
    "        preds: a list of prediction strings\n",
    "        targets: a list of target strings\n",
    "\n",
    "    Returns:\n",
    "        An integer, the character error rate average across\n",
    "        all predictions and targets\n",
    "    \"\"\"\n",
    "\n",
    "    cer = CharErrorRate()\n",
    "    return cer(preds, target)\n",
    "\n",
    "def create_strings_from_tensor(int_tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        int_tensor: A shape (N, 82) tensor where each row corresponds to\n",
    "        a integer mapping of a string. Includes padding\n",
    "    \n",
    "    Returns:\n",
    "        A list of N strings\n",
    "    \"\"\"\n",
    "\n",
    "    strings = []\n",
    "    for string_map in int_tensor:\n",
    "        strings.append(\"\".join([int_to_char[int(i)] for i in string_map[string_map != 0]]))\n",
    "    return strings\n",
    "    \n",
    "\n",
    "def get_accuracy(device, recognizer, recognizer_loader):\n",
    "\n",
    "    acc = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(recognizer_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device)\n",
    "        line_text_batch\n",
    "        recognizer_outputs = torch.argmax(recognizer(line_image_batch), 1)\n",
    "        recognizer_pred = create_strings_from_tensor(recognizer_outputs)\n",
    "        \n",
    "        label = create_strings_from_tensor(line_text_batch)\n",
    "        \n",
    "        acc += calculate_recog_accuracy(recognizer_pred, label)\n",
    "        \n",
    "        \n",
    "    return acc / (i+1)\n",
    "        \n",
    "    \n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    \n",
    "    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n",
    "    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n",
    "    \n",
    "    n = len(train_loss) # number of epochs\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Train Loss\", \"Validation Loss\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend([\"Train Error\", \"Validation Error\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    #print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #print(len(train_line_dataset_loader))\n",
    "\n",
    "    recognizer_optimizer = optim.Adam(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.NLLLoss()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = np.zeros(num_epochs)\n",
    "    recognizer_train_accuracies = np.zeros(num_epochs)\n",
    "    recognizer_val_losses = np.zeros(num_epochs)\n",
    "    recognizer_val_accuracies = np.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "            # print(line_text_batch.shape)\n",
    "\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            #print(line_text_batch, line_text_batch.shape)\n",
    "            recognizer_outputs = recognizer(line_image_batch)  # Mult factor to incentivize padding\n",
    "   \n",
    "            # print(recognizer_outputs, recognizer_outputs.shape)\n",
    "            # print(line_text_batch, line_text_batch.shape)\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            #line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            #target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            #target = torch.cat(line_text_batch_pad_remove)\n",
    "            #print(target, target.shape)\n",
    "            #input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\n",
    "                F.log_softmax(recognizer_outputs, 1),  # Requires number of classes to move from 2nd to 1st dimension after log_softmax\n",
    "                line_text_batch\n",
    "            )\n",
    "            test2 = recognizer_outputs[0,:,:]\n",
    "            test2 = torch.argmax(test2, dim=0)  # Removed 0 dim\n",
    "            test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            recognizer_optimizer.zero_grad()\n",
    "    \n",
    "            recognizer_train_loss += recognizer_loss.item()\n",
    "        \n",
    "        print(\"\\t\",test)\n",
    "        print(f\"_{test2}_\")\n",
    "        recognizer_train_losses[epoch] = float(recognizer_train_loss) / (i+1)\n",
    "        \n",
    "        recognizer.eval()\n",
    "        recognizer_val_losses[epoch] = evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function)\n",
    "        recognizer.train()\n",
    "        \n",
    "        recognizer_train_accuracies[epoch] = get_accuracy(device, recognizer, train_line_dataset_loader)\n",
    "        recognizer_val_accuracies[epoch]= get_accuracy(device, recognizer, val_line_dataset_loader)\n",
    "        \n",
    "        print((\"Epoch {}: Train loss: {} | Train Accuracy: {} | \"+\n",
    "            \" Validation loss: {} | Validation Accuracy: {}\").format(\n",
    "                    epoch + 1,\n",
    "                    recognizer_train_losses[epoch],\n",
    "                    recognizer_train_accuracies[epoch],\n",
    "                    recognizer_val_losses[epoch],\n",
    "                    recognizer_val_accuracies[epoch]))\n",
    "\n",
    "        model_path = get_model_name(recognizer.name, batch_size, recognizer_lr, epoch)\n",
    "        torch.save(recognizer.state_dict(), os.path.join(\"./recognizers\", model_path))\n",
    "        model_path_const_batch = get_model_name(recognizer.name, batch_size, recognizer_lr, -1)\n",
    "\n",
    "        np.savetxt(\"./recognizers/{}_train_loss.csv\".format(model_path_const_batch), recognizer_train_losses)\n",
    "        np.savetxt(\"./recognizers/{}_val_loss.csv\".format(model_path_const_batch),  recognizer_val_losses)\n",
    "        np.savetxt(\"./recognizers/{}_train_acc.csv\".format(model_path_const_batch), recognizer_train_accuracies)\n",
    "        np.savetxt(\"./recognizers/{}_val_acc.csv\".format(model_path_const_batch), recognizer_val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_curve(\"./recognizers/model_recognizer_bs8_lr0.0005_epoch-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t the pictures just for the fun of it was\n",
      "_he     e                     _\n",
      "Epoch 1: Train loss: 1.668057032920527 | Train Accuracy: 0.822993814945221 |  Validation loss: 1.570131316581602 | Validation Accuracy: 0.8233802914619446\n",
      "\t boy . He settled me into the car\n",
      "_tee                                    _\n",
      "Epoch 2: Train loss: 1.5233965459908025 | Train Accuracy: 0.7595633864402771 |  Validation loss: 1.462021153014203 | Validation Accuracy: 0.7611292004585266\n",
      "\t into the darkness .\n",
      "_the    eeee     ._\n",
      "Epoch 3: Train loss: 1.3851299382567976 | Train Accuracy: 0.6888989210128784 |  Validation loss: 1.30318131159642 | Validation Accuracy: 0.6942667365074158\n",
      "\t ambition .\n",
      "_aaneee   _\n",
      "Epoch 4: Train loss: 1.1942217172332927 | Train Accuracy: 0.5594062209129333 |  Validation loss: 1.0813953851422662 | Validation Accuracy: 0.5726074576377869\n",
      "\t gang of crooks .\n",
      "_tag  oof ee    _\n",
      "Epoch 5: Train loss: 0.9831016676562825 | Train Accuracy: 0.4498908221721649 |  Validation loss: 0.8931933429455438 | Validation Accuracy: 0.46115416288375854\n",
      "\t movement , Vittoria was separated from\n",
      "_teeee        ei           _\n",
      "Epoch 6: Train loss: 0.857571899577191 | Train Accuracy: 0.4249662458896637 |  Validation loss: 0.8197553726785726 | Validation Accuracy: 0.43864619731903076\n",
      "\t Hostettler\n",
      "_hanlnens   _\n",
      "Epoch 7: Train loss: 0.8123377099847109 | Train Accuracy: 0.4211103916168213 |  Validation loss: 0.8248034052650514 | Validation Accuracy: 0.43543919920921326\n",
      "\t of half words .\n",
      "_o  teo     .._\n",
      "Epoch 8: Train loss: 0.7593564685808415 | Train Accuracy: 0.41502976417541504 |  Validation loss: 0.8141609738023513 | Validation Accuracy: 0.4294244647026062\n",
      "\t waving , unkissed , from the window . And Dai , on the pavement , knowing in his\n",
      "_naving , unkissed , from the window . And Dai , on the pavement , knowing in his_\n",
      "Epoch 9: Train loss: 0.7196051044024206 | Train Accuracy: 0.37891486287117004 |  Validation loss: 0.7428236522820438 | Validation Accuracy: 0.3964933156967163\n",
      "\t my tongue like spray .\n",
      "_my  ooggehe ee   rry _\n",
      "Epoch 10: Train loss: 0.7079109393202803 | Train Accuracy: 0.3555416166782379 |  Validation loss: 0.7029508765266913 | Validation Accuracy: 0.378775030374527\n",
      "\t to Heather's and saw two more flying saucers on\n",
      "_tt  aeooss  s ee n    t      s   o ti o   ae_\n",
      "Epoch 11: Train loss: 0.6429552100759232 | Train Accuracy: 0.3320038914680481 |  Validation loss: 0.682863278727601 | Validation Accuracy: 0.35481759905815125\n",
      "\t carried on .\n",
      "_carried on ._\n",
      "Epoch 12: Train loss: 0.6520220664699682 | Train Accuracy: 0.3135557770729065 |  Validation loss: 0.663098858737068 | Validation Accuracy: 0.3411727845668793\n",
      "\t brother-in-law , the diplomatist Lord\n",
      "_thril  -            e    roeertt ttt_\n",
      "Epoch 13: Train loss: 0.6086801113551884 | Train Accuracy: 0.30036529898643494 |  Validation loss: 0.637567222374803 | Validation Accuracy: 0.3313620984554291\n",
      "\t ' What a frightful event ! ' he wrote . ' I tremble ! What infatuation !\n",
      "_' What a frightful event ! ' he wrote . ' I tremble ! What infatuation !_\n",
      "Epoch 14: Train loss: 0.5649064326204324 | Train Accuracy: 0.28024449944496155 |  Validation loss: 0.6100489781959335 | Validation Accuracy: 0.31017109751701355\n",
      "\t floundered on to the electrocution\n",
      "_Ahostttang       hhtttecdrraaeio_\n",
      "Epoch 15: Train loss: 0.533356010668764 | Train Accuracy: 0.2624940276145935 |  Validation loss: 0.581322715003364 | Validation Accuracy: 0.2935143709182739\n",
      "\t persevering , ' but he appears to despair of ever being cured . '\n",
      "_persevering , ' but he appears to despair of ever being cured . '_\n",
      "Epoch 16: Train loss: 0.5512664445212476 | Train Accuracy: 0.2572765648365021 |  Validation loss: 0.5801537551954531 | Validation Accuracy: 0.28978973627090454\n",
      "\t as a statesman . \"\n",
      "_as a statesman . _\n",
      "Epoch 17: Train loss: 0.5208823194112361 | Train Accuracy: 0.2516401410102844 |  Validation loss: 0.5878040203842446 | Validation Accuracy: 0.2814125716686249\n",
      "\t had bought a railway ticket .\n",
      "_had bought a railway ticket ._\n",
      "Epoch 18: Train loss: 0.4908671428609871 | Train Accuracy: 0.24037887156009674 |  Validation loss: 0.5691055655942421 | Validation Accuracy: 0.273434579372406\n",
      "\t the Synoptics can be reasonably solved by paying due regard to the time and\n",
      "_the 7ynoptics can be reasonably solved by paying due regard to the time and_\n",
      "Epoch 19: Train loss: 0.4909344825464742 | Train Accuracy: 0.23916444182395935 |  Validation loss: 0.5522303147718709 | Validation Accuracy: 0.2724279463291168\n",
      "\t waving , unkissed , from the window . And Dai , on the pavement , knowing in his\n",
      "_7aving , unkissed , from the window . And Dai , on the pavement , knowing in his_\n",
      "Epoch 20: Train loss: 0.46897529546331845 | Train Accuracy: 0.246846005320549 |  Validation loss: 0.5687050354786317 | Validation Accuracy: 0.2792438566684723\n",
      "\t way of meeting it .\n",
      "_way of meeting it ._\n",
      "Epoch 21: Train loss: 0.4680890756445317 | Train Accuracy: 0.2397991567850113 |  Validation loss: 0.5656379297113912 | Validation Accuracy: 0.27471432089805603\n",
      "\t The third reason is that the supreme\n",
      "_fies hhi  reesernr  o        erre ee_\n",
      "Epoch 22: Train loss: 0.45691796857440337 | Train Accuracy: 0.2546556890010834 |  Validation loss: 0.6189081021522245 | Validation Accuracy: 0.29201823472976685\n",
      "\t whereas on larger establishments the pots\n",
      "_whturd     aeeaee  aaa s inin          _\n",
      "Epoch 23: Train loss: 0.47269633390348065 | Train Accuracy: 0.23242901265621185 |  Validation loss: 0.5505282774159816 | Validation Accuracy: 0.26831698417663574\n",
      "\t I forgot to tell you , we don't usually lend any\n",
      "_I teeo    t                  o              _\n",
      "Epoch 24: Train loss: 0.44600137833998266 | Train Accuracy: 0.23141135275363922 |  Validation loss: 0.5635137958011375 | Validation Accuracy: 0.26786869764328003\n",
      "\t I 'd marry you myself . \" Gay laughed , Doc was\n",
      "_in aauny       o ar   .                    _\n",
      "Epoch 25: Train loss: 0.4416212831051245 | Train Accuracy: 0.23069405555725098 |  Validation loss: 0.5680600170047871 | Validation Accuracy: 0.2687022089958191\n",
      "\t and Cecil , with a further six stops\n",
      "_andt  wll    h  eh ctrtnea   '    _\n",
      "Epoch 26: Train loss: 0.4343171751514327 | Train Accuracy: 0.2292185127735138 |  Validation loss: 0.5644767150406621 | Validation Accuracy: 0.2661316394805908\n",
      "\t blouse and pantaloons .\n",
      "_blouse and pantaloons ._\n",
      "Epoch 27: Train loss: 0.4253594252678597 | Train Accuracy: 0.22579313814640045 |  Validation loss: 0.5652978163084325 | Validation Accuracy: 0.2675554156303406\n",
      "\t President of the Board of Trade played a conspicuous part , \" the\n",
      "_President of the Board of Trade played a conspicuous part , \" the_\n",
      "Epoch 28: Train loss: 0.4222941830063586 | Train Accuracy: 0.22323009371757507 |  Validation loss: 0.5690000680919441 | Validation Accuracy: 0.26799318194389343\n",
      "\t text of the Gospel .\n",
      "_text of the oospel ._\n",
      "Epoch 29: Train loss: 0.41562099200684544 | Train Accuracy: 0.21822793781757355 |  Validation loss: 0.5653330293322876 | Validation Accuracy: 0.2611764073371887\n",
      "\t dark ; and we went on together to\n",
      "_tart .  n  adc  ep  ne ntatt   f_\n",
      "Epoch 30: Train loss: 0.40938548273440345 | Train Accuracy: 0.2190365195274353 |  Validation loss: 0.5736366545694237 | Validation Accuracy: 0.2660069167613983\n",
      "\t She was already up the bus steps , and the bus was away . And Morfydd Owen\n",
      "_She wis already up the bus steps , and the bus was away . And Morfydd Owen_\n",
      "Epoch 31: Train loss: 0.40564541721997255 | Train Accuracy: 0.21401546895503998 |  Validation loss: 0.5753464772226645 | Validation Accuracy: 0.2653021514415741\n",
      "\t and scour the countryside between here and The Traveller's Joy .\n",
      "_and scour the countryside between here and The Traveller's Joy ._\n",
      "Epoch 32: Train loss: 0.39810564502389684 | Train Accuracy: 0.21215973794460297 |  Validation loss: 0.5705631103555371 | Validation Accuracy: 0.2643505930900574\n",
      "\t Apart from their formal Admiralty House talks , followed by lunch\n",
      "_Apart from their formal Admiralty House talks , followed by lunch_\n",
      "Epoch 33: Train loss: 0.4086203842036176 | Train Accuracy: 0.21705219149589539 |  Validation loss: 0.5708988341653094 | Validation Accuracy: 0.2645779550075531\n",
      "\t 1 gives a list of the bodies and persons consulted . The\n",
      "_\" giaeadn  i t oh the  noie o  n  i    hc    ushem h aho_\n",
      "Epoch 34: Train loss: 0.38342640604222256 | Train Accuracy: 0.2081819325685501 |  Validation loss: 0.5807569746823312 | Validation Accuracy: 0.2625695765018463\n",
      "\t when it occurs before dawn ; but this\n",
      "_wadn iit soceesseo rae ern      h   _\n",
      "Epoch 35: Train loss: 0.43335361754361307 | Train Accuracy: 0.2120761275291443 |  Validation loss: 0.5706597078860031 | Validation Accuracy: 0.2651108205318451\n",
      "\t the slightest effect .\n",
      "_the slightest effect ._\n",
      "Epoch 36: Train loss: 0.37798888905549816 | Train Accuracy: 0.20367395877838135 |  Validation loss: 0.5824593167681243 | Validation Accuracy: 0.26199159026145935\n",
      "\t along . \" To her great relief they arrived at the cluster of little houses\n",
      "_along . \" To her great relief they arrived at the cluster of little houses_\n",
      "Epoch 37: Train loss: 0.3780960375504929 | Train Accuracy: 0.20295806229114532 |  Validation loss: 0.5929447820541016 | Validation Accuracy: 0.26383915543556213\n",
      "\t Shorty .\n",
      "_Shorty ._\n",
      "Epoch 38: Train loss: 0.3695600127948907 | Train Accuracy: 0.20272019505500793 |  Validation loss: 0.5906352192741314 | Validation Accuracy: 0.2654534876346588\n",
      "\t nothing to stop it .\n",
      "_nothing to stop it ._\n",
      "Epoch 39: Train loss: 0.36720577677183797 | Train Accuracy: 0.19943255186080933 |  Validation loss: 0.599304946663435 | Validation Accuracy: 0.2657609283924103\n",
      "\t observations . Thus in Euphausia superba\n",
      "_doseriatino  .  h ncmn ouaa a taallt_\n",
      "Epoch 40: Train loss: 0.3607951993875776 | Train Accuracy: 0.19559089839458466 |  Validation loss: 0.6121679913058047 | Validation Accuracy: 0.26411163806915283\n",
      "\t impression that nothing nasty ever\n",
      "_sepadirisn tra  hrsairt drrr     _\n",
      "Epoch 41: Train loss: 0.3614793127399878 | Train Accuracy: 0.2186509668827057 |  Validation loss: 0.6435030577650889 | Validation Accuracy: 0.2824344336986542\n",
      "\t One wondered if this greater awareness of the physical cosmos might\n",
      "_One wondered if this greater awareness of the physical cosmos might_\n",
      "Epoch 42: Train loss: 0.3625808852847273 | Train Accuracy: 0.18923482298851013 |  Validation loss: 0.6186359568516169 | Validation Accuracy: 0.26116669178009033\n",
      "\t Apart from their formal Admiralty House talks , followed by lunch\n",
      "_Apart from their formal Admiralty House talks , followed by lunch_\n",
      "Epoch 43: Train loss: 0.3557041357751517 | Train Accuracy: 0.1904516965150833 |  Validation loss: 0.6192043181803714 | Validation Accuracy: 0.26445746421813965\n",
      "\t of Nato . The Pentagon has made it perfectly\n",
      "_if vaee . The so faeo  hee eth  etmnnynetidl_\n",
      "Epoch 44: Train loss: 0.3392563869783423 | Train Accuracy: 0.18623125553131104 |  Validation loss: 0.6252605729593039 | Validation Accuracy: 0.26615217328071594\n",
      "\t in central Europe of a zone of controlled\n",
      "_tn ceeahal mearfe,o  Agg     iiioioislltiiei_\n",
      "Epoch 45: Train loss: 0.341339414622345 | Train Accuracy: 0.18555380403995514 |  Validation loss: 0.6392636789530062 | Validation Accuracy: 0.2645159661769867\n",
      "\t and the talks fall through . There are bound\n",
      "_bnddtehteaeet lallflhra  h..   eee ree  ed_\n",
      "Epoch 46: Train loss: 0.33313008002267314 | Train Accuracy: 0.18298421800136566 |  Validation loss: 0.6368436178561553 | Validation Accuracy: 0.26876503229141235\n",
      "\t They think there is something\n",
      "_They think there is something_\n",
      "Epoch 47: Train loss: 0.3245681374228524 | Train Accuracy: 0.18328502774238586 |  Validation loss: 0.6691415075567305 | Validation Accuracy: 0.26708292961120605\n",
      "\t crowd had collected . There were firemen\n",
      "_wrowd  ad  wnlsnleddy T.    i  e ef eee_\n",
      "Epoch 48: Train loss: 0.32166753338179754 | Train Accuracy: 0.1765562891960144 |  Validation loss: 0.6491553431402858 | Validation Accuracy: 0.26325270533561707\n",
      "\t herself ; we would be altogether clearer in our\n",
      "_fersylg   hehwouul ue   lr eioeeet  cre   nuo_\n",
      "Epoch 49: Train loss: 0.3143954165832977 | Train Accuracy: 0.17555338144302368 |  Validation loss: 0.6703328007385206 | Validation Accuracy: 0.26772215962409973\n",
      "\t pretty spineless ! \"\n",
      "_pretty spineless ! \"_\n",
      "Epoch 50: Train loss: 0.3105124079537663 | Train Accuracy: 0.17243105173110962 |  Validation loss: 0.6857622954506964 | Validation Accuracy: 0.2676253914833069\n",
      "\t Mr. Isaiah Roberts , landlord of The Traveller's Joy , rang up the\n",
      "_Mr. Isaiah Roberts , landlord of The Traveller's Joy , rang up the_\n",
      "Epoch 51: Train loss: 0.3046408619510531 | Train Accuracy: 0.1668536514043808 |  Validation loss: 0.6782934504778042 | Validation Accuracy: 0.2662648558616638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m recognizer \u001b[39m=\u001b[39m Recognizer()\n\u001b[0;32m      2\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train(recognizer\u001b[39m=\u001b[39;49mrecognizer, \n\u001b[0;32m      6\u001b[0m               train_line_dataset\u001b[39m=\u001b[39;49mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39;49mline_dataset_val, \n\u001b[0;32m      7\u001b[0m               batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, recognizer_lr\u001b[39m=\u001b[39;49m\u001b[39m5e-4\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m               betas\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0.999\u001b[39;49m), num_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, loss_balancing_alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(recognizer, train_line_dataset, val_line_dataset, batch_size, recognizer_lr, betas, num_epochs, loss_balancing_alpha)\u001b[0m\n\u001b[0;32m     64\u001b[0m test2 \u001b[39m=\u001b[39m recognizer_outputs[\u001b[39m0\u001b[39m,:,:]\n\u001b[0;32m     65\u001b[0m test2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(test2, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Removed 0 dim\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m test2 \u001b[39m=\u001b[39m test2[test2\u001b[39m.\u001b[39;49mnonzero()]\n\u001b[0;32m     67\u001b[0m test2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([int_to_char[\u001b[39mint\u001b[39m(i)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test2])\n\u001b[0;32m     70\u001b[0m recognizer_loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x00000184944B7A60> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:120\u001b[0m, in \u001b[0;36m_draw_all_if_interactive\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_draw_all_if_interactive\u001b[39m():\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m matplotlib\u001b[39m.\u001b[39mis_interactive():\n\u001b[1;32m--> 120\u001b[0m         draw_all()\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\_pylab_helpers.py:132\u001b[0m, in \u001b[0;36mGcf.draw_all\u001b[1;34m(cls, force)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m manager \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m force \u001b[39mor\u001b[39;00m manager\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mstale:\n\u001b[1;32m--> 132\u001b[0m         manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mdraw_idle()\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2082\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_idle_drawing:\n\u001b[0;32m   2081\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idle_draw_cntx():\n\u001b[1;32m-> 2082\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdraw(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[0;32m    398\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[0;32m    399\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[0;32m    401\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\figure.py:3140\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3137\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3140\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3141\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3143\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3144\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3061\u001b[0m \u001b[39mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3062\u001b[0m     _draw_rasterized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3064\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3065\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3067\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   3068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[0;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[0;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[0;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[0;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[0;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[1;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[0;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:561\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    557\u001b[0m     output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m output_alpha  \u001b[39m# recombine rgb and alpha\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39m# or an RGBA array of re-sampled input\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_rgba(output, \u001b[39mbytes\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, norm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    562\u001b[0m \u001b[39m# output is now a correctly sized RGBA array of uint8\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \n\u001b[0;32m    564\u001b[0m \u001b[39m# Apply alpha *after* if the input was greyscale without a mask\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\cm.py:494\u001b[0m, in \u001b[0;36mScalarMappable.to_rgba\u001b[1;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m norm:\n\u001b[0;32m    493\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m--> 494\u001b[0m rgba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcmap(x, alpha\u001b[39m=\u001b[39;49malpha, \u001b[39mbytes\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mbytes\u001b[39;49m)\n\u001b[0;32m    495\u001b[0m \u001b[39mreturn\u001b[39;00m rgba\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py:739\u001b[0m, in \u001b[0;36mColormap.__call__\u001b[1;34m(self, X, alpha, bytes)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m    737\u001b[0m     lut \u001b[39m=\u001b[39m (lut \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m--> 739\u001b[0m rgba \u001b[39m=\u001b[39m lut\u001b[39m.\u001b[39;49mtake(xa, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     alpha \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(alpha, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=8, recognizer_lr=5e-4,\n",
    "              betas=(0, 0.999), num_epochs=500, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "\n",
    "print(torch.softmax(recognizer(image.unsqueeze(0)), 1), torch.softmax(recognizer(image.unsqueeze(0)), 1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
