{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "from torchmetrics.text import CharErrorRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 7135\n"
     ]
    }
   ],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "320 10\n",
      "images\n",
      "320 10\n",
      "both\n",
      "16 4\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(16))\n",
    "line_dataset_val = Subset(line_dataset_val, range(4))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([66, 61,  1,  9, 17,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'to 19 .')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4,2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        #self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,2))\n",
    "        #self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(248, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.3)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        #print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.dense2(img)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "        \n",
    "    \n",
    "#recog = Recognizer()\n",
    "#a =recog(torch.randn((1, 1, 32, 512), dtype=torch.float32))\n",
    "#print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(recognizer, val_line_dataset_loader, recognizer_loss_function):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(val_line_dataset_loader, 0):\n",
    "        \n",
    "        recognizer_outputs = recognizer(line_image_batch)\n",
    "        recognizer_loss = recognizer_loss_function(F.log_softmax(recognizer_outputs, 1), line_text_batch)\n",
    "        \n",
    "        total_loss += recognizer_loss.item()\n",
    "        total_epoch += 1\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    \n",
    "    #print(recognizer_outputs, recognizer_outputs.shape)\n",
    "    #print(torch.argmax(recognizer_outputs, 1), torch.argmax(recognizer_outputs, 1).shape)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_recog_accuracy(preds, target):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the recognizer with character error rate\n",
    "    which is based on edit distance\n",
    "\n",
    "    Params:\n",
    "        preds: a list of prediction strings\n",
    "        targets: a list of target strings\n",
    "\n",
    "    Returns:\n",
    "        An integer, the character error rate average across\n",
    "        all predictions and targets\n",
    "    \"\"\"\n",
    "\n",
    "    cer = CharErrorRate()\n",
    "    return cer(preds, target)\n",
    "\n",
    "def create_strings_from_tensor(int_tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        int_tensor: A shape (N, 82) tensor where each row corresponds to\n",
    "        a integer mapping of a string. Includes padding\n",
    "    \n",
    "    Returns:\n",
    "        A list of N strings\n",
    "    \"\"\"\n",
    "\n",
    "    strings = []\n",
    "    for string_map in int_tensor:\n",
    "        strings.append(\"\".join([int_to_char[int(i)] for i in string_map[string_map != 0]]))\n",
    "    return strings\n",
    "    \n",
    "\n",
    "def get_accuracy(recognizer, recognizer_loader):\n",
    "\n",
    "    acc = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(recognizer_loader, 0):\n",
    "        \n",
    "        recognizer_outputs = torch.argmax(recognizer(line_image_batch), 1)\n",
    "        recognizer_pred = create_strings_from_tensor(recognizer_outputs)\n",
    "        \n",
    "        label = create_strings_from_tensor(line_text_batch)\n",
    "        \n",
    "        acc += calculate_recog_accuracy(recognizer_pred, label)\n",
    "        \n",
    "        \n",
    "    return acc / (i+1)\n",
    "        \n",
    "    \n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    \n",
    "    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n",
    "    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n",
    "    \n",
    "    n = len(train_loss) # number of epochs\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss*1000, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Train Loss * 1000\", \"Validation Loss\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Train vs Validation Accuracy\")\n",
    "    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"Train Accuracy\", \"Validation Accuracy\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    #print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #print(len(train_line_dataset_loader))\n",
    "\n",
    "    recognizer_optimizer = optim.Adam(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.NLLLoss()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = np.zeros(num_epochs)\n",
    "    recognizer_train_accuracies = np.zeros(num_epochs)\n",
    "    recognizer_val_losses = np.zeros(num_epochs)\n",
    "    recognizer_val_accuracies = np.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "            # print(line_text_batch.shape)\n",
    "\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            print(\"\\t\",test)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            #print(line_text_batch, line_text_batch.shape)\n",
    "            recognizer_outputs = recognizer(line_image_batch)  # Mult factor to incentivize padding\n",
    "   \n",
    "            # print(recognizer_outputs, recognizer_outputs.shape)\n",
    "            # print(line_text_batch, line_text_batch.shape)\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            #line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            #target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            #target = torch.cat(line_text_batch_pad_remove)\n",
    "            #print(target, target.shape)\n",
    "            #input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\n",
    "                F.log_softmax(recognizer_outputs, 1),  # Requires number of classes to move from 2nd to 1st dimension after log_softmax\n",
    "                line_text_batch\n",
    "            )\n",
    "            test2 = recognizer_outputs[0,:,:]\n",
    "            test2 = torch.argmax(test2, dim=0)  # Removed 0 dim\n",
    "            test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "            print(f\"_{test2}_\")\n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            recognizer_optimizer.zero_grad()\n",
    "    \n",
    "            recognizer_train_loss += recognizer_loss.item()\n",
    "        \n",
    "        recognizer_train_losses[epoch] = float(recognizer_train_loss) / (i+1)\n",
    "        recognizer_val_losses[epoch] = evaluate(recognizer, val_line_dataset_loader, recognizer_loss_function)\n",
    "        \n",
    "        recognizer_train_accuracies[epoch] = get_accuracy(recognizer, train_line_dataset_loader)\n",
    "        recognizer_val_accuracies[epoch]= get_accuracy(recognizer, val_line_dataset_loader)\n",
    "        \n",
    "        print((\"Epoch {}: Train loss: {} | Train Accuracy: {} | \"+\n",
    "            \" Validation loss: {} | Validation Accuracy: {}\").format(\n",
    "                    epoch + 1,\n",
    "                    recognizer_train_losses[epoch],\n",
    "                    recognizer_train_accuracies[epoch],\n",
    "                    recognizer_val_losses[epoch],\n",
    "                    recognizer_val_accuracies[epoch]))\n",
    "\n",
    "        model_path = get_model_name(recognizer.name, batch_size, recognizer_lr, epoch)\n",
    "        torch.save(recognizer.state_dict(), model_path)\n",
    "\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), recognizer_train_losses)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path),  recognizer_val_losses)\n",
    "    np.savetxt(\"{}_train_acc.csv\".format(model_path), recognizer_train_accuracies)\n",
    "    np.savetxt(\"{}_val_acc.csv\".format(model_path), recognizer_val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t mistresses . Anyway it might be much worse .\n",
      "_!!!!.s!!j!H!d!R1A!!!!!s!!Jk7nn!J !'LnD!!w4!cdcduu!cc!!! L;!7!!!2;R3JoJ!qo8c!sM_\n",
      "\t more than ever like a pink and gold\n",
      "_l;'njGl'lsll6l9u7'''luD'ljcEMSlzall!Jl'slwzlya!ly'l';jl's't!MjlsldlqfcW'lj\"ljj'''_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_ RsWJL ssD'sYs1LT;sl s ss 4qDc cs jj sllsT  !qllssssss llWsl  l'8 uyWLssj  l l_\n",
      "\t felt that part of her was leaving\n",
      "_ai HsjRl''izsq jzsa9asafn aaa_\n",
      "Epoch 1: Train loss: 4.255194067955017 | Train Accuracy: 0.9530504941940308 |  Validation loss: 4.078408241271973 | Validation Accuracy: 0.8910256624221802\n",
      "\t felt that part of her was leaving\n",
      "_G;4; ; jj'jn2n_\n",
      "\t tied . He dare not precipitate\n",
      "_4xtFfa_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_s\"o_\n",
      "\t to 19 .\n",
      "_  wsa_\n",
      "Epoch 2: Train loss: 3.698919713497162 | Train Accuracy: 0.9717342853546143 |  Validation loss: 2.8246302604675293 | Validation Accuracy: 0.9807692170143127\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_im_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ih_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Uli_\n",
      "\t more than ever like a pink and gold\n",
      "_U_\n",
      "Epoch 3: Train loss: 2.4895034432411194 | Train Accuracy: 0.9984471797943115 |  Validation loss: 2.388575315475464 | Validation Accuracy: 1.0\n",
      "\t peace of mind ? Philip put out\n",
      "_U_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_U_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_U _\n",
      "\t because the buoyancy of industry generates\n",
      "_ut_\n",
      "Epoch 4: Train loss: 2.100357860326767 | Train Accuracy: 0.952507495880127 |  Validation loss: 2.186330556869507 | Validation Accuracy: 0.9807692170143127\n",
      "\t simplicity , though nowadays their\n",
      "_U_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_Uo_\n",
      "\t peace of mind ? Philip put out\n",
      "_s   _\n",
      "\t to 19 .\n",
      "_e_\n",
      "Epoch 5: Train loss: 1.9912044405937195 | Train Accuracy: 0.97905433177948 |  Validation loss: 2.07853627204895 | Validation Accuracy: 0.9935897588729858\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_a_\n",
      "\t simplicity , though nowadays their\n",
      "__\n",
      "\t tremble ! What infatuation ! Personally\n",
      "__\n",
      "\t to 19 .\n",
      "__\n",
      "Epoch 6: Train loss: 1.9367621541023254 | Train Accuracy: 1.0 |  Validation loss: 2.000361919403076 | Validation Accuracy: 1.0\n",
      "\t because the buoyancy of industry generates\n",
      "__\n",
      "\t revered master at Ko\"then , near\n",
      "__\n",
      "\t tremble ! What infatuation ! Personally\n",
      "__\n",
      "\t tied . He dare not precipitate\n",
      "_t_\n",
      "Epoch 7: Train loss: 1.8617433905601501 | Train Accuracy: 0.9842028021812439 |  Validation loss: 1.9567357301712036 | Validation Accuracy: 0.9871794581413269\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_t_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_t_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "__\n",
      "\t simplicity , though nowadays their\n",
      "_ _\n",
      "Epoch 8: Train loss: 1.7687450647354126 | Train Accuracy: 0.9708665609359741 |  Validation loss: 1.9341892004013062 | Validation Accuracy: 0.9679487347602844\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_   _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_    _\n",
      "\t tied . He dare not precipitate\n",
      "_    _\n",
      "Epoch 9: Train loss: 1.7364084124565125 | Train Accuracy: 0.9169251322746277 |  Validation loss: 1.938401699066162 | Validation Accuracy: 0.9358974099159241\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_    _\n",
      "\t more than ever like a pink and gold\n",
      "_ _\n",
      "\t revered master at Ko\"then , near\n",
      "_    _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_       _\n",
      "Epoch 10: Train loss: 1.7120957374572754 | Train Accuracy: 0.8788539171218872 |  Validation loss: 1.9206042289733887 | Validation Accuracy: 0.8910256624221802\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_         _\n",
      "\t huh ? \"\n",
      "__\n",
      "\t simplicity , though nowadays their\n",
      "_          _\n",
      "Epoch 11: Train loss: 1.6938936412334442 | Train Accuracy: 0.858400285243988 |  Validation loss: 1.8687199354171753 | Validation Accuracy: 0.8846153616905212\n",
      "\t slowly , \" If you must drag the truth\n",
      "_              _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_t   t    _\n",
      "\t tied . He dare not precipitate\n",
      "_       _\n",
      "\t huh ? \"\n",
      "__\n",
      "Epoch 12: Train loss: 1.6575250923633575 | Train Accuracy: 0.8719103336334229 |  Validation loss: 1.8854824304580688 | Validation Accuracy: 0.9038461446762085\n",
      "\t to 19 .\n",
      "__\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_t       _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_t t         _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ttt  t    t ttt      _\n",
      "Epoch 13: Train loss: 1.6166787445545197 | Train Accuracy: 0.8426528573036194 |  Validation loss: 1.8452928066253662 | Validation Accuracy: 0.8589743375778198\n",
      "\t felt that part of her was leaving\n",
      "_t        _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_             _\n",
      "\t huh ? \"\n",
      "__\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_  t                      _\n",
      "Epoch 14: Train loss: 1.608742356300354 | Train Accuracy: 0.8364440202713013 |  Validation loss: 1.8147029876708984 | Validation Accuracy: 0.8461538553237915\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_                   _\n",
      "\t to 19 .\n",
      "__\n",
      "\t tied . He dare not precipitate\n",
      "_                  _\n",
      "\t simplicity , though nowadays their\n",
      "_                        _\n",
      "Epoch 15: Train loss: 1.57170969247818 | Train Accuracy: 0.8377760648727417 |  Validation loss: 1.8179692029953003 | Validation Accuracy: 0.8333333134651184\n",
      "\t peace of mind ? Philip put out\n",
      "_               _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_               _\n",
      "\t tied . He dare not precipitate\n",
      "_               _\n",
      "\t huh ? \"\n",
      "_  _\n",
      "Epoch 16: Train loss: 1.5814608335494995 | Train Accuracy: 0.8269690275192261 |  Validation loss: 1.8245092630386353 | Validation Accuracy: 0.8333333134651184\n",
      "\t slowly , \" If you must drag the truth\n",
      "_t                       _\n",
      "\t revered master at Ko\"then , near\n",
      "_t                          _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_t  t t       t              _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_t  t                        _\n",
      "Epoch 17: Train loss: 1.5627128779888153 | Train Accuracy: 0.81883704662323 |  Validation loss: 1.7947649955749512 | Validation Accuracy: 0.8205128312110901\n",
      "\t more than ever like a pink and gold\n",
      "_                    _\n",
      "\t simplicity , though nowadays their\n",
      "_t                          _\n",
      "\t peace of mind ? Philip put out\n",
      "_  t                   _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_t t      t t t      t    _\n",
      "Epoch 18: Train loss: 1.5155183970928192 | Train Accuracy: 0.8249591588973999 |  Validation loss: 1.7832576036453247 | Validation Accuracy: 0.8269230723381042\n",
      "\t peace of mind ? Philip put out\n",
      "_                     _\n",
      "\t revered master at Ko\"then , near\n",
      "_                       _\n",
      "\t felt that part of her was leaving\n",
      "_                    _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_                         _\n",
      "Epoch 19: Train loss: 1.4867216050624847 | Train Accuracy: 0.8385069370269775 |  Validation loss: 1.7647982835769653 | Validation Accuracy: 0.8333333134651184\n",
      "\t simplicity , though nowadays their\n",
      "_                            _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                            _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                               _\n",
      "\t because the buoyancy of industry generates\n",
      "_  _\n",
      "Epoch 20: Train loss: 1.4872763752937317 | Train Accuracy: 0.836335301399231 |  Validation loss: 1.7480474710464478 | Validation Accuracy: 0.8333333134651184\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                            _\n",
      "\t more than ever like a pink and gold\n",
      "_          _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                               _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_                              _\n",
      "Epoch 21: Train loss: 1.4618135392665863 | Train Accuracy: 0.8334983587265015 |  Validation loss: 1.7362185716629028 | Validation Accuracy: 0.8333333134651184\n",
      "\t peace of mind ? Philip put out\n",
      "_                         _\n",
      "\t revered master at Ko\"then , near\n",
      "_                            _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                               _\n",
      "\t to 19 .\n",
      "_ _\n",
      "Epoch 22: Train loss: 1.449979692697525 | Train Accuracy: 0.8265907764434814 |  Validation loss: 1.7592453956604004 | Validation Accuracy: 0.8333333134651184\n",
      "\t simplicity , though nowadays their\n",
      "_                               _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_        _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_                          _\n",
      "\t peace of mind ? Philip put out\n",
      "_                         _\n",
      "Epoch 23: Train loss: 1.4735398292541504 | Train Accuracy: 0.8184186220169067 |  Validation loss: 1.7609727382659912 | Validation Accuracy: 0.8205128312110901\n",
      "\t simplicity , though nowadays their\n",
      "_t                            _\n",
      "\t because the buoyancy of industry generates\n",
      "_  _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_                                _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_                             _\n",
      "Epoch 24: Train loss: 1.45453479886055 | Train Accuracy: 0.8304526805877686 |  Validation loss: 1.739974021911621 | Validation Accuracy: 0.8333333134651184\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_               _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "\t revered master at Ko\"then , near\n",
      "_                                _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_                                 _\n",
      "Epoch 25: Train loss: 1.4409094154834747 | Train Accuracy: 0.8239879608154297 |  Validation loss: 1.7309892177581787 | Validation Accuracy: 0.8333333134651184\n",
      "\t because the buoyancy of industry generates\n",
      "_        _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "\t to 19 .\n",
      "_  _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_t                           t       _\n",
      "Epoch 26: Train loss: 1.437181755900383 | Train Accuracy: 0.8221348524093628 |  Validation loss: 1.7286529541015625 | Validation Accuracy: 0.8141025900840759\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_t                               _\n",
      "\t simplicity , though nowadays their\n",
      "_                                _\n",
      "\t more than ever like a pink and gold\n",
      "_                       _\n",
      "\t to 19 .\n",
      "_ _\n",
      "Epoch 27: Train loss: 1.4055677950382233 | Train Accuracy: 0.8289105892181396 |  Validation loss: 1.747933030128479 | Validation Accuracy: 0.8205128312110901\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                          _\n",
      "\t felt that part of her was leaving\n",
      "_                          _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ e                         _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "Epoch 28: Train loss: 1.4139628410339355 | Train Accuracy: 0.8224743604660034 |  Validation loss: 1.7502458095550537 | Validation Accuracy: 0.8269230723381042\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                 _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                                      _\n",
      "\t revered master at Ko\"then , near\n",
      "_                               _\n",
      "\t tied . He dare not precipitate\n",
      "_                              _\n",
      "Epoch 29: Train loss: 1.3998262882232666 | Train Accuracy: 0.8295707702636719 |  Validation loss: 1.726845383644104 | Validation Accuracy: 0.8269230723381042\n",
      "\t to 19 .\n",
      "_   _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                              _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "\t simplicity , though nowadays their\n",
      "_                               _\n",
      "Epoch 30: Train loss: 1.3819585591554642 | Train Accuracy: 0.826255738735199 |  Validation loss: 1.746896505355835 | Validation Accuracy: 0.8333333134651184\n",
      "\t because the buoyancy of industry generates\n",
      "_        _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_                             _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                                    _\n",
      "\t more than ever like a pink and gold\n",
      "_                             _\n",
      "Epoch 31: Train loss: 1.3723473250865936 | Train Accuracy: 0.8197138905525208 |  Validation loss: 1.7758889198303223 | Validation Accuracy: 0.807692289352417\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "\t huh ? \"\n",
      "_      _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_     e                       _\n",
      "\t tied . He dare not precipitate\n",
      "_                            _\n",
      "Epoch 32: Train loss: 1.3478685766458511 | Train Accuracy: 0.8127804398536682 |  Validation loss: 1.7304049730300903 | Validation Accuracy: 0.8141025900840759\n",
      "\t huh ? \"\n",
      "_      _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_       e    e                  _\n",
      "\t simplicity , though nowadays their\n",
      "_                       e        _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                     e        _\n",
      "Epoch 33: Train loss: 1.342421442270279 | Train Accuracy: 0.8119295835494995 |  Validation loss: 1.7843929529190063 | Validation Accuracy: 0.8012820482254028\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_                            _\n",
      "\t huh ? \"\n",
      "_      _\n",
      "\t revered master at Ko\"then , near\n",
      "_ e                         e _\n",
      "\t because the buoyancy of industry generates\n",
      "_         _\n",
      "Epoch 34: Train loss: 1.3248607218265533 | Train Accuracy: 0.8180278539657593 |  Validation loss: 1.8089885711669922 | Validation Accuracy: 0.8205128312110901\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_     e                       _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_       e  ee e e      e_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                     e         _\n",
      "\t felt that part of her was leaving\n",
      "_            e                _\n",
      "Epoch 35: Train loss: 1.3221206068992615 | Train Accuracy: 0.7955048084259033 |  Validation loss: 1.7951730489730835 | Validation Accuracy: 0.807692289352417\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_                     e          _\n",
      "\t simplicity , though nowadays their\n",
      "_                       e        _\n",
      "\t huh ? \"\n",
      "_      e_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                 ee ee     _\n",
      "Epoch 36: Train loss: 1.3037812411785126 | Train Accuracy: 0.806739091873169 |  Validation loss: 1.7911933660507202 | Validation Accuracy: 0.7948718070983887\n",
      "\t slowly , \" If you must drag the truth\n",
      "_  e                               e  _\n",
      "\t tied . He dare not precipitate\n",
      "_    e                      _\n",
      "\t huh ? \"\n",
      "_       _\n",
      "\t to 19 .\n",
      "_   _\n",
      "Epoch 37: Train loss: 1.2908246517181396 | Train Accuracy: 0.7947203516960144 |  Validation loss: 1.9020830392837524 | Validation Accuracy: 0.7948718070983887\n",
      "\t peace of mind ? Philip put out\n",
      "_            e                _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_ e   e                         _\n",
      "\t felt that part of her was leaving\n",
      "_                          _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_e        e      e        e _\n",
      "Epoch 38: Train loss: 1.2813767194747925 | Train Accuracy: 0.7898362874984741 |  Validation loss: 1.8369449377059937 | Validation Accuracy: 0.8141025900840759\n",
      "\t slowly , \" If you must drag the truth\n",
      "_           e     e                _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_                                _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_  h                ee         ss_\n",
      "\t peace of mind ? Philip put out\n",
      "_                             _\n",
      "Epoch 39: Train loss: 1.2936790883541107 | Train Accuracy: 0.7938375473022461 |  Validation loss: 1.8448137044906616 | Validation Accuracy: 0.7820512652397156\n",
      "\t to 19 .\n",
      "_ e  _\n",
      "\t more than ever like a pink and gold\n",
      "_ ee      eee    e       e    e  e_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_       e                       _\n",
      "\t simplicity , though nowadays their\n",
      "_    a                  e         _\n",
      "Epoch 40: Train loss: 1.2732767313718796 | Train Accuracy: 0.8029381036758423 |  Validation loss: 1.8699555397033691 | Validation Accuracy: 0.8012820482254028\n",
      "\t tied . He dare not precipitate\n",
      "_    e                      _\n",
      "\t more than ever like a pink and gold\n",
      "_               a  a  a _\n",
      "\t because the buoyancy of industry generates\n",
      "_          e        _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_    a         aa         a   _\n",
      "Epoch 41: Train loss: 1.282286450266838 | Train Accuracy: 0.787161111831665 |  Validation loss: 1.8907984495162964 | Validation Accuracy: 0.8012820482254028\n",
      "\t tied . He dare not precipitate\n",
      "_ ee     e e                 _\n",
      "\t felt that part of her was leaving\n",
      "_e      e         e       e  _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_               h                 e       _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_                                     e _\n",
      "Epoch 42: Train loss: 1.2358573973178864 | Train Accuracy: 0.7839663624763489 |  Validation loss: 1.8048032522201538 | Validation Accuracy: 0.7820512652397156\n",
      "\t huh ? \"\n",
      "_      e_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_           e                          _\n",
      "\t because the buoyancy of industry generates\n",
      "_ e                      e       _\n",
      "\t revered master at Ko\"then , near\n",
      "_ e   ee        e   e    e e     _\n",
      "Epoch 43: Train loss: 1.2669161558151245 | Train Accuracy: 0.7831885814666748 |  Validation loss: 1.8030983209609985 | Validation Accuracy: 0.7948718070983887\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_     h    e                   eeee _\n",
      "\t tied . He dare not precipitate\n",
      "_ ee      e         e       _\n",
      "\t peace of mind ? Philip put out\n",
      "_            e     e  e e  e  _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ et h  e h     e      e e   _\n",
      "Epoch 44: Train loss: 1.2163927853107452 | Train Accuracy: 0.7732809782028198 |  Validation loss: 1.9724924564361572 | Validation Accuracy: 0.7820512652397156\n",
      "\t simplicity , though nowadays their\n",
      "_   eee        e   ee  e   e _\n",
      "\t peace of mind ? Philip put out\n",
      "_       e  e      ee ee  e  _\n",
      "\t more than ever like a pink and gold\n",
      "_ e    e e a   e      e    e _\n",
      "\t because the buoyancy of industry generates\n",
      "_e      e        a   e _\n",
      "Epoch 45: Train loss: 1.2135913372039795 | Train Accuracy: 0.7644978761672974 |  Validation loss: 1.893403172492981 | Validation Accuracy: 0.807692289352417\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_ee        e e  e ee    e     e_\n",
      "\t peace of mind ? Philip put out\n",
      "_e      e    e     ee e e  e  _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ e   e         ee   e  e     e eeeeee e _\n",
      "\t more than ever like a pink and gold\n",
      "_ re       e    e       e    e  e_\n",
      "Epoch 46: Train loss: 1.190027505159378 | Train Accuracy: 0.7762975692749023 |  Validation loss: 1.884401798248291 | Validation Accuracy: 0.7820512652397156\n",
      "\t felt that part of her was leaving\n",
      "_e      e    e     e  e    ee _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ eet e  e ee   e e      e e   te_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_ee  a     e e  e ee  e  ee    eee_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_              e         e   h   eee eee _\n",
      "Epoch 47: Train loss: 1.1842625439167023 | Train Accuracy: 0.7609539031982422 |  Validation loss: 1.8977071046829224 | Validation Accuracy: 0.7820512652397156\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_      a     e   e     eea  o  a  a     a  e e   _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_     a         feh      e       e eee ee e    e     _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  ae    o e  e ee    o    ae_\n",
      "\t felt that part of her was leaving\n",
      "_h     h   r   h  a e   hh_\n",
      "Epoch 48: Train loss: 1.1680275350809097 | Train Accuracy: 0.7552425861358643 |  Validation loss: 2.043884038925171 | Validation Accuracy: 0.7692307829856873\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_ hh    he        hhh    h h  s   _\n",
      "\t felt that part of her was leaving\n",
      "_h        h      ee e    e  _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ e     h      hh   h  eh    eeeeeeeeeeeee   _\n",
      "\t more than ever like a pink and gold\n",
      "_    e   e   e        e  e _\n",
      "Epoch 49: Train loss: 1.1726849228143692 | Train Accuracy: 0.7605324983596802 |  Validation loss: 1.9486498832702637 | Validation Accuracy: 0.7820512652397156\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ e      e et     e      e t   e  e  _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_     e  h      ffh      h        e ef esee f s  f ff    _\n",
      "\t revered master at Ko\"then , near\n",
      "_eeeeeee  e  ee     ee   e e e  e_\n",
      "\t tied . He dare not precipitate\n",
      "_ ee     eee   e   e e e     _\n",
      "Epoch 50: Train loss: 1.1374508440494537 | Train Accuracy: 0.7560406923294067 |  Validation loss: 1.9646377563476562 | Validation Accuracy: 0.7692307829856873\n",
      "\t felt that part of her was leaving\n",
      "_h     ee    e     ee e e  ee e_\n",
      "\t because the buoyancy of industry generates\n",
      "_e  e   e    e   e e  ee eeee e _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_tet ae eaa    aa  e  eee   de  eeee _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_teee    a  e a   ee   ae a   e  ee  _\n",
      "Epoch 51: Train loss: 1.1530885845422745 | Train Accuracy: 0.7467058897018433 |  Validation loss: 1.923614501953125 | Validation Accuracy: 0.7884615659713745\n",
      "\t to 19 .\n",
      "_  e _\n",
      "\t tied . He dare not precipitate\n",
      "_ eeo o te ee  ee  eeee e  e  _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_t e t      h      h    e a      r   _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_h  a e   o a  e eee e  e  e ae_\n",
      "Epoch 52: Train loss: 1.134152039885521 | Train Accuracy: 0.8031805157661438 |  Validation loss: 1.913108229637146 | Validation Accuracy: 0.7756410241127014\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_  e he        a        e   ae  eee   _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aee   d     a   ea   neaa    a  a        e   _\n",
      "\t more than ever like a pink and gold\n",
      "_ ee      aeh eea       e a e e ee_\n",
      "\t to 19 .\n",
      "_  r _\n",
      "Epoch 53: Train loss: 1.1318447589874268 | Train Accuracy: 0.7538145184516907 |  Validation loss: 2.039900064468384 | Validation Accuracy: 0.7884615659713745\n",
      "\t revered master at Ko\"then , near\n",
      "_eeehe  e  eh a    he a e ee_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_te t     ah a   eh   a a   e    _\n",
      "\t because the buoyancy of industry generates\n",
      "_e  e        n   e na e  eee a _\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee   e     ee eee  e  _\n",
      "Epoch 54: Train loss: 1.119892343878746 | Train Accuracy: 0.7285732626914978 |  Validation loss: 2.044780969619751 | Validation Accuracy: 0.7884615659713745\n",
      "\t felt that part of her was leaving\n",
      "_o     ee  e   e  ee e   eee_\n",
      "\t simplicity , though nowadays their\n",
      "_   eee        oh  ee e         _\n",
      "\t tied . He dare not precipitate\n",
      "_ e   a eo      e e      _\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee   e     eeeeee  e  _\n",
      "Epoch 55: Train loss: 1.0878648459911346 | Train Accuracy: 0.7308735251426697 |  Validation loss: 2.039485454559326 | Validation Accuracy: 0.7692307829856873\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_tet re   a    aa     eee   re  eee  _\n",
      "\t tied . He dare not precipitate\n",
      "_ eeo   e ee  ee  eeeeee  e  _\n",
      "\t simplicity , though nowadays their\n",
      "_   h e       eoon  eo  h   e ae_\n",
      "\t more than ever like a pink and gold\n",
      "_ ei  eae ea eee  a e ee a d e _\n",
      "Epoch 56: Train loss: 1.0766911655664444 | Train Accuracy: 0.724980354309082 |  Validation loss: 2.074751615524292 | Validation Accuracy: 0.7756410241127014\n",
      "\t tied . He dare not precipitate\n",
      "_ e   e ee  ie  eeeeee  e  _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ etae   hh     ea ie  e h   t_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  a     h a   ahh r  h  h a _\n",
      "\t simplicity , though nowadays their\n",
      "_te h e       hh  hr  h   h a _\n",
      "Epoch 57: Train loss: 1.0621114373207092 | Train Accuracy: 0.707423210144043 |  Validation loss: 2.1526286602020264 | Validation Accuracy: 0.7564102411270142\n",
      "\t tied . He dare not precipitate\n",
      "_ e  te ee  ee  eeeeeete e_\n",
      "\t simplicity , though nowadays their\n",
      "_   hfe       hn  hh  h   h   _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ e ea  h  t t h   ee eh   teeeeeeeeeee ee_\n",
      "\t because the buoyancy of industry generates\n",
      "_e  e        n   e d a e eeee a _\n",
      "Epoch 58: Train loss: 1.0638747215270996 | Train Accuracy: 0.7070360779762268 |  Validation loss: 2.133171319961548 | Validation Accuracy: 0.7692307829856873\n",
      "\t revered master at Ko\"then , near\n",
      "_eeehe  e  eh a   oe hee a e ee_\n",
      "\t felt that part of her was leaving\n",
      "_o  t  ee  e   ef  ee e   eee_\n",
      "\t tied . He dare not precipitate\n",
      "_ ee  ee ee  ee  eeeeeee eee_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_teetre   a    roa  e   eo   fe  eee eee _\n",
      "Epoch 59: Train loss: 1.041169211268425 | Train Accuracy: 0.6862394213676453 |  Validation loss: 2.0675697326660156 | Validation Accuracy: 0.7564102411270142\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ eetare e eh     e  ee te e terteeee_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_teoe a a ar aa eh   rag t e  re e_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aee   de   hee  a   nredh o  a ae  e   o  eee   _\n",
      "\t more than ever like a pink and gold\n",
      "_ ere   ae aey oea  a  eee aa aoa_\n",
      "Epoch 60: Train loss: 1.0122760385274887 | Train Accuracy: 0.6840585470199585 |  Validation loss: 2.117067813873291 | Validation Accuracy: 0.7564102411270142\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ eeef  h tt   fff  he eh   teeieeess fiifsifeffi s _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retffe f hh   heea he th h hhtir_\n",
      "\t huh ? \"\n",
      "_ e    _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aee  ad  a hd    h aaadh   a aa  n     nea  _\n",
      "Epoch 61: Train loss: 1.0097038745880127 | Train Accuracy: 0.6774154901504517 |  Validation loss: 2.1737492084503174 | Validation Accuracy: 0.7884615659713745\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee   e   e  eeeeee  e  _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_ e  he   h .  hhda      eeh  fh  eee e e _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aeed   d    h    h  hnoadh     aa  n           _\n",
      "\t because the buoyancy of industry generates\n",
      "_e  e        n   e d a ee eeee e _\n",
      "Epoch 62: Train loss: 0.9904884845018387 | Train Accuracy: 0.6904929876327515 |  Validation loss: 2.1602749824523926 | Validation Accuracy: 0.7756410241127014\n",
      "\t more than ever like a pink and gold\n",
      "_ eee    e ee eea  a  ene a d eoed_\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee  ee   e  eeeeee  e  _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retffe f hh tth ea h tth hthtterr_\n",
      "\t because the buoyancy of industry generates\n",
      "_e ee     n  na n e dna ne eeee ae_\n",
      "Epoch 63: Train loss: 0.9797956198453903 | Train Accuracy: 0.6480079293251038 |  Validation loss: 2.233482837677002 | Validation Accuracy: 0.7564102411270142\n",
      "\t more than ever like a pink and gold\n",
      "_ e  teae a  n an a n nn aad a_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aen tada a h   trh aan dh  aa aa an     nea e_\n",
      "\t huh ? \"\n",
      "_ ee r _\n",
      "\t tied . He dare not precipitate\n",
      "_ ee  te er  e  eeee  e e_\n",
      "Epoch 64: Train loss: 0.9671781361103058 | Train Accuracy: 0.6705230474472046 |  Validation loss: 2.2620041370391846 | Validation Accuracy: 0.7628205418586731\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_hhf afa heh   a   rrrr oahd hhr  sh  hs _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  aer h   a   hh n y    a _\n",
      "\t tied . He dare not precipitate\n",
      "_ ee   e er  ee  eeee  e e_\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee   e     eeeeee  e  _\n",
      "Epoch 65: Train loss: 0.9461140930652618 | Train Accuracy: 0.6543035507202148 |  Validation loss: 2.20479416847229 | Validation Accuracy: 0.7564102411270142\n",
      "\t felt that part of her was leaving\n",
      "_oeet tee  e   r  ee ee eeeve_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  aeh h h a   ehdh rn h  h an_\n",
      "\t simplicity , though nowadays their\n",
      "_tetreeath    hngn  roatg   ra e_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ etareththh t e eatrette htherteee_\n",
      "Epoch 66: Train loss: 0.939082682132721 | Train Accuracy: 0.6063588261604309 |  Validation loss: 2.2772574424743652 | Validation Accuracy: 0.7884615659713745\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ee teetttetteeeeeettee _\n",
      "\t revered master at Ko\"then , near\n",
      "_feere  h  ehha  hdehha  a haer_\n",
      "\t felt that part of her was leaving\n",
      "_oee   ee      e  ee ee eeeve_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  aer e t a   eer r  e  e an_\n",
      "Epoch 67: Train loss: 0.9103248566389084 | Train Accuracy: 0.6417933702468872 |  Validation loss: 2.285043478012085 | Validation Accuracy: 0.7948718070983887\n",
      "\t tied . He dare not precipitate\n",
      "_ ee d  e er  ee  re eei  i e_\n",
      "\t revered master at Ko\"then , near\n",
      "_eeere  a  er a   oethan a eaar_\n",
      "\t huh ? \"\n",
      "_ i    _\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aeid  ade a h    n aenr dh  aa aa  n     nea  _\n",
      "Epoch 68: Train loss: 0.9327214360237122 | Train Accuracy: 0.6388537883758545 |  Validation loss: 2.2601194381713867 | Validation Accuracy: 0.75\n",
      "\t revered master at Ko\"then , near\n",
      "_eeere     ah a  teethan   ea e_\n",
      "\t because the buoyancy of industry generates\n",
      "_deaaee  ya di  ny  indua e eeee a _\n",
      "\t simplicity , though nowadays their\n",
      "_te hee, h    hygp  oo eh   ha _\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_hee eee e h a  eeegh on e  a an_\n",
      "Epoch 69: Train loss: 0.891849160194397 | Train Accuracy: 0.6267882585525513 |  Validation loss: 2.3512609004974365 | Validation Accuracy: 0.7756410241127014\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aend tade a h   th aenadh  ae ae  u    eneeee_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ eethhe h hh   eeoa na th h hertneee_\n",
      "\t because the buoyancy of industry generates\n",
      "_deeeee  ye din no  endea o eoea a _\n",
      "\t tied . He dare not precipitate\n",
      "_ ee d  e er  ie  roeere e e_\n",
      "Epoch 70: Train loss: 0.8701221197843552 | Train Accuracy: 0.619078516960144 |  Validation loss: 2.469041109085083 | Validation Accuracy: 0.7820512652397156\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  aar a   at ahh oh r  htan_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retff  h hh t h ea ra th hthhrtnrr_\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  eh   e   r  reteee  r  _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_hhd ada whah  ha  ahhrn oehd hhr r h  hs _\n",
      "Epoch 71: Train loss: 0.8499642312526703 | Train Accuracy: 0.6152160167694092 |  Validation loss: 2.419102668762207 | Validation Accuracy: 0.7628205418586731\n",
      "\t more than ever like a pink and gold\n",
      "_ ere   an eer e e  a nine a d eged_\n",
      "\t because the buoyancy of industry generates\n",
      "_deead e     dia  n    i d a ee eeee ee_\n",
      "\t felt that part of her was leaving\n",
      "_feet tfe tn   rftterereeteeeve_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_hhd ada wheh tha  thhhh hhhd hhssfsshsdshsf_\n",
      "Epoch 72: Train loss: 0.8482026010751724 | Train Accuracy: 0.559916079044342 |  Validation loss: 2.361086130142212 | Validation Accuracy: 0.8012820482254028\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_he  aer e   at eehhton e  etan_\n",
      "\t simplicity , though nowadays their\n",
      "_metheehth    hngp  ro eht  ha e_\n",
      "\t peace of mind ? Philip put out\n",
      "_heee  ef tee t e tfeeefe  re _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_trdlp , g If aa eha  .rag tha  re h_\n",
      "Epoch 73: Train loss: 0.8156201392412186 | Train Accuracy: 0.5747218132019043 |  Validation loss: 2.443434238433838 | Validation Accuracy: 0.7692307829856873\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_ es hiea a . Ihyoay e   egh  .e  eae n e _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ etof  p hh t n oa na ty h hertnn_\n",
      "\t revered master at Ko\"then , near\n",
      "_feere  a  ah a   oothan , haar_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_tovdai a g ao eoa hha  drad hae  e h_\n",
      "Epoch 74: Train loss: 0.8080470263957977 | Train Accuracy: 0.558419942855835 |  Validation loss: 2.4724280834198 | Validation Accuracy: 0.7820512652397156\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aeid tade a ha n an arn dh aaa aa  n    nneaee_\n",
      "\t felt that part of her was leaving\n",
      "_feee tee  e   if  ereree ieevee_\n",
      "\t tied . He dare not precipitate\n",
      "_ ee g te ere ie  poeeepitete_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retfretfthh tthtf thettrh hthertreeer_\n",
      "Epoch 75: Train loss: 0.8053009659051895 | Train Accuracy: 0.5588692426681519 |  Validation loss: 2.5141072273254395 | Validation Accuracy: 0.7628205418586731\n",
      "\t tied . He dare not precipitate\n",
      "_ ee g te eretiet peeipitite_\n",
      "\t peace of mind ? Philip put out\n",
      "_reee  ef tee tiitteeeeeet le _\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aed alet.hah th t arrrp oerd hee i re h  _\n",
      "\t to 19 .\n",
      "_t   _\n",
      "Epoch 76: Train loss: 0.7646259218454361 | Train Accuracy: 0.5558455586433411 |  Validation loss: 2.5644891262054443 | Validation Accuracy: 0.7692307829856873\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aed ala whah tha  arrrp oehd hee e he h  _\n",
      "\t tied . He dare not precipitate\n",
      "_ ee g te ere ie  peipe e e_\n",
      "\t more than ever like a pink and gold\n",
      "_ re  han aar neaa a ninn a d god_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aend eada a h n  e ahpradh   a ae  n    nnee e_\n",
      "Epoch 77: Train loss: 0.7487843036651611 | Train Accuracy: 0.525956928730011 |  Validation loss: 2.5680952072143555 | Validation Accuracy: 0.7692307829856873\n",
      "\t peace of mind ? Philip put out\n",
      "_reee  eh  ee  i  eeeeee  ee _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ eyufg tothheetfff the rhtt treeesf he e  f e-effe s _\n",
      "\t to 19 .\n",
      "_tr   _\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retff  hthh tte eatratth hthertnnr_\n",
      "Epoch 78: Train loss: 0.7232489138841629 | Train Accuracy: 0.46478408575057983 |  Validation loss: 2.6179144382476807 | Validation Accuracy: 0.7884615659713745\n",
      "\t huh ? \"\n",
      "_ et h e_\n",
      "\t revered master at Ko\"then , near\n",
      "_fevered ta  ehtat toethen , haar_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ eetlfe f hhat eieatratth hthertieen_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_ard aratohah that arrrn oeod hee nshed hss_\n",
      "Epoch 79: Train loss: 0.7366855293512344 | Train Accuracy: 0.48043543100357056 |  Validation loss: 2.650794506072998 | Validation Accuracy: 0.7628205418586731\n",
      "\t huh ? \"\n",
      "_ ee h e_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_ ordlp , g If ee th   drag the  oo h_\n",
      "\t felt that part of her was leaving\n",
      "_feee  ee  r r  if te rei ieeve_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_aeud made a h ntive ahpreach o  e ae  uut te incree e_\n",
      "Epoch 80: Train loss: 0.7028998360037804 | Train Accuracy: 0.4635583460330963 |  Validation loss: 2.5980277061462402 | Validation Accuracy: 0.7884615659713745\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ euipu tf hh etfoff the ehtt  hecessef hsef efs fsef-effef f f_\n",
      "\t because the buoyancy of industry generates\n",
      "_decaase  ha cnoyendy  indiaton enee e _\n",
      "\t simplicity , though nowadays their\n",
      "_sithgec,ty ,  hugp nrwa yt  ha _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_ i triaehh .thhhc,h  t tight fh tiaf cor e _\n",
      "Epoch 81: Train loss: 0.6854750961065292 | Train Accuracy: 0.47161853313446045 |  Validation loss: 2.6539361476898193 | Validation Accuracy: 0.7884615659713745\n",
      "\t peace of mind ? Philip put out\n",
      "_peee eef tee tit eeeeeet le _\n",
      "\t because the buoyancy of industry generates\n",
      "_decae e  ya dnoyendy  induaton eea e _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_ e triaeta . hhyaah e  tught fe tiae wr e _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_ rvdlp , g If ee ey   drag the tro r_\n",
      "Epoch 82: Train loss: 0.6836996525526047 | Train Accuracy: 0.44563451409339905 |  Validation loss: 2.71349835395813 | Validation Accuracy: 0.7628205418586731\n",
      "\t felt that part of her was leaving\n",
      "_fefa tra  h r  rf ter ree ieevl_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retol  h hh tth eathatth hthertnnr_\n",
      "\t peace of mind ? Philip put out\n",
      "_reea  ef tae    tfehtae hlae_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aed hlatchah that ahrrn othd hee eshed hss_\n",
      "Epoch 83: Train loss: 0.6693073660135269 | Train Accuracy: 0.4672248363494873 |  Validation loss: 2.8329970836639404 | Validation Accuracy: 0.7756410241127014\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_hes aer e t at  etdhtln et e and_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ eyipg to hhee -ff the estt treessf hnn ic  f e-effec s -_\n",
      "\t more than ever like a pink and gold\n",
      "_ ere than ever niaa a pine and ged_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_sovflf , g If ewe mu   drag the  e h_\n",
      "Epoch 84: Train loss: 0.6680746972560883 | Train Accuracy: 0.4148397743701935 |  Validation loss: 2.827022075653076 | Validation Accuracy: 0.7692307829856873\n",
      "\t slowly , \" If you must drag the truth\n",
      "_ dpt, g ,d uoa tu   drag the  e h_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retfl  f hhat s fathattrh h hertlisen_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aed aletlhah tt t arrrp ouhd yevi i te ni _\n",
      "\t peace of mind ? Philip put out\n",
      "_hiea  lf tae t i tlee fe  lt _\n",
      "Epoch 85: Train loss: 0.6656097918748856 | Train Accuracy: 0.3917519450187683 |  Validation loss: 2.746460437774658 | Validation Accuracy: 0.807692289352417\n",
      "\t more than ever like a pink and gold\n",
      "_ ere   an eveh iiaa a pine aid eoedroo_\n",
      "\t revered master at Ko\"then , near\n",
      "_eevered taa artat toethen , eaar_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_sordlp , g If ew  mu t drag the touth_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_sesihesa a . fhhwah e  i oht fe  icf worse _\n",
      "Epoch 86: Train loss: 0.6259389072656631 | Train Accuracy: 0.4361119270324707 |  Validation loss: 2.8930702209472656 | Validation Accuracy: 0.7692307829856873\n",
      "\t huh ? \"\n",
      "_ eh h _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_serdrp , g If eyp ay t wr g the trnth_\n",
      "\t more than ever like a pink and gold\n",
      "_ re than ever iae a pine and od_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retel  p rhat n eatrattp t rer ne_\n",
      "Epoch 87: Train loss: 0.6019052714109421 | Train Accuracy: 0.3783102035522461 |  Validation loss: 2.957031726837158 | Validation Accuracy: 0.7820512652397156\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retfl  f hh t h fa hahth h her   er_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_ erdlp , \" If eo  tu   drag the t  h_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_sestresees . Ahywah  t teght fe ticf wor e _\n",
      "\t simplicity , though nowadays their\n",
      "_simpgec,tn ,  hngp  owadeyt  ha o_\n",
      "Epoch 88: Train loss: 0.5681034550070763 | Train Accuracy: 0.35509419441223145 |  Validation loss: 2.912865161895752 | Validation Accuracy: 0.7884615659713745\n",
      "\t huh ? \"\n",
      "_tit t e_\n",
      "\t because the buoyancy of industry generates\n",
      "_becaesd tya cnoysndy oy induaton eeretea_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_hes aer a t at eeughtlt et ,tand_\n",
      "\t simplicity , though nowadays their\n",
      "_simplec,th ,  hugp nrwadeyt  haar_\n",
      "Epoch 89: Train loss: 0.5745168477296829 | Train Accuracy: 0.31746435165405273 |  Validation loss: 2.9171645641326904 | Validation Accuracy: 0.7948718070983887\n",
      "\t because the buoyancy of industry generates\n",
      "_becaese tya bngyency r. induatry enerate _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mestresaas . Ahywah et tight be tich wcrse ._\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retbfe h hhat e fatraherh h hertrnsr_\n",
      "\t tied . He dare not precipitate\n",
      "_teed . te ere iii peipi ite_\n",
      "Epoch 90: Train loss: 0.5193095803260803 | Train Accuracy: 0.34496617317199707 |  Validation loss: 3.0329809188842773 | Validation Accuracy: 0.7564102411270142\n",
      "\t tied . He dare not precipitate\n",
      "_ eed g te ere ie  poieipi ite_\n",
      "\t more than ever like a pink and gold\n",
      "_ ore than evar eike a pine and good_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mi tresaea .t yyway it tight de tnch wr e _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slrdlp , g If eo  mu t drag the truth_\n",
      "Epoch 91: Train loss: 0.5285448208451271 | Train Accuracy: 0.3127482235431671 |  Validation loss: 3.03938364982605 | Validation Accuracy: 0.7692307829856873\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slrdlp , g Ih eoe mu t drag the truth_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ta  er at  oethen , haar_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retble h hhat eioatnatiop h hertoieen_\n",
      "\t peace of mind ? Philip put out\n",
      "_peeceeef taa h eh peh fet oai_\n",
      "Epoch 92: Train loss: 0.494972325861454 | Train Accuracy: 0.26969122886657715 |  Validation loss: 3.0684211254119873 | Validation Accuracy: 0.7948718070983887\n",
      "\t peace of mind ? Philip put out\n",
      "_peec  ef taa h lh pep le  oa _\n",
      "\t simplicity , though nowadays their\n",
      "_sihplic,th , thugh nowadeh   ha r_\n",
      "\t because the buoyancy of industry generates\n",
      "_becahse tha bioyahcy o. induatrn enerates_\n",
      "\t to 19 .\n",
      "_t 19 r_\n",
      "Epoch 93: Train loss: 0.48478811234235764 | Train Accuracy: 0.2658027112483978 |  Validation loss: 3.1358416080474854 | Validation Accuracy: 0.7884615659713745\n",
      "\t to 19 .\n",
      "_t 1r _\n",
      "\t because the buoyancy of industry generates\n",
      "_becaese  he cnoyancy oy induetor eerate _\n",
      "\t felt that part of her was leaving\n",
      "_felt tla  n r  of he .ai reevl_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_sorflp , \" If yo  tnat drag tfe treth_\n",
      "Epoch 94: Train loss: 0.46273335814476013 | Train Accuracy: 0.23427538573741913 |  Validation loss: 3.1661136150360107 | Validation Accuracy: 0.7884615659713745\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slrwlp , g If eo  mu t drag the trith_\n",
      "\t huh ? \"\n",
      "_t h h _\n",
      "\t to 19 .\n",
      "_to 1 r_\n",
      "\t peace of mind ? Philip put out\n",
      "_piece ef tan   i  pii oe  oi _\n",
      "Epoch 95: Train loss: 0.4305330067873001 | Train Accuracy: 0.22588053345680237 |  Validation loss: 3.2123546600341797 | Validation Accuracy: 0.7884615659713745\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever eike a pink and goed_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ta ter at  oetten , tear_\n",
      "\t because the buoyancy of industry generates\n",
      "_decanse tya bngysncy o. indn try enerates_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresees . Anyway it uight be mich wrse ._\n",
      "Epoch 96: Train loss: 0.4537511616945267 | Train Accuracy: 0.2289653718471527 |  Validation loss: 3.314910411834717 | Validation Accuracy: 0.7884615659713745\n",
      "\t peace of mind ? Philip put out\n",
      "_riica pf tnnd h htpip pa  oi _\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_tryipg to  haka-off thc rest trace  of hypntic eftir-effects ._\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ retble h hhat eifattattrp tthersonsen_\n",
      "\t huh ? \"\n",
      "_tuh h _\n",
      "Epoch 97: Train loss: 0.43715089559555054 | Train Accuracy: 0.2106696218252182 |  Validation loss: 3.325486660003662 | Validation Accuracy: 0.7948718070983887\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_ard Fletcher that ayron .ouod hsve ished his_\n",
      "\t because the buoyancy of industry generates\n",
      "_becayse tye bnoyancy o. indu tny eneyate _\n",
      "\t tied . He dare not precipitate\n",
      "_heed . te dare iit poecipitita_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Lend made a positive approach .  e  et out to increase_\n",
      "Epoch 98: Train loss: 0.3934639096260071 | Train Accuracy: 0.22084587812423706 |  Validation loss: 3.29071307182312 | Validation Accuracy: 0.7884615659713745\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to  haoa-off thcseast traces of hypnotic efte-effects -_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slrdlpt, e Iw you mu   drag the  rto_\n",
      "\t huh ? \"\n",
      "_tit h e_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_ard Fletcher that syrrn wouod hevesishedshis_\n",
      "Epoch 99: Train loss: 0.4168310984969139 | Train Accuracy: 0.20767048001289368 |  Validation loss: 3.4417507648468018 | Validation Accuracy: 0.7948718070983887\n",
      "\t peace of mind ? Philip put out\n",
      "_pieca el tand  ii tliialit lit_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . te aet out to increase_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to  haka-off the rast trace  of hynnotic aftir-effect  ._\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aod Flatcher thas shron wotod have weshed his_\n",
      "Epoch 100: Train loss: 0.3843502923846245 | Train Accuracy: 0.17225223779678345 |  Validation loss: 3.563678741455078 | Validation Accuracy: 0.7756410241127014\n",
      "\t peace of mind ? Philip put out\n",
      "_peec  el  and   o  lie oe  ois_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aod Fletcher that ahron wouod heve wnshed his_\n",
      "\t to 19 .\n",
      "_to 19 r_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slrwlp , \" If eo  mu t drag the trnth_\n",
      "Epoch 101: Train loss: 0.35978154838085175 | Train Accuracy: 0.18067267537117004 |  Validation loss: 3.544178009033203 | Validation Accuracy: 0.8012820482254028\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir ,   at seightlh et , and_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that ayrln ould heveswished his_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_sllwlp , \" If yoy must drag the trnth_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever nike a pink and good_\n",
      "Epoch 102: Train loss: 0.34576570242643356 | Train Accuracy: 0.15449225902557373 |  Validation loss: 3.5765724182128906 | Validation Accuracy: 0.7884615659713745\n",
      "\t huh ? \"\n",
      "_tih y e_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_ ryinr td ahakarff the rest tradee ow hyyntic aftir-effects ._\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever oike a pink and good_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Koethen , near_\n",
      "Epoch 103: Train loss: 0.33495204523205757 | Train Accuracy: 0.1747925877571106 |  Validation loss: 3.527113437652588 | Validation Accuracy: 0.8205128312110901\n",
      "\t felt that part of her was leaving\n",
      "_feft tha  nar  of har .ai oaavnn_\n",
      "\t huh ? \"\n",
      "_tit n e_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that arron wouod heve wished his_\n",
      "\t because the buoyancy of industry generates\n",
      "_becayse tha buoyancy of indu try ceierates_\n",
      "Epoch 104: Train loss: 0.33636385947465897 | Train Accuracy: 0.15947580337524414 |  Validation loss: 3.684678554534912 | Validation Accuracy: 0.7948718070983887\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that syron wouod heve wished his_\n",
      "\t tied . He dare not precipitate\n",
      "_heed . He dare iit pecipitate_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the eest traces f hypnotic efter-effects ._\n",
      "\t to 19 .\n",
      "_t 19 r_\n",
      "Epoch 105: Train loss: 0.31408654153347015 | Train Accuracy: 0.14287953078746796 |  Validation loss: 3.7307679653167725 | Validation Accuracy: 0.7756410241127014\n",
      "\t to 19 .\n",
      "_t 19 r_\n",
      "\t because the buoyancy of industry generates\n",
      "_becaese the bnoyancy of indu try generates_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir a   at  lightln wet , and_\n",
      "\t huh ? \"\n",
      "_tih y _\n",
      "Epoch 106: Train loss: 0.293668482452631 | Train Accuracy: 0.1292096972465515 |  Validation loss: 3.6708593368530273 | Validation Accuracy: 0.7884615659713745\n",
      "\t slowly , \" If you must drag the truth\n",
      "_sorwlp , \" If yo  muat drag the truth_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_sistresies . Anywah it might be mich worse ._\n",
      "\t tied . He dare not precipitate\n",
      "_heed . He dare ioi poecipitate_\n",
      "\t felt that part of her was leaving\n",
      "_feoe tha  part of herehee neevl_\n",
      "Epoch 107: Train loss: 0.3076581731438637 | Train Accuracy: 0.09797706454992294 |  Validation loss: 3.7466604709625244 | Validation Accuracy: 0.7884615659713745\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What eufat ation h Personsoy_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_tryingttf ahaka-rff the rast trace  ow hypnotic aftir-effects ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir ,   am slhghtlh wet , and_\n",
      "\t because the buoyancy of industry generates\n",
      "_because tye bnoysncy of induetry generates_\n",
      "Epoch 108: Train loss: 0.26911303773522377 | Train Accuracy: 0.13504129648208618 |  Validation loss: 3.932492256164551 | Validation Accuracy: 0.7820512652397156\n",
      "\t because the buoyancy of industry generates\n",
      "_becayse the bnoyancy of indu try generate _\n",
      "\t slowly , \" If you must drag the truth\n",
      "_solwlp , \" If yo  must drag the trnth_\n",
      "\t simplicity , though nowadays their\n",
      "_simplictty , though nowaday  tha r_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_aod hletoher that nlrln wotld havesished his_\n",
      "Epoch 109: Train loss: 0.2570612095296383 | Train Accuracy: 0.09920178353786469 |  Validation loss: 3.880385160446167 | Validation Accuracy: 0.7756410241127014\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever iike a pink and gold_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to  haka-off the east traces of hypnotic after-effects ._\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Ko\"then , near_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the bnoyancy of indu try generates_\n",
      "Epoch 110: Train loss: 0.24031657353043556 | Train Accuracy: 0.10443916916847229 |  Validation loss: 3.9271726608276367 | Validation Accuracy: 0.8269230723381042\n",
      "\t peace of mind ? Philip put out\n",
      "_pieci of  and h oh pip pet oit_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresies . Anyway it might be mich worse ._\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Ko\"then , near_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her .ai oeavin_\n",
      "Epoch 111: Train loss: 0.24357133731245995 | Train Accuracy: 0.0966208204627037 |  Validation loss: 3.970569610595703 | Validation Accuracy: 0.8012820482254028\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He aet out to increase_\n",
      "\t because the buoyancy of industry generates\n",
      "_because tha bnoysncy of indu try generates_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron wouod have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever iike a pink and good_\n",
      "Epoch 112: Train loss: 0.2474638968706131 | Train Accuracy: 0.09255211800336838 |  Validation loss: 4.091660499572754 | Validation Accuracy: 0.8141025900840759\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever oike a pink and god_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Ko\"then , near_\n",
      "\t because the buoyancy of industry generates\n",
      "_because tye buoyancy of indu try generate _\n",
      "Epoch 113: Train loss: 0.25769152492284775 | Train Accuracy: 0.13287317752838135 |  Validation loss: 4.058835506439209 | Validation Accuracy: 0.7756410241127014\n",
      "\t felt that part of her was leaving\n",
      "_felt that p rt of her wa  oeavon_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to ahaka-off the east trace  ef hypnotic after-effects ._\n",
      "\t simplicity , though nowadays their\n",
      "_simplpcety , thaugh nowaday  tha r_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He  et out to increase_\n",
      "Epoch 114: Train loss: 0.2295241728425026 | Train Accuracy: 0.09571389108896255 |  Validation loss: 4.108609676361084 | Validation Accuracy: 0.7884615659713745\n",
      "\t peace of mind ? Philip put out\n",
      "_peece of hand h lh lep fut fa _\n",
      "\t to 19 .\n",
      "_to 19 r_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresie  . Auyway it might be much worse ._\n",
      "\t simplicity , though nowadays their\n",
      "_simpgicuty , thaugh nlwedeyt  ha r_\n",
      "Epoch 115: Train loss: 0.22177228331565857 | Train Accuracy: 0.10410690307617188 |  Validation loss: 4.174335956573486 | Validation Accuracy: 0.8141025900840759\n",
      "\t because the buoyancy of industry generates\n",
      "_because the bnoysncy of indu try generates_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_soowlt , \" If yoy must drag the truth_\n",
      "\t huh ? \"\n",
      "_huh ? _\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anygay it might be mnch worse ._\n",
      "Epoch 116: Train loss: 0.2057478427886963 | Train Accuracy: 0.11050814390182495 |  Validation loss: 4.2044196128845215 | Validation Accuracy: 0.7692307829856873\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir ,   at slightlt wet , and_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_tryini to shake-off the east trace  of hypno ic after-effects ._\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of indu try generates_\n",
      "\t huh ? \"\n",
      "_huh ? e_\n",
      "Epoch 117: Train loss: 0.21487442404031754 | Train Accuracy: 0.08704560995101929 |  Validation loss: 4.2954607009887695 | Validation Accuracy: 0.807692289352417\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t to 19 .\n",
      "_to 19 r_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the eest traces of hypnotic after-effects ._\n",
      "\t because the buoyancy of industry generates\n",
      "_because the bnoyancy of indu try cenerates_\n",
      "Epoch 118: Train loss: 0.2114359587430954 | Train Accuracy: 0.05896558612585068 |  Validation loss: 4.267210006713867 | Validation Accuracy: 0.807692289352417\n",
      "\t because the buoyancy of industry generates\n",
      "_because the bnoyancy of indu try generates_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_tryin to shake-off the east traces of hypnotic after-effects ._\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuition ! Personally_\n",
      "Epoch 119: Train loss: 0.21172944828867912 | Train Accuracy: 0.055198490619659424 |  Validation loss: 4.217800140380859 | Validation Accuracy: 0.7884615659713745\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Liud made a positive approach . He set out to increase_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuatiop ! Personey_\n",
      "Epoch 120: Train loss: 0.1759457364678383 | Train Accuracy: 0.04901054874062538 |  Validation loss: 4.332777976989746 | Validation Accuracy: 0.7692307829856873\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightlt et , and_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anyway it might be much worse ._\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What iifatuation ! Personaly_\n",
      "Epoch 121: Train loss: 0.16438088566064835 | Train Accuracy: 0.047849588096141815 |  Validation loss: 4.466592311859131 | Validation Accuracy: 0.8012820482254028\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to  hake-off the east trace  of hypnotic after-effects ._\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Ko\"then , near_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her fai oeavin_\n",
      "Epoch 122: Train loss: 0.1520524825900793 | Train Accuracy: 0.05996066704392433 |  Validation loss: 4.432511806488037 | Validation Accuracy: 0.8205128312110901\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of indu try generates_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "Epoch 123: Train loss: 0.1410583294928074 | Train Accuracy: 0.042860373854637146 |  Validation loss: 4.5247321128845215 | Validation Accuracy: 0.807692289352417\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare iot precipitate_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byrln would have wished his_\n",
      "Epoch 124: Train loss: 0.1391450148075819 | Train Accuracy: 0.03758798539638519 |  Validation loss: 4.532800674438477 | Validation Accuracy: 0.8141025900840759\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t because the buoyancy of industry generates\n",
      "_because the bnoyancy of industry generates_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered ma ter at Ko\"then , near_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "Epoch 125: Train loss: 0.13197772204875946 | Train Accuracy: 0.042753640562295914 |  Validation loss: 4.6055121421813965 | Validation Accuracy: 0.8205128312110901\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t felt that part of her was leaving\n",
      "_feot that part of her wa  reavin_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistres es . Anyway it might be mnah worse ._\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_anf Fletcher that Byron wouod have wished his_\n",
      "Epoch 126: Train loss: 0.1388512086123228 | Train Accuracy: 0.04449126124382019 |  Validation loss: 4.578183650970459 | Validation Accuracy: 0.807692289352417\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What iifatuation ! Personelly_\n",
      "\t tied . He dare not precipitate\n",
      "_teed . te dare not precipitate_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "Epoch 127: Train loss: 0.11991908587515354 | Train Accuracy: 0.02209213562309742 |  Validation loss: 4.765191078186035 | Validation Accuracy: 0.8269230723381042\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her wa  leaving_\n",
      "\t peace of mind ? Philip put out\n",
      "_piace if mind  tPhtlip put lit_\n",
      "Epoch 128: Train loss: 0.11951943300664425 | Train Accuracy: 0.02334148809313774 |  Validation loss: 4.732022285461426 | Validation Accuracy: 0.7756410241127014\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "Epoch 129: Train loss: 0.11328135430812836 | Train Accuracy: 0.024088792502880096 |  Validation loss: 4.703432559967041 | Validation Accuracy: 0.8205128312110901\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "\t huh ? \"\n",
      "_huh ? e_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces of hypnotic after-effects ._\n",
      "\t tied . He dare not precipitate\n",
      "_teed . He dare not precipttate_\n",
      "Epoch 130: Train loss: 0.09816574119031429 | Train Accuracy: 0.02379084751009941 |  Validation loss: 4.718570232391357 | Validation Accuracy: 0.7820512652397156\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personaly_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "Epoch 131: Train loss: 0.09716840088367462 | Train Accuracy: 0.021615445613861084 |  Validation loss: 4.6946306228637695 | Validation Accuracy: 0.7948718070983887\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir ,   am slightly wet , and_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays thair_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her wai oeavin_\n",
      "Epoch 132: Train loss: 0.09706451557576656 | Train Accuracy: 0.0335356630384922 |  Validation loss: 4.860787868499756 | Validation Accuracy: 0.7564102411270142\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "\t peace of mind ? Philip put out\n",
      "_peece of mind h Phioip put oit_\n",
      "Epoch 133: Train loss: 0.08947752043604851 | Train Accuracy: 0.036185555160045624 |  Validation loss: 4.977475643157959 | Validation Accuracy: 0.8012820482254028\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "Epoch 134: Train loss: 0.07799051888287067 | Train Accuracy: 0.0070251175202429295 |  Validation loss: 4.876912593841553 | Validation Accuracy: 0.7884615659713745\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces ef hypnotic after-effects ._\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "Epoch 135: Train loss: 0.08001429308205843 | Train Accuracy: 0.011062460020184517 |  Validation loss: 4.9550371170043945 | Validation Accuracy: 0.7948718070983887\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anyway it might be much worse ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part oo her wes leavin_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , thaugh nowadays their_\n",
      "Epoch 136: Train loss: 0.08017036132514477 | Train Accuracy: 0.005394791252911091 |  Validation loss: 5.039851188659668 | Validation Accuracy: 0.8012820482254028\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicuty , though nowadays thair_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "Epoch 137: Train loss: 0.06727956049144268 | Train Accuracy: 0.0056461067870259285 |  Validation loss: 5.091623783111572 | Validation Accuracy: 0.7948718070983887\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces of hypnotic after-effects ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t peace of mind ? Philip put out\n",
      "_peeca of mind ? Philip put out_\n",
      "Epoch 138: Train loss: 0.06775897368788719 | Train Accuracy: 0.007394405081868172 |  Validation loss: 5.101280212402344 | Validation Accuracy: 0.8012820482254028\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "Epoch 139: Train loss: 0.06484631169587374 | Train Accuracy: 0.0164046473801136 |  Validation loss: 5.198185920715332 | Validation Accuracy: 0.8012820482254028\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t peace of mind ? Philip put out\n",
      "_peacu of  ind   Phioip put ott_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you mu t drag the truth_\n",
      "Epoch 140: Train loss: 0.06948890723288059 | Train Accuracy: 0.009427487850189209 |  Validation loss: 5.215054035186768 | Validation Accuracy: 0.807692289352417\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t peace of mind ? Philip put out\n",
      "_peace of mind ? Philip put out_\n",
      "Epoch 141: Train loss: 0.053748211823403835 | Train Accuracy: 0.0031839071307331324 |  Validation loss: 5.139252185821533 | Validation Accuracy: 0.8141025900840759\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "Epoch 142: Train loss: 0.06203578691929579 | Train Accuracy: 0.017492378130555153 |  Validation loss: 5.239384651184082 | Validation Accuracy: 0.807692289352417\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her was leaving_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces of hypnotic after-effects ._\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "Epoch 143: Train loss: 0.0549446614459157 | Train Accuracy: 0.005510876886546612 |  Validation loss: 5.237712383270264 | Validation Accuracy: 0.7948718070983887\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t peace of mind ? Philip put out\n",
      "_peace of  ind ? Philip put out_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "Epoch 144: Train loss: 0.06198798678815365 | Train Accuracy: 0.0022727272007614374 |  Validation loss: 5.237090110778809 | Validation Accuracy: 0.7948718070983887\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays thair_\n",
      "Epoch 145: Train loss: 0.0522250859066844 | Train Accuracy: 0.0020491802133619785 |  Validation loss: 5.33174467086792 | Validation Accuracy: 0.7820512652397156\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t peace of mind ? Philip put out\n",
      "_peace of mind ? Philip put out_\n",
      "Epoch 146: Train loss: 0.05156778823584318 | Train Accuracy: 0.0039290934801101685 |  Validation loss: 5.362030982971191 | Validation Accuracy: 0.807692289352417\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "Epoch 147: Train loss: 0.057584308087825775 | Train Accuracy: 0.0 |  Validation loss: 5.345988750457764 | Validation Accuracy: 0.7820512652397156\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anyway it might be much worse ._\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "Epoch 148: Train loss: 0.045787007082253695 | Train Accuracy: 0.008628985844552517 |  Validation loss: 5.364434242248535 | Validation Accuracy: 0.7692307829856873\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "Epoch 149: Train loss: 0.05281619913876057 | Train Accuracy: 0.0017123287543654442 |  Validation loss: 5.514049053192139 | Validation Accuracy: 0.8012820482254028\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her was leaving_\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anyway it might be much worse ._\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "Epoch 150: Train loss: 0.04622021410614252 | Train Accuracy: 0.0038438967894762754 |  Validation loss: 5.470610618591309 | Validation Accuracy: 0.7820512652397156\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t because the buoyancy of industry generates\n",
      "_because the buoyancy of industry generates_\n",
      "Epoch 151: Train loss: 0.040742283686995506 | Train Accuracy: 0.003189371433109045 |  Validation loss: 5.541655540466309 | Validation Accuracy: 0.7948718070983887\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces of hypnotic after-effects ._\n",
      "Epoch 152: Train loss: 0.046505150850862265 | Train Accuracy: 0.0014367816038429737 |  Validation loss: 5.440430641174316 | Validation Accuracy: 0.7884615659713745\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_tremble ! What infatuation ! Personally_\n",
      "Epoch 153: Train loss: 0.04116458771750331 | Train Accuracy: 0.0 |  Validation loss: 5.4688029289245605 | Validation Accuracy: 0.7884615659713745\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "\t peace of mind ? Philip put out\n",
      "_peace of mind ? Philip put out_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "Epoch 154: Train loss: 0.049226569943130016 | Train Accuracy: 0.0 |  Validation loss: 5.607750415802002 | Validation Accuracy: 0.8012820482254028\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "Epoch 155: Train loss: 0.04030310828238726 | Train Accuracy: 0.0035714285913854837 |  Validation loss: 5.564330577850342 | Validation Accuracy: 0.7756410241127014\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her was leaving_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "Epoch 156: Train loss: 0.031249004881829023 | Train Accuracy: 0.001552795059978962 |  Validation loss: 5.553772449493408 | Validation Accuracy: 0.7948718070983887\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "Epoch 157: Train loss: 0.031024907249957323 | Train Accuracy: 0.0 |  Validation loss: 5.567850589752197 | Validation Accuracy: 0.7884615659713745\n",
      "\t and Fletcher that Byron would have wished his\n",
      "_and Fletcher that Byron would have wished his_\n",
      "\t more than ever like a pink and gold\n",
      "_more than ever like a pink and gold_\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t huh ? \"\n",
      "_huh ? \"_\n",
      "Epoch 158: Train loss: 0.03225936647504568 | Train Accuracy: 0.0 |  Validation loss: 5.525218486785889 | Validation Accuracy: 0.7820512652397156\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "\t Yes sir , I am slightly wet , and\n",
      "_Yes sir , I am slightly wet , and_\n",
      "\t revered master at Ko\"then , near\n",
      "_revered master at Ko\"then , near_\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_Laud made a positive approach . He set out to increase_\n",
      "Epoch 159: Train loss: 0.045302844140678644 | Train Accuracy: 0.003246753243729472 |  Validation loss: 5.671544075012207 | Validation Accuracy: 0.8012820482254028\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t peace of mind ? Philip put out\n",
      "_piace of mind ? Philip put out_\n",
      "\t simplicity , though nowadays their\n",
      "_simplicity , though nowadays their_\n",
      "\t to 19 .\n",
      "_to 19 ._\n",
      "Epoch 160: Train loss: 0.03373359143733978 | Train Accuracy: 0.0 |  Validation loss: 5.737068176269531 | Validation Accuracy: 0.807692289352417\n",
      "\t tied . He dare not precipitate\n",
      "_tied . He dare not precipitate_\n",
      "\t slowly , \" If you must drag the truth\n",
      "_slowly , \" If you must drag the truth_\n",
      "\t trying to shake-off the last traces of hypnotic after-effects .\n",
      "_trying to shake-off the last traces of hypnotic after-effects ._\n",
      "\t mistresses . Anyway it might be much worse .\n",
      "_mistresses . Anyway it might be much worse ._\n",
      "Epoch 161: Train loss: 0.030516077298671007 | Train Accuracy: 0.0 |  Validation loss: 5.769871234893799 | Validation Accuracy: 0.7820512652397156\n",
      "\t felt that part of her was leaving\n",
      "_felt that part of her was oeaving_\n",
      "\t because the buoyancy of industry generates\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m recognizer \u001b[39m=\u001b[39m Recognizer()\n\u001b[1;32m      2\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(recognizer\u001b[39m=\u001b[39;49mrecognizer, \n\u001b[1;32m      6\u001b[0m               train_line_dataset\u001b[39m=\u001b[39;49mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39;49mline_dataset_val, \n\u001b[1;32m      7\u001b[0m               batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, recognizer_lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m               betas\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0.999\u001b[39;49m), num_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, loss_balancing_alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(recognizer, train_line_dataset, val_line_dataset, batch_size, recognizer_lr, betas, num_epochs, loss_balancing_alpha)\u001b[0m\n\u001b[1;32m     60\u001b[0m recognizer_loss \u001b[39m=\u001b[39m recognizer_loss_function(\n\u001b[1;32m     61\u001b[0m     \u001b[39m# torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     F\u001b[39m.\u001b[39mlog_softmax(recognizer_outputs, \u001b[39m1\u001b[39m),  \u001b[39m# Requires number of classes to move from 2nd to 1st dimension after log_softmax\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     line_text_batch\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m test2 \u001b[39m=\u001b[39m recognizer_outputs[\u001b[39m0\u001b[39m,:,:]\n\u001b[0;32m---> 66\u001b[0m test2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49margmax(test2, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# Removed 0 dim\u001b[39;00m\n\u001b[1;32m     67\u001b[0m test2 \u001b[39m=\u001b[39m test2[test2\u001b[39m.\u001b[39mnonzero()]\n\u001b[1;32m     68\u001b[0m test2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([int_to_char[\u001b[39mint\u001b[39m(i)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test2])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=4, recognizer_lr=1e-3,\n",
    "              betas=(0, 0.999), num_epochs=500, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n",
      "tensor([[0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0139, 0.0137],\n",
      "        [0.0138, 0.0138, 0.0138,  ..., 0.0138, 0.0134, 0.0138],\n",
      "        [0.0137, 0.0138, 0.0137,  ..., 0.0137, 0.0135, 0.0137],\n",
      "        ...,\n",
      "        [0.0140, 0.0140, 0.0140,  ..., 0.0139, 0.0135, 0.0139],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0138, 0.0139, 0.0137],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0138, 0.0137]],\n",
      "       grad_fn=<SoftmaxBackward0>) torch.Size([82, 73])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "\n",
    "print(torch.softmax(recognizer(image.unsqueeze(0)), 1), torch.softmax(recognizer(image.unsqueeze(0)), 1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
