{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "from torchmetrics.text import CharErrorRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "# preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m line_transcription_dataset \u001b[39m=\u001b[39m LineDataset(\u001b[39m\"\u001b[39m\u001b[39m./data/lines_improved.txt\u001b[39m\u001b[39m\"\u001b[39m, ty\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtxt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m line_image_dataset \u001b[39m=\u001b[39m LineDataset(\u001b[39m\"\u001b[39m\u001b[39m./data/lines_improved.txt\u001b[39m\u001b[39m\"\u001b[39m, ty\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m line_dataset \u001b[39m=\u001b[39m LineDataset(\u001b[39m\"\u001b[39;49m\u001b[39m./data/lines_improved.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Don't change this, we want to maintain consistent split\u001b[39;00m\n\u001b[1;32m      6\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m12345678\u001b[39m)  \u001b[39m# DO NOT REMOVE THIS LINE\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mLineDataset.__init__\u001b[0;34m(self, lines_improved_dir, ty)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mret_ctoi_paddeds \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mpad(ret_ctois[i], pad\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_transcription_len\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(ret_ctois[i])), value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length)]\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mty \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mret_images \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mload(line_datas[i][\u001b[39m\"\u001b[39m\u001b[39mimage_pt_path\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length)]\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mret_ctoi_paddeds \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39mpad(ret_ctois[i], pad\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_transcription_len\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(ret_ctois[i])), value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length)]\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mty \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mret_images \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mload(line_datas[i][\u001b[39m\"\u001b[39;49m\u001b[39mimage_pt_path\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length)]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(500))\n",
    "line_dataset_val = Subset(line_dataset_val, range(100))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([66, 61,  1,  9, 17,  1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'to 19 .')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4,2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        #self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,2))\n",
    "        #self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(248, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.3)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        #print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.dense2(img)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "        \n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((1, 1, 32, 512), dtype=torch.float32))\n",
    "#print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(val_line_dataset_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device) \n",
    "        line_text_batch = line_text_batch.to(device)\n",
    "        recognizer_outputs = recognizer(line_image_batch)\n",
    "        recognizer_loss = recognizer_loss_function(F.log_softmax(recognizer_outputs, 1), line_text_batch)\n",
    "        \n",
    "        total_loss += recognizer_loss.item()\n",
    "        total_epoch += 1\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    \n",
    "    #print(recognizer_outputs, recognizer_outputs.shape)\n",
    "    #print(torch.argmax(recognizer_outputs, 1), torch.argmax(recognizer_outputs, 1).shape)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_recog_accuracy(preds, target):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the recognizer with character error rate\n",
    "    which is based on edit distance\n",
    "\n",
    "    Params:\n",
    "        preds: a list of prediction strings\n",
    "        targets: a list of target strings\n",
    "\n",
    "    Returns:\n",
    "        An integer, the character error rate average across\n",
    "        all predictions and targets\n",
    "    \"\"\"\n",
    "\n",
    "    cer = CharErrorRate()\n",
    "    return cer(preds, target)\n",
    "\n",
    "def create_strings_from_tensor(int_tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        int_tensor: A shape (N, 82) tensor where each row corresponds to\n",
    "        a integer mapping of a string. Includes padding\n",
    "    \n",
    "    Returns:\n",
    "        A list of N strings\n",
    "    \"\"\"\n",
    "\n",
    "    strings = []\n",
    "    for string_map in int_tensor:\n",
    "        strings.append(\"\".join([int_to_char[int(i)] for i in string_map[string_map != 0]]))\n",
    "    return strings\n",
    "    \n",
    "\n",
    "def get_accuracy(device, recognizer, recognizer_loader):\n",
    "\n",
    "    acc = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(recognizer_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device)\n",
    "        line_text_batch\n",
    "        recognizer_outputs = torch.argmax(recognizer(line_image_batch), 1)\n",
    "        recognizer_pred = create_strings_from_tensor(recognizer_outputs)\n",
    "        \n",
    "        label = create_strings_from_tensor(line_text_batch)\n",
    "        \n",
    "        acc += calculate_recog_accuracy(recognizer_pred, label)\n",
    "        \n",
    "        \n",
    "    return acc / (i+1)\n",
    "        \n",
    "    \n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    \n",
    "    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n",
    "    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n",
    "    \n",
    "    n = len(train_loss) # number of epochs\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Train Loss\", \"Validation Loss\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend([\"Train Error\", \"Validation Error\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    #print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #print(len(train_line_dataset_loader))\n",
    "\n",
    "    recognizer_optimizer = optim.Adam(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.NLLLoss()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = np.zeros(num_epochs)\n",
    "    recognizer_train_accuracies = np.zeros(num_epochs)\n",
    "    recognizer_val_losses = np.zeros(num_epochs)\n",
    "    recognizer_val_accuracies = np.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "            # print(line_text_batch.shape)\n",
    "\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            #print(line_text_batch, line_text_batch.shape)\n",
    "            recognizer_outputs = recognizer(line_image_batch)  # Mult factor to incentivize padding\n",
    "   \n",
    "            # print(recognizer_outputs, recognizer_outputs.shape)\n",
    "            # print(line_text_batch, line_text_batch.shape)\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            #line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            #target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            #target = torch.cat(line_text_batch_pad_remove)\n",
    "            #print(target, target.shape)\n",
    "            #input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\n",
    "                F.log_softmax(recognizer_outputs, 1),  # Requires number of classes to move from 2nd to 1st dimension after log_softmax\n",
    "                line_text_batch\n",
    "            )\n",
    "            test2 = recognizer_outputs[0,:,:]\n",
    "            test2 = torch.argmax(test2, dim=0)  # Removed 0 dim\n",
    "            test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            recognizer_optimizer.zero_grad()\n",
    "    \n",
    "            recognizer_train_loss += recognizer_loss.item()\n",
    "        \n",
    "        print(\"\\t\",test)\n",
    "        print(f\"_{test2}_\")\n",
    "        recognizer_train_losses[epoch] = float(recognizer_train_loss) / (i+1)\n",
    "        \n",
    "        recognizer.eval()\n",
    "        recognizer_val_losses[epoch] = evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function)\n",
    "        recognizer.train()\n",
    "        \n",
    "        recognizer_train_accuracies[epoch] = get_accuracy(device, recognizer, train_line_dataset_loader)\n",
    "        recognizer_val_accuracies[epoch]= get_accuracy(device, recognizer, val_line_dataset_loader)\n",
    "        \n",
    "        print((\"Epoch {}: Train loss: {} | Train Accuracy: {} | \"+\n",
    "            \" Validation loss: {} | Validation Accuracy: {}\").format(\n",
    "                    epoch + 1,\n",
    "                    recognizer_train_losses[epoch],\n",
    "                    recognizer_train_accuracies[epoch],\n",
    "                    recognizer_val_losses[epoch],\n",
    "                    recognizer_val_accuracies[epoch]))\n",
    "\n",
    "        model_path = get_model_name(recognizer.name, batch_size, recognizer_lr, epoch)\n",
    "        torch.save(recognizer.state_dict(), os.path.join(\"./recognizers\", model_path))\n",
    "        model_path_const_batch = get_model_name(recognizer.name, batch_size, recognizer_lr, -1)\n",
    "\n",
    "        np.savetxt(\"./recognizers/{}_train_loss.csv\".format(model_path_const_batch), recognizer_train_losses)\n",
    "        np.savetxt(\"./recognizers/{}_val_loss.csv\".format(model_path_const_batch),  recognizer_val_losses)\n",
    "        np.savetxt(\"./recognizers/{}_train_acc.csv\".format(model_path_const_batch), recognizer_train_accuracies)\n",
    "        np.savetxt(\"./recognizers/{}_val_acc.csv\".format(model_path_const_batch), recognizer_val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./recognizers/model_recognizer_bs4_lr0.001_epoch99_train_loss.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_training_curve(\u001b[39m\"\u001b[39;49m\u001b[39m./recognizers/model_recognizer_bs4_lr0.001_epoch99\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m, in \u001b[0;36mplot_training_curve\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_training_curve\u001b[39m(path):\n\u001b[1;32m     82\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     train_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m\"\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m_train_loss.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(path))\n\u001b[1;32m     84\u001b[0m     val_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_val_loss.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[1;32m     86\u001b[0m     train_acc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_train_acc.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/npyio.py:1313\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1311\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1313\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1314\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1315\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1316\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1318\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/npyio.py:955\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    953\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(fname)\n\u001b[1;32m    954\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 955\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         encoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./recognizers/model_recognizer_bs4_lr0.001_epoch99_train_loss.csv not found."
     ]
    }
   ],
   "source": [
    "plot_training_curve(\"./recognizers/model_recognizer_bs4_lr0.001_epoch99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t same afternoon .\n",
      "__\n",
      "Epoch 1: Train loss: 3.5950592383742332 | Train Accuracy: 1.0 |  Validation loss: 2.6149441514696394 | Validation Accuracy: 1.0\n",
      "\t boy . He settled me into the car\n",
      "__\n",
      "Epoch 2: Train loss: 2.0388574078679085 | Train Accuracy: 1.0 |  Validation loss: 1.8804083381380354 | Validation Accuracy: 1.0\n",
      "\t will go . The main topic under review is\n",
      "__\n",
      "Epoch 3: Train loss: 1.8478251434862614 | Train Accuracy: 1.0 |  Validation loss: 1.8581132888793945 | Validation Accuracy: 1.0\n",
      "\t The journey has been against me , as there has\n",
      "__\n",
      "Epoch 4: Train loss: 1.8426941558718681 | Train Accuracy: 1.0 |  Validation loss: 1.8406400169645036 | Validation Accuracy: 1.0\n",
      "\t little spiv from Brighton seemed an\n",
      "__\n",
      "Epoch 5: Train loss: 1.8388057015836239 | Train Accuracy: 1.0 |  Validation loss: 1.8619779348373413 | Validation Accuracy: 1.0\n",
      "\t temporary surroundings .\n",
      "__\n",
      "Epoch 6: Train loss: 1.8515421859920025 | Train Accuracy: 1.0 |  Validation loss: 1.8844933339527674 | Validation Accuracy: 1.0\n",
      "\t An hour's riding brought us to a trail that\n",
      "__\n",
      "Epoch 7: Train loss: 1.8424618020653725 | Train Accuracy: 1.0 |  Validation loss: 1.8504094736916679 | Validation Accuracy: 1.0\n",
      "\t money could have been disposed of in London\n",
      "__\n",
      "Epoch 8: Train loss: 1.852486789226532 | Train Accuracy: 1.0 |  Validation loss: 1.8413256747382027 | Validation Accuracy: 1.0\n",
      "\t classes in the early 19th century which\n",
      "__\n",
      "Epoch 9: Train loss: 1.8435654267668724 | Train Accuracy: 1.0 |  Validation loss: 1.8363512413842338 | Validation Accuracy: 1.0\n",
      "\t inside a submarine casing to defuse a Jerry\n",
      "__\n",
      "Epoch 10: Train loss: 1.8431506305932999 | Train Accuracy: 1.0 |  Validation loss: 1.8415389231273107 | Validation Accuracy: 1.0\n",
      "\t Mr. Macleod thought the two Rhodesian parties\n",
      "__\n",
      "Epoch 11: Train loss: 1.8380770273506641 | Train Accuracy: 1.0 |  Validation loss: 1.8676797832761491 | Validation Accuracy: 1.0\n",
      "\t cabinet work the necessity for\n",
      "_              _\n",
      "Epoch 12: Train loss: 1.8055588230490685 | Train Accuracy: 0.8776136636734009 |  Validation loss: 2.363082068307059 | Validation Accuracy: 0.8767372965812683\n",
      "\t Sir Roy's United Federal Party is\n",
      "_                      _\n",
      "Epoch 13: Train loss: 1.7818773165345192 | Train Accuracy: 0.8517919778823853 |  Validation loss: 1.7663377693721227 | Validation Accuracy: 0.8535360097885132\n",
      "\t Mr. Joshua Logan , attempted but failed to create\n",
      "_                                 _\n",
      "Epoch 14: Train loss: 1.7679470293223858 | Train Accuracy: 0.8407691717147827 |  Validation loss: 2.5935561316353932 | Validation Accuracy: 0.8414803743362427\n",
      "\t Again the resentment was widespread and\n",
      "_               _\n",
      "Epoch 15: Train loss: 1.739455919712782 | Train Accuracy: 0.8347543478012085 |  Validation loss: 1.7336232491901942 | Validation Accuracy: 0.8297527432441711\n",
      "\t create the atmosphere of a city .\n",
      "_                                 _\n",
      "Epoch 16: Train loss: 1.7119094990193844 | Train Accuracy: 0.8340571522712708 |  Validation loss: 2.544339690889631 | Validation Accuracy: 0.8299331665039062\n",
      "\t boy . He settled me into the car\n",
      "_                                _\n",
      "Epoch 17: Train loss: 1.7073333635926247 | Train Accuracy: 0.8325296640396118 |  Validation loss: 2.3801277365003313 | Validation Accuracy: 0.8273530006408691\n",
      "\t relieve his feelings . On the small-to-medium\n",
      "_                                         _\n",
      "Epoch 18: Train loss: 1.6945233829319477 | Train Accuracy: 0.8321576714515686 |  Validation loss: 1.7045752150671822 | Validation Accuracy: 0.8277487754821777\n",
      "\t comically grotesque ; they were not\n",
      "_                        _\n",
      "Epoch 19: Train loss: 1.6866135373711586 | Train Accuracy: 0.8322374820709229 |  Validation loss: 2.2227416038513184 | Validation Accuracy: 0.8266776204109192\n",
      "\t roped in . The only result was\n",
      "_                                _\n",
      "Epoch 20: Train loss: 1.6986703649163246 | Train Accuracy: 0.8309528231620789 |  Validation loss: 1.693523645401001 | Validation Accuracy: 0.8286138772964478\n",
      "\t his surviving submarines a good deal harder than\n",
      "_                                          _\n",
      "Epoch 21: Train loss: 1.670877255499363 | Train Accuracy: 0.8306776881217957 |  Validation loss: 1.6768745865140642 | Validation Accuracy: 0.825509250164032\n",
      "\t grandchildren who had sat , rather intimidated\n",
      "_                                         _\n",
      "Epoch 22: Train loss: 1.6635810360312462 | Train Accuracy: 0.8306848406791687 |  Validation loss: 1.6805167879377092 | Validation Accuracy: 0.8298383951187134\n",
      "\t companion . \" You want me to talk , is that it ? \"\n",
      "_ee                                _\n",
      "Epoch 23: Train loss: 1.6501184217631817 | Train Accuracy: 0.8283730745315552 |  Validation loss: 1.6960347550255912 | Validation Accuracy: 0.8234566450119019\n",
      "\t but I really tremble for my country ! I may\n",
      "_ e                                   _\n",
      "Epoch 24: Train loss: 1.6542328596115112 | Train Accuracy: 0.8295436501502991 |  Validation loss: 2.3456964833395824 | Validation Accuracy: 0.8236085772514343\n",
      "\t steadily and looked about the room ,\n",
      "_                                     _\n",
      "Epoch 25: Train loss: 1.6518548987805843 | Train Accuracy: 0.8326624035835266 |  Validation loss: 1.7726081780024938 | Validation Accuracy: 0.8262709379196167\n",
      "\t pressed as when he left Mrs. Halliday's office ,\n",
      "_ e                                   _\n",
      "Epoch 26: Train loss: 1.650849711149931 | Train Accuracy: 0.8197120428085327 |  Validation loss: 1.7406333003725325 | Validation Accuracy: 0.8172343373298645\n",
      "\t fine girl , intelligent , and pretty , and I had\n",
      "_ e                                        _\n",
      "Epoch 27: Train loss: 1.6405932307243347 | Train Accuracy: 0.8208515048027039 |  Validation loss: 1.781879186630249 | Validation Accuracy: 0.8189072608947754\n",
      "\t rate , have proved that wrong . And they say , too ,\n",
      "_ a                                           _\n",
      "Epoch 28: Train loss: 1.6377154476940632 | Train Accuracy: 0.8339930772781372 |  Validation loss: 1.760463731629508 | Validation Accuracy: 0.8369265794754028\n",
      "\t substances . The Poisons Board had already\n",
      "_ a                                 _\n",
      "Epoch 29: Train loss: 1.6576238386332989 | Train Accuracy: 0.8299229145050049 |  Validation loss: 1.687529104096549 | Validation Accuracy: 0.830205500125885\n",
      "\t sequentially operated : the closing of the shutter\n",
      "_hh                                     _\n",
      "Epoch 30: Train loss: 1.6401512920856476 | Train Accuracy: 0.8210573792457581 |  Validation loss: 2.3508266040257046 | Validation Accuracy: 0.816342830657959\n",
      "\t negotiations with Sir Roy's representative ,\n",
      "_ah                                         _\n",
      "Epoch 31: Train loss: 1.6274058371782303 | Train Accuracy: 0.8429555296897888 |  Validation loss: 1.7318016801561629 | Validation Accuracy: 0.8342065811157227\n",
      "\t in the chancery , or the corridors of the United\n",
      "_ h                                        _\n",
      "Epoch 32: Train loss: 1.6278539635241032 | Train Accuracy: 0.8282471299171448 |  Validation loss: 1.6908705745424544 | Validation Accuracy: 0.825922966003418\n",
      "\t nisation of the movement of employing firms with the\n",
      "_ah                                             _\n",
      "Epoch 33: Train loss: 1.6248851865530014 | Train Accuracy: 0.8204485177993774 |  Validation loss: 1.6463860954557146 | Validation Accuracy: 0.8156755566596985\n",
      "\t country ! I may be mistaken , 1tho' I cannot\n",
      "_ e                                     _\n",
      "Epoch 34: Train loss: 1.6239438019692898 | Train Accuracy: 0.8207287192344666 |  Validation loss: 1.6200495958328247 | Validation Accuracy: 0.8252654075622559\n",
      "\t ' immediately carried into effect ' - not in spite of himself\n",
      "_nh                                    _\n",
      "Epoch 35: Train loss: 1.6304908841848373 | Train Accuracy: 0.820850670337677 |  Validation loss: 1.6506534474236625 | Validation Accuracy: 0.8187269568443298\n",
      "\t verbs , and other parts of speech occurring\n",
      "_ee                                   _\n",
      "Epoch 36: Train loss: 1.6274086982011795 | Train Accuracy: 0.8200145363807678 |  Validation loss: 1.6226797955376762 | Validation Accuracy: 0.8237692713737488\n",
      "\t had formerly been reluctant to approve the\n",
      "_ e                                 _\n",
      "Epoch 37: Train loss: 1.613261342048645 | Train Accuracy: 0.819419264793396 |  Validation loss: 1.629449418612889 | Validation Accuracy: 0.8219738006591797\n",
      "\t This new party , the British Socialist Party , was\n",
      "_ah                                         _\n",
      "Epoch 38: Train loss: 1.6251819133758545 | Train Accuracy: 0.8302604556083679 |  Validation loss: 1.6598234346934728 | Validation Accuracy: 0.8289372324943542\n",
      "\t finmarchicus in the summer at\n",
      "_hh                            _\n",
      "Epoch 39: Train loss: 1.608324807137251 | Train Accuracy: 0.8214246034622192 |  Validation loss: 1.6591702018465315 | Validation Accuracy: 0.8190613389015198\n",
      "\t Ultratoryism , that the Commons House upon\n",
      "_ h                                      _\n",
      "Epoch 40: Train loss: 1.6026454493403435 | Train Accuracy: 0.8183260560035706 |  Validation loss: 1.6391982010432653 | Validation Accuracy: 0.8146747946739197\n",
      "\t history of Anglesey's unceasing search for an\n",
      "_ a                                   _\n",
      "Epoch 41: Train loss: 1.604675218462944 | Train Accuracy: 0.821425199508667 |  Validation loss: 1.6559801612581526 | Validation Accuracy: 0.8193742632865906\n",
      "\t should . Germany exports much more than she imports .\n",
      "_ h                                          _\n",
      "Epoch 42: Train loss: 1.6057696864008904 | Train Accuracy: 0.8208000063896179 |  Validation loss: 1.6084446566445487 | Validation Accuracy: 0.8201187252998352\n",
      "\t ' Good heavens , darling , why on earth\n",
      "_te                                  _\n",
      "Epoch 43: Train loss: 1.6132717579603195 | Train Accuracy: 0.8238522410392761 |  Validation loss: 1.6367087023598808 | Validation Accuracy: 0.8162785172462463\n",
      "\t put Vittoria on guard . ' Santa Maria ! These spying\n",
      "_hh                                      _\n",
      "Epoch 44: Train loss: 1.6107509322464466 | Train Accuracy: 0.8274690508842468 |  Validation loss: 1.6024063314710344 | Validation Accuracy: 0.8364699482917786\n",
      "\t The journey has been against me , as there has\n",
      "_aa                                             _\n",
      "Epoch 45: Train loss: 1.6040798388421535 | Train Accuracy: 0.8183916807174683 |  Validation loss: 1.6036181790488107 | Validation Accuracy: 0.8170858025550842\n",
      "\t too fresh in both their minds to be a comfortable\n",
      "_aa                                           _\n",
      "Epoch 46: Train loss: 1.5970434732735157 | Train Accuracy: 0.8170673847198486 |  Validation loss: 1.6126835005623954 | Validation Accuracy: 0.8166280388832092\n",
      "\t technical error in allowing Irene to speak\n",
      "_ao                               _\n",
      "Epoch 47: Train loss: 1.5977452397346497 | Train Accuracy: 0.8143342137336731 |  Validation loss: 1.63413405418396 | Validation Accuracy: 0.8149910569190979\n",
      "\t in deep water , the later larval stages\n",
      "_te                               _\n",
      "Epoch 48: Train loss: 1.5855682380497456 | Train Accuracy: 0.8160241842269897 |  Validation loss: 1.6861100707735335 | Validation Accuracy: 0.8223022818565369\n",
      "\t Herring fishermen call this \" the outset \" . It\n",
      "_th                                     _\n",
      "Epoch 49: Train loss: 1.5909481048583984 | Train Accuracy: 0.8233534097671509 |  Validation loss: 1.6266323668616158 | Validation Accuracy: 0.8290717005729675\n",
      "\t he was merely interested in the unfamiliar\n",
      "_th                                    _\n",
      "Epoch 50: Train loss: 1.590865883976221 | Train Accuracy: 0.8202266693115234 |  Validation loss: 1.5761333533695765 | Validation Accuracy: 0.8209977746009827\n",
      "\t a boy , a cheerful , good-natured\n",
      "_te                                  _\n",
      "Epoch 51: Train loss: 1.585206899791956 | Train Accuracy: 0.8209270238876343 |  Validation loss: 1.64814977986472 | Validation Accuracy: 0.830752432346344\n",
      "\t passed all too quickly . As they parted in\n",
      "_ta                                         _\n",
      "Epoch 52: Train loss: 1.588884748518467 | Train Accuracy: 0.8243075013160706 |  Validation loss: 1.6540872710091727 | Validation Accuracy: 0.8253599405288696\n",
      "\t Germany's armament , the new attempt\n",
      "_toe  e                            _\n",
      "Epoch 53: Train loss: 1.5879323035478592 | Train Accuracy: 0.8245373964309692 |  Validation loss: 1.6212935107094901 | Validation Accuracy: 0.8246305584907532\n",
      "\t president 89-year-old Earl Russell and\n",
      "_tu                                      _\n",
      "Epoch 54: Train loss: 1.5813623182475567 | Train Accuracy: 0.8173954486846924 |  Validation loss: 1.691693936075483 | Validation Accuracy: 0.818584144115448\n",
      "\t in the chancery , or the corridors of the United\n",
      "_we                                           _\n",
      "Epoch 55: Train loss: 1.5814445838332176 | Train Accuracy: 0.8266539573669434 |  Validation loss: 1.8338661704744612 | Validation Accuracy: 0.8394354581832886\n",
      "\t verbs , and other parts of speech occurring\n",
      "_te                                     _\n",
      "Epoch 56: Train loss: 1.585675086826086 | Train Accuracy: 0.8173999786376953 |  Validation loss: 1.6287626368658883 | Validation Accuracy: 0.8210152387619019\n",
      "\t locally are filled as far as possible from people\n",
      "_wh                                            _\n",
      "Epoch 57: Train loss: 1.5780427902936935 | Train Accuracy: 0.8181005716323853 |  Validation loss: 1.6569268192563738 | Validation Accuracy: 0.8243125677108765\n",
      "\t geography . Geography , too , names the\n",
      "_to                             _\n",
      "Epoch 58: Train loss: 1.5725781247019768 | Train Accuracy: 0.8154560327529907 |  Validation loss: 1.6558086361203874 | Validation Accuracy: 0.8147149682044983\n",
      "\t some dreadful ravine , or beset by\n",
      "_to                                _\n",
      "Epoch 59: Train loss: 1.5750438757240772 | Train Accuracy: 0.8134909272193909 |  Validation loss: 1.5740123306001936 | Validation Accuracy: 0.8157213926315308\n",
      "\t amount of good over and above relieving\n",
      "_th                                        _\n",
      "Epoch 60: Train loss: 1.569373045116663 | Train Accuracy: 0.8315662741661072 |  Validation loss: 1.6697153534208025 | Validation Accuracy: 0.8443633317947388\n",
      "\t the glaring beacon of a lighthouse .\n",
      "_th                              _\n",
      "Epoch 61: Train loss: 1.5714684687554836 | Train Accuracy: 0.8167592883110046 |  Validation loss: 1.6517563377107893 | Validation Accuracy: 0.819029688835144\n",
      "\t a nostalgic affection for them .\n",
      "_te                      _\n",
      "Epoch 62: Train loss: 1.5669018253684044 | Train Accuracy: 0.813125491142273 |  Validation loss: 1.617934192929949 | Validation Accuracy: 0.818208634853363\n",
      "\t the diplomatist Lord Ponsonby , had written to\n",
      "_wa                                       _\n",
      "Epoch 63: Train loss: 1.56717162206769 | Train Accuracy: 0.8149599432945251 |  Validation loss: 1.627979380743844 | Validation Accuracy: 0.8209456205368042\n",
      "\t a nostalgic affection for them .\n",
      "_te                e          _\n",
      "Epoch 64: Train loss: 1.5571935549378395 | Train Accuracy: 0.8084300756454468 |  Validation loss: 1.6308705806732178 | Validation Accuracy: 0.8096547722816467\n",
      "\t be on the point of death , the new German curative\n",
      "_te                                               _\n",
      "Epoch 65: Train loss: 1.5634951144456863 | Train Accuracy: 0.8188342452049255 |  Validation loss: 1.624631132398333 | Validation Accuracy: 0.8240453004837036\n",
      "\t fresher courses at the ages of fifty-five and sixty :\n",
      "_wh                                               _\n",
      "Epoch 66: Train loss: 1.553504291921854 | Train Accuracy: 0.816804051399231 |  Validation loss: 1.6290040867669242 | Validation Accuracy: 0.8184987902641296\n",
      "\t of Home , Foreign Secretary , and with other\n",
      "_Ae                e                e   _\n",
      "Epoch 67: Train loss: 1.5656503774225712 | Train Accuracy: 0.8030855059623718 |  Validation loss: 1.6504579271589006 | Validation Accuracy: 0.803283154964447\n",
      "\t Centres . The former consists of representatives from\n",
      "_ta                                               _\n",
      "Epoch 68: Train loss: 1.559160590171814 | Train Accuracy: 0.8173223733901978 |  Validation loss: 1.6558096408843994 | Validation Accuracy: 0.8230361342430115\n",
      "\t health permitted him to enjoy\n",
      "_te                            _\n",
      "Epoch 69: Train loss: 1.5465740114450455 | Train Accuracy: 0.8125099539756775 |  Validation loss: 1.6680375507899694 | Validation Accuracy: 0.8158001899719238\n",
      "\t ' What a frightful event ! ' he wrote . ' I tremble !\n",
      "_th t                                            _\n",
      "Epoch 70: Train loss: 1.542882762849331 | Train Accuracy: 0.8144673705101013 |  Validation loss: 1.6291706562042236 | Validation Accuracy: 0.8215988874435425\n",
      "\t It 's hereabouts that the budge takes to the bottle , but I\n",
      "_tt                                                  _\n",
      "Epoch 71: Train loss: 1.5516232922673225 | Train Accuracy: 0.8114662766456604 |  Validation loss: 1.6001774413245065 | Validation Accuracy: 0.8267474174499512\n",
      "\t will matter most . ' Strange how just\n",
      "_tee                             _\n",
      "Epoch 72: Train loss: 1.5443415343761444 | Train Accuracy: 0.8088062405586243 |  Validation loss: 1.6910498142242432 | Validation Accuracy: 0.823625385761261\n",
      "\t of mind akin to the atoms of matter ?\n",
      "_te                e                    _\n",
      "Epoch 73: Train loss: 1.5610854625701904 | Train Accuracy: 0.8102293610572815 |  Validation loss: 1.651603136743818 | Validation Accuracy: 0.817675769329071\n",
      "\t whose presence I could almost smell ,\n",
      "_th                                 _\n",
      "Epoch 74: Train loss: 1.5390656106173992 | Train Accuracy: 0.7924322485923767 |  Validation loss: 1.6856038570404053 | Validation Accuracy: 0.7954998016357422\n",
      "\t But lace-making is by no means a lost\n",
      "_th                                   _\n",
      "Epoch 75: Train loss: 1.5396181233227253 | Train Accuracy: 0.8010419011116028 |  Validation loss: 1.6692648785454887 | Validation Accuracy: 0.8087206482887268\n",
      "\t tremble ! What infatuation ! Personally\n",
      "_ta          a                       _\n",
      "Epoch 76: Train loss: 1.535688903182745 | Train Accuracy: 0.8088716268539429 |  Validation loss: 1.6884040491921561 | Validation Accuracy: 0.823916494846344\n",
      "\t Bertram's face was grim . \" You think it was the Snort ,\n",
      "_tee               e                               _\n",
      "Epoch 77: Train loss: 1.5453779213130474 | Train Accuracy: 0.7941341996192932 |  Validation loss: 2.4331909929003035 | Validation Accuracy: 0.8009724617004395\n",
      "\t alleviation of his painful malady . None of the numerous\n",
      "_ton  e                                          e   ee  _\n",
      "Epoch 78: Train loss: 1.5528796799480915 | Train Accuracy: 0.8149199485778809 |  Validation loss: 1.6559185981750488 | Validation Accuracy: 0.8410348296165466\n",
      "\t a roll or a codex . Rolls were prepared for\n",
      "_te                                     _\n",
      "Epoch 79: Train loss: 1.539708185940981 | Train Accuracy: 0.8034901022911072 |  Validation loss: 1.6403552293777466 | Validation Accuracy: 0.8151057958602905\n",
      "\t as possible . Anyway the Parsifal affair was far\n",
      "_so  re          a e          a   aa         _\n",
      "Epoch 80: Train loss: 1.5352097861468792 | Train Accuracy: 0.7998160719871521 |  Validation loss: 2.8006318977900913 | Validation Accuracy: 0.8097140192985535\n",
      "\t illusion in the Conservative Party that their\n",
      "_th                                      _\n",
      "Epoch 81: Train loss: 1.5354484878480434 | Train Accuracy: 0.8070322275161743 |  Validation loss: 1.6567575590951102 | Validation Accuracy: 0.8280956149101257\n",
      "\t talks with the Soviet Union , for the establishment\n",
      "_thot e           a     a           e     _\n",
      "Epoch 82: Train loss: 1.5360272973775864 | Train Accuracy: 0.7983672022819519 |  Validation loss: 1.6191450016839164 | Validation Accuracy: 0.8075535893440247\n",
      "\t talks with the Soviet Union , for the establishment\n",
      "_tart e     e     a                 e       _\n",
      "Epoch 83: Train loss: 1.5223397128283978 | Train Accuracy: 0.7808721661567688 |  Validation loss: 1.6868806396211897 | Validation Accuracy: 0.7918983697891235\n",
      "\t asked to help . To begin with , both tempe-\n",
      "_shee      h                       _\n",
      "Epoch 84: Train loss: 1.5132431648671627 | Train Accuracy: 0.7952044606208801 |  Validation loss: 1.8622795002801078 | Validation Accuracy: 0.8168293237686157\n",
      "\t other hand , there is really no replacement\n",
      "_aee                                 _\n",
      "Epoch 85: Train loss: 1.5225772969424725 | Train Accuracy: 0.7865699529647827 |  Validation loss: 1.6248806033815657 | Validation Accuracy: 0.7924948334693909\n",
      "\t in the chancery , or the corridors of the United\n",
      "_wa  ee     eee   a e a      a e eeeee eetee     _\n",
      "Epoch 86: Train loss: 1.5115368999540806 | Train Accuracy: 0.7829613089561462 |  Validation loss: 1.6241659607206071 | Validation Accuracy: 0.8010081648826599\n",
      "\t fore ! ' After the curry , I wanted only to go upstairs to\n",
      "_toe                                                 _\n",
      "Epoch 87: Train loss: 1.522438932210207 | Train Accuracy: 0.7927958369255066 |  Validation loss: 1.6504799297877721 | Validation Accuracy: 0.811263382434845\n",
      "\t Laud made a positive approach . He set out to increase\n",
      "_hhm       e     e  e          e  e   ee      eee_\n",
      "Epoch 88: Train loss: 1.5216547474265099 | Train Accuracy: 0.7822377681732178 |  Validation loss: 1.6127292939594813 | Validation Accuracy: 0.7953440546989441\n",
      "\t little spiv from Brighton seemed an\n",
      "_aaee   a    a e   eea tt t    t  e_\n",
      "Epoch 89: Train loss: 1.5071375146508217 | Train Accuracy: 0.7832821607589722 |  Validation loss: 1.6243371622903007 | Validation Accuracy: 0.7937849760055542\n",
      "\t boy . He settled me into the car\n",
      "_aee      e                       _\n",
      "Epoch 90: Train loss: 1.4955684319138527 | Train Accuracy: 0.7954097986221313 |  Validation loss: 1.6323820352554321 | Validation Accuracy: 0.8189897537231445\n",
      "\t of mind akin to the atoms of matter ?\n",
      "_t  se                          r     _\n",
      "Epoch 91: Train loss: 1.501545112580061 | Train Accuracy: 0.7742718458175659 |  Validation loss: 1.6692640440804618 | Validation Accuracy: 0.7848596572875977\n",
      "\t peace of mind ? Philip put out\n",
      "_tor a aa ai a ia   aa        _\n",
      "Epoch 92: Train loss: 1.4951366782188416 | Train Accuracy: 0.7822945713996887 |  Validation loss: 1.6807881253106254 | Validation Accuracy: 0.7958641052246094\n",
      "\t and knew they were back . I wondered , as I\n",
      "_Ioe                                    e  _\n",
      "Epoch 93: Train loss: 1.4986364021897316 | Train Accuracy: 0.7805931568145752 |  Validation loss: 1.6441707100187029 | Validation Accuracy: 0.7942865490913391\n",
      "\t home , or perhaps a student studying to\n",
      "_ternee e  a aaa a         a         s_\n",
      "Epoch 94: Train loss: 1.4829111658036709 | Train Accuracy: 0.781079888343811 |  Validation loss: 1.6421123402459281 | Validation Accuracy: 0.8023846745491028\n",
      "\t to 19 .\n",
      "_t  ._\n",
      "Epoch 95: Train loss: 1.4700295347720385 | Train Accuracy: 0.7740445733070374 |  Validation loss: 1.6629765374319894 | Validation Accuracy: 0.785399854183197\n",
      "\t Again the resentment was widespread and\n",
      "_aiit         nan       a    e       eee_\n",
      "Epoch 96: Train loss: 1.4908931143581867 | Train Accuracy: 0.7691413164138794 |  Validation loss: 1.670671718461173 | Validation Accuracy: 0.7857595086097717\n",
      "\t the river . The film starts off brightly enough but ,\n",
      "_te  e                              e           _\n",
      "Epoch 97: Train loss: 1.482017882168293 | Train Accuracy: 0.7712806463241577 |  Validation loss: 1.6861375059400285 | Validation Accuracy: 0.7952262759208679\n",
      "\t All types of trader have been encouraged ,\n",
      "_Iee nnae      ee e   e   a           e_\n",
      "Epoch 98: Train loss: 1.4782173559069633 | Train Accuracy: 0.7648007273674011 |  Validation loss: 2.6976027148110524 | Validation Accuracy: 0.7799637913703918\n",
      "\t homoeopathy had been brought to\n",
      "_whmmaaa a aaa a  a a  a      e_\n",
      "Epoch 99: Train loss: 1.4661418534815311 | Train Accuracy: 0.7705107927322388 |  Validation loss: 1.7216495445796423 | Validation Accuracy: 0.7956387400627136\n",
      "\t electrodes are completely separated from one\n",
      "_te   ee           e      ee   e   e         _\n",
      "Epoch 100: Train loss: 1.4750131703913212 | Train Accuracy: 0.7656731605529785 |  Validation loss: 1.695083567074367 | Validation Accuracy: 0.7815488576889038\n",
      "\t losses will be even greater . This does not\n",
      "_Ih        e                             tt tt  _\n",
      "Epoch 101: Train loss: 1.4695838615298271 | Train Accuracy: 0.7655478715896606 |  Validation loss: 1.6873029470443726 | Validation Accuracy: 0.7868772149085999\n",
      "\t who , under apartheid , will be forced back to\n",
      "_Tho     n                                  _\n",
      "Epoch 102: Train loss: 1.472543466836214 | Train Accuracy: 0.7726454138755798 |  Validation loss: 1.723443797656468 | Validation Accuracy: 0.7934834361076355\n",
      "\t work on Sundays , \" he said . \" They 're worse off than\n",
      "_whee      a ee     e         e                     _\n",
      "Epoch 103: Train loss: 1.4714419655501842 | Train Accuracy: 0.7767522931098938 |  Validation loss: 1.6941514015197754 | Validation Accuracy: 0.8064391016960144\n",
      "\t And these had been his great qualities ,\n",
      "_the heee eeea re   r a t      ss_\n",
      "Epoch 104: Train loss: 1.4710226356983185 | Train Accuracy: 0.7564616799354553 |  Validation loss: 1.7292462927954537 | Validation Accuracy: 0.7745121717453003\n",
      "\t passes on through two pairs of\n",
      "_thleie   aa a    nn   n a     _\n",
      "Epoch 105: Train loss: 1.4678911939263344 | Train Accuracy: 0.7726933360099792 |  Validation loss: 1.7085601772580827 | Validation Accuracy: 0.7963425517082214\n",
      "\t when we met at tea at the rectory\n",
      "_thes   eeh   e  ehee       e  e _\n",
      "Epoch 106: Train loss: 1.453006662428379 | Train Accuracy: 0.7614324688911438 |  Validation loss: 1.690612520490374 | Validation Accuracy: 0.7929372787475586\n",
      "\t boy . He settled me into the car\n",
      "_t        a        h             _\n",
      "Epoch 107: Train loss: 1.4519530609250069 | Train Accuracy: 0.7665910124778748 |  Validation loss: 1.704518505505153 | Validation Accuracy: 0.8016349077224731\n",
      "\t and a top-hat .\n",
      "_thr        _\n",
      "Epoch 108: Train loss: 1.4391814544796944 | Train Accuracy: 0.7609729766845703 |  Validation loss: 1.7330120291028703 | Validation Accuracy: 0.7956368327140808\n",
      "\t Labour has to have an adequate number of\n",
      "_te nhe           i      aa       e e  n e_\n",
      "Epoch 109: Train loss: 1.439009115099907 | Train Accuracy: 0.7514688372612 |  Validation loss: 1.691288845879691 | Validation Accuracy: 0.7827643156051636\n",
      "\t them .\n",
      "_thr   ._\n",
      "Epoch 110: Train loss: 1.439462073147297 | Train Accuracy: 0.7736644148826599 |  Validation loss: 1.7108257498059953 | Validation Accuracy: 0.8010497093200684\n",
      "\t that Weaver once had Communist affiliations .\n",
      "_th   eea    a a      aaaa a     iiii i ei _\n",
      "Epoch 111: Train loss: 1.4416733160614967 | Train Accuracy: 0.7568685412406921 |  Validation loss: 1.7330659117017473 | Validation Accuracy: 0.7892821431159973\n"
     ]
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=16, recognizer_lr=5e-4,\n",
    "              betas=(0, 0.999), num_epochs=500, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n",
      "tensor([[0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0139, 0.0137],\n",
      "        [0.0138, 0.0138, 0.0138,  ..., 0.0138, 0.0134, 0.0138],\n",
      "        [0.0137, 0.0138, 0.0137,  ..., 0.0137, 0.0135, 0.0137],\n",
      "        ...,\n",
      "        [0.0140, 0.0140, 0.0140,  ..., 0.0139, 0.0135, 0.0139],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0138, 0.0139, 0.0137],\n",
      "        [0.0137, 0.0137, 0.0137,  ..., 0.0137, 0.0138, 0.0137]],\n",
      "       grad_fn=<SoftmaxBackward0>) torch.Size([82, 73])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "\n",
    "print(torch.softmax(recognizer(image.unsqueeze(0)), 1), torch.softmax(recognizer(image.unsqueeze(0)), 1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
