{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "c:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "from torchmetrics.text import CharErrorRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 39049\n"
     ]
    }
   ],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved_recognizer.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel, lambda x: x)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "\n",
    "            # RECOGNIZER EXCLUSIVE\n",
    "            # Some ways to augment images:\n",
    "            # - horizontally compress\n",
    "            # - horizontally stretch\n",
    "            # - vertically compress\n",
    "            # - vertically stretch\n",
    "            if len(transcription) > 75:\n",
    "                for i in range(int(len(transcription)*6)):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_compress)\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hc{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "                    \n",
    "            if len(transcription) >= 65:\n",
    "                for i in range(int(len(transcription))):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_compress)\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hc{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "                    \n",
    "            elif len(transcription) >= 50:\n",
    "                for i in range(4):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_compress)\n",
    "                    # Just in case after stretching we have cut off boundary\n",
    "                    if resized_tensor is None:\n",
    "                        continue\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hs{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "            \n",
    "            elif len(transcription) >= 35:\n",
    "                for i in range(1):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_stretch)\n",
    "                    # Just in case after stretching we have cut off boundary\n",
    "                    if resized_tensor is None:\n",
    "                        continue\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hs{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "                    \n",
    "            elif len(transcription) >= 25:\n",
    "                for i in range(4):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_stretch)\n",
    "                    # Just in case after stretching we have cut off boundary\n",
    "                    if resized_tensor is None:\n",
    "                        continue\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hs{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "            \n",
    "            elif len(transcription) < 25:\n",
    "                for i in range(20):\n",
    "                    resized_tensor = process_image(tmp_image, graylevel, horizontally_stretch)\n",
    "                    # Just in case after stretching we have cut off boundary\n",
    "                    if resized_tensor is None:\n",
    "                        continue\n",
    "                    image_pt_path = os.path.join(image_path_head, f\"{image_id}hs{i}.pt\")\n",
    "                    torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "                    # A fully valid image\n",
    "                    # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "                    fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "                    valid_samples += 1\n",
    "\n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def horizontally_compress(cv_image):\n",
    "    \"\"\" by random percent \"\"\"\n",
    "    factor = 1 - random.random()/2\n",
    "    height, width = cv_image.shape\n",
    "    output = cv.resize(cv_image, (width, int(height*factor)), interpolation=cv.INTER_AREA)\n",
    "    return output\n",
    "    ...\n",
    "\n",
    "\n",
    "def horizontally_stretch(cv_image):\n",
    "    \"\"\" by random percent\"\"\"\n",
    "    factor = 1 + random.random()/2\n",
    "    height, width = cv_image.shape\n",
    "    output = cv.resize(cv_image, (width, int(height*factor)), interpolation=cv.INTER_LINEAR)\n",
    "    return output\n",
    "    ...\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel, additional_func=None):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # Apply additional filter\n",
    "    output = additional_func(output)\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "\n",
    "    if padding_to_add < 0:\n",
    "        return\n",
    "\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i])), value=0) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "images\n",
      "both\n",
      "31240 7809\n"
     ]
    }
   ],
   "source": [
    "#line_transcription_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\", ty=\"txt\")\n",
    "#line_image_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved_recognizer.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "#line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "#line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "#line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(64*5))\n",
    "#line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(10))\n",
    "\n",
    "#line_image_dataset_train = Subset(line_image_dataset_train, range(64*5))\n",
    "#line_image_dataset_val = Subset(line_image_dataset_val, range(10))\n",
    "\n",
    "#line_dataset_train = Subset(line_dataset_train, range(19000))\n",
    "#line_dataset_val = Subset(line_dataset_val, range(1000))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "#print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "#print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 840., 2247., 1890., 2457., 2730., 1245., 1780., 2517., 2288.,\n",
       "        2080., 2576., 2449., 2900., 1460.,  750., 2493., 1524., 1333.,\n",
       "         685., 2805.]),\n",
       " array([ 4. ,  7.9, 11.8, 15.7, 19.6, 23.5, 27.4, 31.3, 35.2, 39.1, 43. ,\n",
       "        46.9, 50.8, 54.7, 58.6, 62.5, 66.4, 70.3, 74.2, 78.1, 82. ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = line_dataset.lines_df[\"transcription_len\"].to_numpy()\n",
    "plt.hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([59, 61, 60, 66, 54, 65,  1, 47, 65, 54, 61, 64, 51,  1, 69, 54, 55, 58,\n",
       "         51,  1, 36, 51, 64, 55, 49, 58, 51, 65,  1, 69, 47, 65,  1, 64, 51, 52,\n",
       "         55, 66, 66, 55, 60, 53,  1, 55, 60,  1, 66, 54, 51,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'months ashore while Pericles was refitting in the')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[9995]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 73, 82])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4,2))\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        #self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4,2))\n",
    "        #xself.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        #self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,2))\n",
    "        #self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=4, bidirectional=True, batch_first=True)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(505, 82)\n",
    "        \n",
    "        #self.fc = nn.Linear(10, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.conv1(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        #img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        #print(img.shape)\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.lrelu(self.dense2(img))\n",
    "        \n",
    "        #img = self.fc(img)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)\n",
    "    \n",
    "recog = Recognizer()\n",
    "a =recog(torch.randn((1, 1, 32, 512), dtype=torch.float32))\n",
    "#print(recog)\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function):\n",
    "    recognizer.eval() \n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(val_line_dataset_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device) \n",
    "        line_text_batch = line_text_batch.to(device)\n",
    "        recognizer_outputs = recognizer(line_image_batch)\n",
    "        recognizer_loss = recognizer_loss_function(F.log_softmax(recognizer_outputs, 1), line_text_batch)\n",
    "        \n",
    "        total_loss += recognizer_loss.item()\n",
    "        total_epoch += 1\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    \n",
    "    #print(recognizer_outputs, recognizer_outputs.shape)\n",
    "    #print(torch.argmax(recognizer_outputs, 1), torch.argmax(recognizer_outputs, 1).shape)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_recog_accuracy(preds, target):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the recognizer with character error rate\n",
    "    which is based on edit distance\n",
    "\n",
    "    Params:\n",
    "        preds: a list of prediction strings\n",
    "        targets: a list of target strings\n",
    "\n",
    "    Returns:\n",
    "        An integer, the character error rate average across\n",
    "        all predictions and targets\n",
    "    \"\"\"\n",
    "\n",
    "    cer = CharErrorRate()\n",
    "    return cer(preds, target)\n",
    "\n",
    "def create_strings_from_tensor(int_tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        int_tensor: A shape (N, 82) tensor where each row corresponds to\n",
    "        a integer mapping of a string. Includes padding\n",
    "    \n",
    "    Returns:\n",
    "        A list of N strings\n",
    "    \"\"\"\n",
    "\n",
    "    strings = []\n",
    "    for string_map in int_tensor:\n",
    "        strings.append(\"\".join([int_to_char[int(i)] for i in string_map[string_map != 0]]))\n",
    "    return strings\n",
    "    \n",
    "\n",
    "def get_accuracy(device, recognizer, recognizer_loader):\n",
    "\n",
    "    acc = 0\n",
    "    \n",
    "    for i, (line_image_batch, line_text_batch) in enumerate(recognizer_loader, 0):\n",
    "        line_image_batch = line_image_batch.to(device)\n",
    "        line_text_batch\n",
    "        recognizer_outputs = torch.argmax(recognizer(line_image_batch), 1)\n",
    "        recognizer_pred = create_strings_from_tensor(recognizer_outputs)\n",
    "        \n",
    "        label = create_strings_from_tensor(line_text_batch)\n",
    "        \n",
    "        acc += calculate_recog_accuracy(recognizer_pred, label)\n",
    "        \n",
    "        \n",
    "    return acc / (i+1)\n",
    "        \n",
    "    \n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    \n",
    "    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n",
    "    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n",
    "    \n",
    "    n = len(train_loss) # number of epochs\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Train Loss\", \"Validation Loss\"])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend([\"Train Error\", \"Validation Error\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(recognizer, \n",
    "              train_line_dataset, val_line_dataset, \n",
    "              batch_size=64, recognizer_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    #print(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_dataset_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_dataset_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #print(len(train_line_dataset_loader))\n",
    "\n",
    "    recognizer_optimizer = optim.Adam(recognizer.parameters(), lr=recognizer_lr)\n",
    "    \n",
    "    recognizer_loss_function = nn.NLLLoss()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(recognizer.parameters(), max_norm=0.5)\n",
    "    recognizer_train_losses = np.zeros(num_epochs)\n",
    "    recognizer_train_accuracies = np.zeros(num_epochs)\n",
    "    recognizer_val_losses = np.zeros(num_epochs)\n",
    "    recognizer_val_accuracies = np.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        recognizer_train_loss = 0\n",
    "\n",
    "        for i, (line_image_batch, line_text_batch) in enumerate(train_line_dataset_loader):\n",
    "#             print(\"epoch\", epoch, \"batch\", i)\n",
    "#             print(\"line_image_batch.shape\", line_image_batch.shape)\n",
    "            cur_batch_size, _ = line_text_batch.shape\n",
    "            # print(line_text_batch.shape)\n",
    "\n",
    "#             print(\"line_text_batch.shape\", line_text_batch.shape)\n",
    "            test = line_text_batch[0]\n",
    "            test = test[test.nonzero()]\n",
    "            test = \"\".join([int_to_char[int(i)] for i in test])\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            line_text_batch = line_text_batch.to(device)\n",
    "            plt.imshow(line_image_batch[0].cpu().squeeze(0), cmap='gray')\n",
    "            #print(line_text_batch, line_text_batch.shape)\n",
    "            recognizer_outputs = recognizer(line_image_batch)  # Mult factor to incentivize padding\n",
    "   \n",
    "            # print(recognizer_outputs, recognizer_outputs.shape)\n",
    "            # print(line_text_batch, line_text_batch.shape)\n",
    "#             test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "\n",
    "#             Refer to CTC documentation\n",
    "            #line_text_batch_pad_remove = [line_text[line_text.nonzero().squeeze(1)] for line_text in line_text_batch]  # Array of tensors\n",
    "            #target_lengths = torch.tensor([len(line_text_pad_remove) for line_text_pad_remove in line_text_batch_pad_remove])\n",
    "            #target = torch.cat(line_text_batch_pad_remove)\n",
    "            #print(target, target.shape)\n",
    "            #input_lengths = torch.full(size=(cur_batch_size,), fill_value=248)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # torch.argmax(F.log_softmax(recognizer_outputs, 2), 1),\n",
    "                F.log_softmax(recognizer_outputs, 1),  # Requires number of classes to move from 2nd to 1st dimension after log_softmax\n",
    "                line_text_batch\n",
    "            )\n",
    "            test2 = recognizer_outputs[0,:,:]\n",
    "            test2 = torch.argmax(test2, dim=0)  # Removed 0 dim\n",
    "            test2 = test2[test2.nonzero()]\n",
    "            test2 = \"\".join([int_to_char[int(i)] for i in test2])\n",
    "            \n",
    "\n",
    "            recognizer_loss.backward()\n",
    "            recognizer_optimizer.step()\n",
    "            recognizer_optimizer.zero_grad()\n",
    "    \n",
    "            recognizer_train_loss += recognizer_loss.item()\n",
    "        \n",
    "        print(\"\\t\",test)\n",
    "        print(f\"_{test2}_\")\n",
    "        recognizer_train_losses[epoch] = float(recognizer_train_loss) / (i+1)\n",
    "        \n",
    "        recognizer.eval()\n",
    "        recognizer_val_losses[epoch] = evaluate(device, recognizer, val_line_dataset_loader, recognizer_loss_function)\n",
    "        recognizer.train()\n",
    "        \n",
    "        recognizer_train_accuracies[epoch] = get_accuracy(device, recognizer, train_line_dataset_loader)\n",
    "        recognizer_val_accuracies[epoch]= get_accuracy(device, recognizer, val_line_dataset_loader)\n",
    "        \n",
    "        print((\"Epoch {}: Train loss: {} | Train Accuracy: {} | \"+\n",
    "            \" Validation loss: {} | Validation Accuracy: {}\").format(\n",
    "                    epoch + 1,\n",
    "                    recognizer_train_losses[epoch],\n",
    "                    recognizer_train_accuracies[epoch],\n",
    "                    recognizer_val_losses[epoch],\n",
    "                    recognizer_val_accuracies[epoch]))\n",
    "\n",
    "        model_path = get_model_name(recognizer.name, batch_size, recognizer_lr, epoch)\n",
    "        torch.save(recognizer.state_dict(), os.path.join(\"./recognizers\", model_path))\n",
    "        model_path_const_batch = get_model_name(recognizer.name, batch_size, recognizer_lr, -1)\n",
    "\n",
    "        np.savetxt(\"./recognizers/{}_train_loss.csv\".format(model_path_const_batch), recognizer_train_losses)\n",
    "        np.savetxt(\"./recognizers/{}_val_loss.csv\".format(model_path_const_batch),  recognizer_val_losses)\n",
    "        np.savetxt(\"./recognizers/{}_train_acc.csv\".format(model_path_const_batch), recognizer_train_accuracies)\n",
    "        np.savetxt(\"./recognizers/{}_val_acc.csv\".format(model_path_const_batch), recognizer_val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_curve(\"./recognizers/model_recognizer_bs8_lr0.0005_epoch-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t fifty thousand feet .\n",
      "_aae      i          _\n",
      "Epoch 1: Train loss: 1.6149380786165537 | Train Accuracy: 0.6469097137451172 |  Validation loss: 1.3093003794139093 | Validation Accuracy: 0.6463625431060791\n",
      "\t to play an important part in my life . For five years in succession till\n",
      "_tn plat  n t oortet  p rt tt  ttttt  n eoe oine aeershnotswsseseion iilldTbwbc_\n",
      "Epoch 2: Train loss: 1.2116960652811732 | Train Accuracy: 0.7296003699302673 |  Validation loss: 1.526802482905139 | Validation Accuracy: 0.7304742336273193\n",
      "\t meat or fish burnt and roasted in the\n",
      "_aaat                     e              _\n",
      "Epoch 3: Train loss: 1.099505235229961 | Train Accuracy: 0.5077443718910217 |  Validation loss: 1.0327333110930854 | Validation Accuracy: 0.5138401985168457\n",
      "\t Russian Embassy .\n",
      "_Dieseee    an_\n",
      "Epoch 4: Train loss: 1.0083520597211835 | Train Accuracy: 0.4892277717590332 |  Validation loss: 0.9776744550606855 | Validation Accuracy: 0.49727609753608704\n",
      "\t in her quivering flesh . \" You 've an answer - of sorts - for everything , \" she\n",
      "_in her quivering flesh . \" You 've an answer - of sorts - for everything , \" she_\n",
      "Epoch 5: Train loss: 0.9751862183003358 | Train Accuracy: 0.4710741937160492 |  Validation loss: 0.9345358673196206 | Validation Accuracy: 0.47565126419067383\n",
      "\t becomes an offence punishable with imprisonment\n",
      "_seepree       oot e   e eeeeeee  e      tmttt_\n",
      "Epoch 6: Train loss: 0.9272616544306737 | Train Accuracy: 0.4510500133037567 |  Validation loss: 0.8990969495007057 | Validation Accuracy: 0.46115776896476746\n",
      "\t gral part of the story .\n",
      "_sec  sartttf hhteeee  ._\n",
      "Epoch 7: Train loss: 0.8699983789772749 | Train Accuracy: 0.42779770493507385 |  Validation loss: 0.8470218561841375 | Validation Accuracy: 0.439950555562973\n",
      "\t waving , unkissed , from the window . And Dai , on the pavement , knowing in his\n",
      "_waving , unkissed , from the window . And Dai , on the pavement , knowing in his_\n",
      "Epoch 8: Train loss: 0.8217182162810456 | Train Accuracy: 0.40767166018486023 |  Validation loss: 0.8052992580559566 | Validation Accuracy: 0.42080768942832947\n",
      "\t waving , unkissed , from the window . And Dai , on the pavement , knowing in his\n",
      "_waving , unkissed , from the window . And Dai , on the pavement , knowing in his_\n",
      "Epoch 9: Train loss: 0.7654554725427862 | Train Accuracy: 0.3804965913295746 |  Validation loss: 0.7705214145224438 | Validation Accuracy: 0.398792564868927\n",
      "\t to give the system a trial , adding that it\n",
      "_to  ree he  ssese              nd  e  e  _\n",
      "Epoch 10: Train loss: 0.724103789526144 | Train Accuracy: 0.3652205169200897 |  Validation loss: 0.7317941704185388 | Validation Accuracy: 0.38420388102531433\n",
      "\t directed \" Six-Five Special , \" Turner is aiming to go into free-lance\n",
      "_directed \" Six-Five Special , \" Turner is aiming to go into free-lance_\n",
      "Epoch 11: Train loss: 0.694354053722902 | Train Accuracy: 0.354236364364624 |  Validation loss: 0.7123111792408242 | Validation Accuracy: 0.37700504064559937\n",
      "\t compartment when the train left , but this did not help much , since it\n",
      "_compartment when the train left , but this did not help much , since it_\n",
      "Epoch 12: Train loss: 0.6588035696850334 | Train Accuracy: 0.33229386806488037 |  Validation loss: 0.6872047064673373 | Validation Accuracy: 0.3576715588569641\n",
      "\t incredibly ancient car , surrounded by innumerable friends and relatives .\n",
      "_incredibly ancient car , surrounded by innumerable friends and relatives ._\n",
      "Epoch 13: Train loss: 0.6244711190097215 | Train Accuracy: 0.3197272717952728 |  Validation loss: 0.6612106588732393 | Validation Accuracy: 0.34829649329185486\n",
      "\t The beleaguered passengers peered out of\n",
      "_Tee  eecagpeanits naioiigtptt  ey_\n",
      "Epoch 14: Train loss: 0.5897841194321409 | Train Accuracy: 0.30410248041152954 |  Validation loss: 0.6446374767804767 | Validation Accuracy: 0.33832189440727234\n",
      "\t late November , he was ' suffering as usual ' , but hoped ,\n",
      "_late Notroore , he was   s  neas i ae   aa    ,  u  hpved,,_\n",
      "Epoch 15: Train loss: 0.5600248835740013 | Train Accuracy: 0.28866294026374817 |  Validation loss: 0.6096425257502133 | Validation Accuracy: 0.3292698860168457\n",
      "\t openly intimidating members of Earl Russell's nuclear-\n",
      "_opeul  intiaianning te eoee oo ool  ellsslloslsledda_\n",
      "Epoch 16: Train loss: 0.5280918331507941 | Train Accuracy: 0.26904574036598206 |  Validation loss: 0.5834743839204468 | Validation Accuracy: 0.3147065043449402\n",
      "\t any idea of reading music . This was the village doctor who was an\n",
      "_any idea of reading music . This was the village doctor who was an_\n",
      "Epoch 17: Train loss: 0.5120474583700672 | Train Accuracy: 0.26117369532585144 |  Validation loss: 0.573107139955007 | Validation Accuracy: 0.3081728518009186\n",
      "\t success in France and Italy , and that\n",
      "_buccess  h  rldrlen   r  hh , tnh hth_\n",
      "Epoch 18: Train loss: 0.47242334698441363 | Train Accuracy: 0.24436698853969574 |  Validation loss: 0.5469705755814019 | Validation Accuracy: 0.29410678148269653\n",
      "\t The third article excluded any allegorical interpretation of the Gospel .\n",
      "_The third article excluded any allegorical interpretation of the Gospel ._\n",
      "Epoch 19: Train loss: 0.4514247102618501 | Train Accuracy: 0.2308264672756195 |  Validation loss: 0.5311045284885718 | Validation Accuracy: 0.284710168838501\n",
      "\t century .\n",
      "_century ._\n",
      "Epoch 20: Train loss: 0.430501837622453 | Train Accuracy: 0.21782033145427704 |  Validation loss: 0.5085278866446064 | Validation Accuracy: 0.2747097611427307\n",
      "\t milk was reduced ; since 1957 it has again declined . Most of the\n",
      "_milk was reduced ; since 1957 it has again declined . Most of the_\n",
      "Epoch 21: Train loss: 0.4072269401138603 | Train Accuracy: 0.21029174327850342 |  Validation loss: 0.49494341723256274 | Validation Accuracy: 0.26899227499961853\n",
      "\t new appointment , at nine a.m. precisely , dressed\n",
      "_nes apppenmmen  ,  t nann aiieaprege.eey,  dee sed_\n",
      "Epoch 22: Train loss: 0.3888384675056311 | Train Accuracy: 0.2060217708349228 |  Validation loss: 0.5027334232141719 | Validation Accuracy: 0.2662874460220337\n",
      "\t was found at Waterloo . Neither the sisters nor Mr Berry thought it\n",
      "_was found at Waterloo . Neither the sisters nor Mr Berry thought it_\n",
      "Epoch 23: Train loss: 0.37064345405889 | Train Accuracy: 0.19297412037849426 |  Validation loss: 0.4789494203940747 | Validation Accuracy: 0.2551102340221405\n",
      "\t cured . '\n",
      "_cured . '_\n",
      "Epoch 24: Train loss: 0.3552391873349444 | Train Accuracy: 0.18105961382389069 |  Validation loss: 0.4592222860061327 | Validation Accuracy: 0.24467377364635468\n",
      "\t Phnom Penh , Cambodia , to invite Prince\n",
      "_Thtom  Kbt  ,acmbnoal      idne eineeslee_\n",
      "Epoch 25: Train loss: 0.34906930784490936 | Train Accuracy: 0.17468808591365814 |  Validation loss: 0.45409725835256587 | Validation Accuracy: 0.24087651073932648\n",
      "\t great deal of tiredness which comes from\n",
      "_ghtbdtde e  f tiisnat s na ia er is leaa_\n",
      "Epoch 26: Train loss: 0.3251952110547644 | Train Accuracy: 0.16562636196613312 |  Validation loss: 0.438081499683278 | Validation Accuracy: 0.22878582775592804\n",
      "\t into it .\n",
      "_intoait ._\n",
      "Epoch 27: Train loss: 0.3145514715397337 | Train Accuracy: 0.1616252362728119 |  Validation loss: 0.4305476767182333 | Validation Accuracy: 0.22755731642246246\n",
      "\t waving , unkissed , from the window . And Dai , on the pavement , knowing in his\n",
      "_waving , unkissed , from the window . And Dai , on the pavement , knowing in his_\n",
      "Epoch 28: Train loss: 0.30325356668300174 | Train Accuracy: 0.15777607262134552 |  Validation loss: 0.42849012835306605 | Validation Accuracy: 0.224216490983963\n",
      "\t ever being cured . '\n",
      "_ever being cured . '_\n",
      "Epoch 29: Train loss: 0.2931091520493783 | Train Accuracy: 0.15064415335655212 |  Validation loss: 0.428732666379306 | Validation Accuracy: 0.21884773671627045\n",
      "\t bed . I had long since become accustomed , and now addicted ,\n",
      "_bed . I had long sin etfecsme ecGuo omed , ao  now adhicted ,_\n",
      "Epoch 30: Train loss: 0.28112851550461093 | Train Accuracy: 0.14400210976600647 |  Validation loss: 0.40741292881946667 | Validation Accuracy: 0.21265718340873718\n",
      "\t don't think it was through boredom . The film is funny enough\n",
      "_don'f think i  whs through boredom .  heofi   is rutnymenough_\n",
      "Epoch 31: Train loss: 0.27220757503604914 | Train Accuracy: 0.1386542022228241 |  Validation loss: 0.4085875422593594 | Validation Accuracy: 0.20763784646987915\n",
      "\t solace in that fact at least .\n",
      "_solace in that fectsft ietstn_\n",
      "Epoch 32: Train loss: 0.26462121775055425 | Train Accuracy: 0.1339970976114273 |  Validation loss: 0.3975341224557084 | Validation Accuracy: 0.20296189188957214\n",
      "\t the feeding tray .\n",
      "_the feeding tray ._\n",
      "Epoch 33: Train loss: 0.2544990292956448 | Train Accuracy: 0.12879981100559235 |  Validation loss: 0.38519380111401014 | Validation Accuracy: 0.19900444149971008\n",
      "\t we are with Thee .\n",
      "_we are with Thee .n_\n",
      "Epoch 34: Train loss: 0.2469349749200881 | Train Accuracy: 0.12791594862937927 |  Validation loss: 0.39347558307770414 | Validation Accuracy: 0.19869905710220337\n",
      "\t by failing until now to co-operate as a creditor nation\n",
      "_by failing until now to co-operate as a creditor nation_\n",
      "Epoch 35: Train loss: 0.2390825896727955 | Train Accuracy: 0.12410008907318115 |  Validation loss: 0.3808754219339021 | Validation Accuracy: 0.19486330449581146\n",
      "\t committees .\n",
      "_committees ._\n",
      "Epoch 36: Train loss: 0.2318783819161489 | Train Accuracy: 0.12062489986419678 |  Validation loss: 0.37698294766815466 | Validation Accuracy: 0.19275808334350586\n",
      "\t but boldly and fearlessly . It is time to\n",
      "_bue boldry asd ernodeesa  .e.   s t   ttee_\n",
      "Epoch 37: Train loss: 0.22505367485175526 | Train Accuracy: 0.1254163682460785 |  Validation loss: 0.383523520551233 | Validation Accuracy: 0.1964290738105774\n",
      "\t in her quivering flesh . \" You 've an answer - of sorts - for everything , \" she\n",
      "_in her quivering flesh . \" You 've an answer - of sorts - for everything , \" she_\n",
      "Epoch 38: Train loss: 0.21889526954110922 | Train Accuracy: 0.12945333123207092 |  Validation loss: 0.3811616745114087 | Validation Accuracy: 0.201795756816864\n",
      "\t which text is used .\n",
      "_which teBt is ussd ._\n",
      "Epoch 39: Train loss: 0.2120769852958083 | Train Accuracy: 0.1112377941608429 |  Validation loss: 0.36611411596450655 | Validation Accuracy: 0.184457927942276\n",
      "\t with the comment given by Samuel .\n",
      "_with the comment given by Samuelt._\n",
      "Epoch 40: Train loss: 0.2065298692945087 | Train Accuracy: 0.10921887308359146 |  Validation loss: 0.3653885824696679 | Validation Accuracy: 0.18163125216960907\n",
      "\t diet in 1959 .\n",
      "_diet in 1959 ._\n",
      "Epoch 41: Train loss: 0.2026271060814487 | Train Accuracy: 0.10903849452733994 |  Validation loss: 0.3738850548454349 | Validation Accuracy: 0.18158680200576782\n",
      "\t ended yesterday in 32 members of the\n",
      "_ended yeseebdtggiAA3i memeers  f thtt_\n",
      "Epoch 42: Train loss: 0.19620378579687692 | Train Accuracy: 0.10280971229076385 |  Validation loss: 0.3545198787642789 | Validation Accuracy: 0.17736142873764038\n",
      "\t Sentence Database\n",
      "_Sentence Database_\n",
      "Epoch 43: Train loss: 0.19220222620667338 | Train Accuracy: 0.10282469540834427 |  Validation loss: 0.355226647870974 | Validation Accuracy: 0.17611348628997803\n",
      "\t supported the sister's statement that Elizabeth Camp had been alone in her\n",
      "_supported the sister's statement that Elizabeth Camp had been alone in her_\n",
      "Epoch 44: Train loss: 0.18777112189005696 | Train Accuracy: 0.09877291321754456 |  Validation loss: 0.3495231033529405 | Validation Accuracy: 0.1730503886938095\n",
      "\t characters to their background , which bring\n",
      "_ccaraeters t tthee hbapn arndo,, ,  w   rii_\n",
      "Epoch 45: Train loss: 0.18340550648120088 | Train Accuracy: 0.09920987486839294 |  Validation loss: 0.34596673047187676 | Validation Accuracy: 0.17626893520355225\n",
      "\t \" And what are you going to do about it ? \" Tendentious , Mr. Roberts sounded .\n",
      "_\" And what are you going to do about it ? \" Tendentious , Mr. Roberts sounded ._\n",
      "Epoch 46: Train loss: 0.1783341585095419 | Train Accuracy: 0.09593445062637329 |  Validation loss: 0.3411812695702957 | Validation Accuracy: 0.1710592657327652\n",
      "\t Hahnemann once again .\n",
      "_Hahnemann once again ._\n",
      "Epoch 47: Train loss: 0.17519778533164643 | Train Accuracy: 0.09451991319656372 |  Validation loss: 0.34117278353136976 | Validation Accuracy: 0.17014504969120026\n",
      "\t - you must , for example , do your shopping on the morning of\n",
      "_- you must , for exa ple s do your shopping on the morning of_\n",
      "Epoch 48: Train loss: 0.1718372362315841 | Train Accuracy: 0.0930626317858696 |  Validation loss: 0.3459889449708761 | Validation Accuracy: 0.16837558150291443\n",
      "\t would never see her Dai again . And\n",
      "_woued dmven hre hees ai aga g .dAn_\n",
      "Epoch 49: Train loss: 0.1682253933613333 | Train Accuracy: 0.08596168458461761 |  Validation loss: 0.3379900999945002 | Validation Accuracy: 0.1611475944519043\n",
      "\t John's father had been a naval officer of the old\n",
      "_Johhts fiteer had beeo a nuval ofetoer of the old_\n",
      "Epoch 50: Train loss: 0.16438328737678135 | Train Accuracy: 0.08299051970243454 |  Validation loss: 0.3284181451965686 | Validation Accuracy: 0.158913716673851\n",
      "Error in callback <function flush_figures at 0x000001DA4D87F740> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib_inline\\backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m InlineBackend\u001b[39m.\u001b[39minstance()\u001b[39m.\u001b[39mclose_figures:\n\u001b[0;32m    124\u001b[0m     \u001b[39m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m show(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    128\u001b[0m         \u001b[39m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         ip \u001b[39m=\u001b[39m get_ipython()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         display(\n\u001b[0;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[0;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    177\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[0;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    341\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2336\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[0;32m   2337\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[0;32m   2338\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m   2339\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m   2340\u001b[0m     )\n\u001b[0;32m   2341\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2342\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m   2344\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2345\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\figure.py:3140\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3137\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3140\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3141\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3143\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3144\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3061\u001b[0m \u001b[39mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3062\u001b[0m     _draw_rasterized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3064\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3065\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3067\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   3068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[0;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[0;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[0;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[0;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[0;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[1;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[0;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:518\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    513\u001b[0m mask \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mwhere(A\u001b[39m.\u001b[39mmask, np\u001b[39m.\u001b[39mfloat32(np\u001b[39m.\u001b[39mnan), np\u001b[39m.\u001b[39mfloat32(\u001b[39m1\u001b[39m))\n\u001b[0;32m    514\u001b[0m         \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m A\u001b[39m.\u001b[39mshape  \u001b[39m# nontrivial mask\u001b[39;00m\n\u001b[0;32m    515\u001b[0m         \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mones_like(A, np\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m    516\u001b[0m \u001b[39m# we always have to interpolate the mask to account for\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m# non-affine transformations\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m out_alpha \u001b[39m=\u001b[39m _resample(\u001b[39mself\u001b[39;49m, mask, out_shape, t, resample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    519\u001b[0m \u001b[39mdel\u001b[39;00m mask  \u001b[39m# Make sure we don't use mask anymore!\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39m# Agg updates out_alpha in place.  If the pixel has no image\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m# data it will not be updated (and still be 0 as we initialized\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[39m# it), if input data that would go into that output pixel than\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m# it will be `nan`, if all the input data for a pixel is good\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m# it will be 1, and if there is _some_ good data in that output\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m# pixel it will be between [0, 1] (such as a rotated image).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NAB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:204\u001b[0m, in \u001b[0;36m_resample\u001b[1;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         interpolation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhanning\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 204\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape \u001b[39m+\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:], data\u001b[39m.\u001b[39mdtype)  \u001b[39m# 2D->2D, 3D->3D.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recognizer = Recognizer()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train(recognizer=recognizer, \n",
    "              train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "              batch_size=8, recognizer_lr=5e-4,\n",
    "              betas=(0, 0.999), num_epochs=50, loss_balancing_alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "\n",
    "print(torch.softmax(recognizer(image.unsqueeze(0)), 1), torch.softmax(recognizer(image.unsqueeze(0)), 1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
