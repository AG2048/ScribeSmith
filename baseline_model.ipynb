{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model for Handwritten Text Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "CHARS_DIR = os.path.join(DATA_DIR, \"chars\")\n",
    "OUT_IMG_WIDTH = 512\n",
    "OUT_IMG_HEIGHT = 32\n",
    "\n",
    "# Sorted by ascii code\n",
    "valid_chars = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "char_to_int = {v: i for i, v in enumerate(valid_chars, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid_chars, 1)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_filenames(char, extension=\"\"):\n",
    "    char_id = str(char_to_int[char]).zfill(2)\n",
    "    filenames = glob.glob(os.path.join(CHARS_DIR, char_id, f\"{char_id}-*{extension}\"))\n",
    "    return filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img, new_height=OUT_IMG_HEIGHT, threshold=230):\n",
    "    # Scale image\n",
    "    width, height = img.size\n",
    "    new_width = int((new_height / height) * width)\n",
    "    scaled_img = img.resize((new_width, new_height), resample=Image.LANCZOS)\n",
    "\n",
    "    # Binarize image\n",
    "    img_arr = np.array(scaled_img.convert(\"L\"), dtype=np.uint8)\n",
    "    bin_img = np.where(img_arr > threshold, 255, 0)\n",
    "\n",
    "    # Crop horizontal padding\n",
    "    non_empty_columns = np.where(bin_img.max(axis=0) > 0)[0]\n",
    "    return bin_img[:, min(non_empty_columns) : max(non_empty_columns) + 1]\n",
    "\n",
    "\n",
    "def process_char_img(filename):\n",
    "    img = Image.open(filename + \".png\")\n",
    "    img = process_img(img)\n",
    "    np.save(filename + \".npy\", img)  # Save as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/j11rmp_n6gl97rjc2kh1mk8m0000gn/T/ipykernel_50723/3453809907.py:5: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  scaled_img = img.resize((new_width, new_height), resample=Image.LANCZOS)\n"
     ]
    }
   ],
   "source": [
    "# Process every file\n",
    "for char in valid_chars:\n",
    "    for filename in get_char_filenames(char, \".png\"):\n",
    "        filename = filename[:-4]  # Remove file extension\n",
    "        process_char_img(filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_img_arr(char):\n",
    "    filenames = get_char_filenames(char, \".npy\")\n",
    "    rand_img = np.load(random.choice(filenames))\n",
    "    return rand_img\n",
    "\n",
    "\n",
    "def gen_hand_text(text):\n",
    "    chars = list(text)\n",
    "\n",
    "    # Initialize a white array of size 1x128x2048\n",
    "    res_array = np.full((OUT_IMG_HEIGHT, OUT_IMG_WIDTH), 255, dtype=np.uint8)\n",
    "\n",
    "    x_offset = 0\n",
    "    for char in chars:\n",
    "        if char == \" \":\n",
    "            # Add random spacing between words\n",
    "            space_width = random.randint(50, 60)\n",
    "            if x_offset + space_width <= OUT_IMG_WIDTH:\n",
    "                x_offset += space_width\n",
    "        else:\n",
    "            char_array = get_rand_img_arr(char)\n",
    "            if x_offset + char_array.shape[1] <= OUT_IMG_WIDTH:\n",
    "                res_array[:, x_offset : x_offset + char_array.shape[1]] = char_array\n",
    "                x_offset += char_array.shape[1]\n",
    "                # Add random spacing between letters\n",
    "                space_width = random.randint(5, 8)\n",
    "                if x_offset + space_width <= OUT_IMG_WIDTH:\n",
    "                    x_offset += space_width\n",
    "\n",
    "    return res_array\n",
    "\n",
    "\n",
    "def gen_hand_lines(lines):\n",
    "    # Initialize an empty list to store arrays\n",
    "    arrays = []\n",
    "    for line in lines:\n",
    "        array = gen_hand_text(line)\n",
    "        arrays.append(array)\n",
    "    # Stack arrays along the first axis\n",
    "    result = np.stack(arrays, axis=0)\n",
    "    # Reshape to Nx1x128x2048\n",
    "    result = result.reshape(len(lines), 1, OUT_IMG_HEIGHT, OUT_IMG_WIDTH)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_imgs(imgs):\n",
    "    for i, img in enumerate(imgs):\n",
    "        plt.figure(i)\n",
    "        plt.imshow(img[0], cmap=\"gray\")\n",
    "        # plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mia ry tad & i € fame \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'&'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m         embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(vec_rep)\n\u001b[1;32m     19\u001b[0m         \u001b[39mreturn\u001b[39;00m embedding\n\u001b[0;32m---> 21\u001b[0m \u001b[39mprint\u001b[39m(Recognizer_Py(np\u001b[39m.\u001b[39;49msqueeze(imgs)))\n",
      "Cell \u001b[0;32mIn[114], line 16\u001b[0m, in \u001b[0;36mRecognizer_Py\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     11\u001b[0m string_rep \u001b[39m=\u001b[39m string_rep\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(string_rep, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m vec_rep \u001b[39m=\u001b[39m [char_to_int[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m string_rep]\n\u001b[1;32m     17\u001b[0m embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(vec_rep)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m embedding\n",
      "Cell \u001b[0;32mIn[114], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m string_rep \u001b[39m=\u001b[39m string_rep\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(string_rep, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m vec_rep \u001b[39m=\u001b[39m [char_to_int[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m string_rep]\n\u001b[1;32m     17\u001b[0m embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(vec_rep)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m embedding\n",
      "\u001b[0;31mKeyError\u001b[0m: '&'"
     ]
    }
   ],
   "source": [
    "#lines = [\"Hello World,\", \"this is the baseline model!\", \"2 x 2 : 4\"]\n",
    "lines = [\"mary had a little lamb\"]\n",
    "imgs = gen_hand_lines(lines)\n",
    "disp_imgs(imgs)\n",
    "\n",
    "def Recognizer_Py(img):\n",
    "        to_pil_image = ToPILImage()\n",
    "        img = to_pil_image(img)\n",
    "        \n",
    "        string_rep = pytesseract.image_to_string(img)\n",
    "        string_rep = string_rep.replace('\\n', '')\n",
    "        \n",
    "        print(string_rep, \"\\n\")\n",
    "                \n",
    "        \n",
    "        vec_rep = [char_to_int[i] for i in string_rep]\n",
    "        embedding = torch.FloatTensor(vec_rep)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "print(Recognizer_Py(np.squeeze(imgs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnacondaPyCharm3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
