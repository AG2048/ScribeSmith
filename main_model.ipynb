{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, Subset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch_fidelity\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying\n",
    "    \"\"\"\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        # TODO replace imag_path with the .pt file later on\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actua items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            num_samples += 1\n",
    "            tmp_image = read_image(os.path.join(image_path_head, image_path_tail))  # Temporarily reading to get dimensions\n",
    "            _, height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            if width * (128/height) >= 2048:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image_tensor(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image_tensor(image_tensor, graylevel):\n",
    "    \"\"\"\n",
    "    Returns a copy of image_tensor, processed (doesn't update the original variable)\n",
    "    \"\"\"\n",
    "\n",
    "    # Grayscale the image - if the image is not already in grayscale\n",
    "    grayscale_transform = Grayscale()\n",
    "    grayscale_tensor = grayscale_transform(image_tensor)\n",
    "\n",
    "    # Threshold it\n",
    "    # Threshold first because threshold was specifically specified for the original\n",
    "    threshold_tensor = grayscale_tensor >= graylevel\n",
    "\n",
    "    # Resize it\n",
    "    resize_transform = Resize(128)\n",
    "    resized_tensor = resize_transform(threshold_tensor)\n",
    "\n",
    "    # Add padding\n",
    "    _,_, resized_height = resized_tensor.shape\n",
    "    padding_to_add = 2048 - resized_height\n",
    "    resized_tensor = F.pad(resized_tensor, (0, padding_to_add), value=1)\n",
    "\n",
    "    # Convert to uint8\n",
    "    resized_tensor = resized_tensor.type(torch.float32)\n",
    "\n",
    "    return resized_tensor\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "# preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)  # List containing the stuff in `lines.txt`\n",
    "        length = self.lines_df.shape[0]\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "        self.ty = ty\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i]['transcription'].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "        self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i]))) for i in range(length)]\n",
    "        self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # line_data = self.lines_df.iloc[index]\n",
    "\n",
    "        # ret_text = line_data['transcription'].replace('|', ' ')\n",
    "        # # Get the numerical mapping\n",
    "        # ret_ctoi = torch.tensor([char_to_int[char] for char in ret_text])\n",
    "        # # Padding to the left amount to make it reach `max_transcription_len`\n",
    "        # # TODO possibility to remove padding in the future and let a dataloader handle it\n",
    "        # ret_ctoi_padded = F.pad(ret_ctoi, pad=(0, self.max_transcription_len-len(ret_ctoi)))\n",
    "        # ret_image = torch.load(line_data[\"image_pt_path\"])\n",
    "\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "800 200\n",
      "images\n",
      "800 200\n",
      "both\n",
      "5708 1427\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(800))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(200))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(800))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(200))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, label = line_dataset_train[283]\n",
    "# print(image.shape)\n",
    "# plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "# print(image.squeeze(0).shape)\n",
    "# plt.imshow(image.squeeze(0), cmap='gray')\n",
    "# label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# # line_dataset.lines_df.iloc[798]\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.imshow(line_dataset.lines_df[\"image_path\"])\n",
    "# image = read_image(line_dataset.lines_df[\"image_path\"].iloc[283]).squeeze(0)\n",
    "# plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(line_image_dataset[1000], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in DataLoader(line_image_dataset, batch_size=64, shuffle=True):\n",
    "#     print(i.shape)\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataloading Functions\"\"\"\n",
    "\"\"\"Datasaving Functions\"\"\"\n",
    "\n",
    "\"\"\"Plotting Functions\"\"\"\n",
    "\"\"\"Evaluation Functions\"\"\"\n",
    "def calculate_gan_loss_and_accuracies(generator, encoder, discriminator, recognizer, \n",
    "                                  real_image_loader, input_text_loader, \n",
    "                                  batch_size=64, adversarial_loss_function=nn.BCELoss(), recognizer_loss_function=nn.CTCLoss(),\n",
    "                                  device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Calculates the loss for the GAN\n",
    "    \n",
    "    Params:\n",
    "        generator: The generator model\n",
    "        encoder: The encoder model\n",
    "        discriminator: The discriminator model\n",
    "        recognizer: The recognizer model\n",
    "        real_image_loader: The dataloader for real images\n",
    "        input_text_loader: The dataloader for input text\n",
    "        batch_size: The batch size to use\n",
    "        adversarial_loss_function: The adversarial loss function to use\n",
    "        recognizer_loss_function: The recognizer loss function to use\n",
    "\n",
    "    Returns:\n",
    "        generator_and_encoder_loss: The loss for the generator and encoder\n",
    "        discriminator_loss: The loss for the discriminator\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=False)\n",
    "    for real_image_batch in real_image_loader:\n",
    "        fid.update((real_image_batch.cpu()*255).type(torch.uint8).repeat(1, 3, 1, 1), real=True)\n",
    "\n",
    "    generator_and_encoder_loss = 0\n",
    "    discriminator_loss = 0\n",
    "    discriminator_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (real_image_batch, input_text_batch) in enumerate(zip(real_image_loader, input_text_loader)):\n",
    "            real_image_batch = real_image_batch.to(device)\n",
    "            input_text_batch = input_text_batch.to(device)\n",
    "\n",
    "            # generate noise of N x noise_dim\n",
    "            noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "\n",
    "            # Use encoder and generator to generate fake images\n",
    "            text_embedding = encoder(input_text_batch)\n",
    "            fake_image_batch = generator(noise, text_embedding)\n",
    "\n",
    "            # train discriminator\n",
    "            discriminator_output_for_real_images = discriminator(real_image_batch)\n",
    "            discriminator_output_for_fake_images = discriminator(fake_image_batch)\n",
    "            label_for_real_images = torch.ones_like(discriminator_output_for_real_images).to(device)\n",
    "            label_for_fake_images = torch.zeros_like(discriminator_output_for_fake_images).to(device)\n",
    "            real_images_loss = adversarial_loss_function(discriminator_output_for_real_images, label_for_real_images)\n",
    "            fake_images_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_fake_images)\n",
    "            discriminator_loss = (real_images_loss + fake_images_loss) / 2\n",
    "            discriminator_loss += discriminator_loss.item()\n",
    "            discriminator_accuracy += (torch.sum(discriminator_output_for_real_images >= 0.5).item() + torch.sum(discriminator_output_for_fake_images < 0.5).item()) / (2 * batch_size)\n",
    "\n",
    "            # train generator\n",
    "            \n",
    "            adversarial_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_real_images)\n",
    "            # recognizer_outputs = recognizer(fake_image_batch)\n",
    "            # recognizer_loss = recognizer_loss_function(recognizer_outputs, input_text_batch)\n",
    "            \n",
    "            # balance the losses from different sources, according to https://arxiv.org/pdf/1903.00277.pdf\n",
    "            adversarial_loss_mean, adversarial_loss_std = torch.mean(adversarial_loss), torch.std(adversarial_loss)\n",
    "            # recognizer_loss_mean, recognizer_loss_std = torch.mean(recognizer_loss), torch.std(recognizer_loss)\n",
    "            # recognizer_loss = loss_balancing_alpha * (adversarial_loss_std / recognizer_loss_std) * (recognizer_loss - recognizer_loss_mean) + adversarial_loss_mean\n",
    "            \n",
    "            generator_loss = adversarial_loss #+ recognizer_loss\n",
    "            generator_and_encoder_loss += generator_loss.item()\n",
    "\n",
    "            # calculate fid\n",
    "            fid.update((real_image_batch.cpu()*255).type(torch.uint8).repeat(1, 3, 1, 1), real=True)\n",
    "            fid.update((fake_image_batch.cpu()*255).type(torch.uint8).repeat(1, 3, 1, 1), real=False)\n",
    "\n",
    "    generator_and_encoder_error = fid.compute().item()\n",
    "    fid.reset()\n",
    "    discriminator_accuracy /= len(real_image_loader)\n",
    "    generator_and_encoder_loss /= len(real_image_loader)\n",
    "    discriminator_loss /= len(input_text_loader)\n",
    "\n",
    "    return generator_and_encoder_loss, discriminator_loss, generator_and_encoder_error, discriminator_accuracy\n",
    "\n",
    "\"\"\"Training Functions\"\"\"\n",
    "def train_recognizer(recognizer, train_line_loader, val_line_loader, batch_size=64, learning_rate=2e-4, betas=(0, 0.999), num_epochs=30):\n",
    "    # only train on real images\n",
    "    # also save model, plot graphs, save graphs\n",
    "    pass\n",
    "\n",
    "def train_gan(generator, encoder, discriminator, recognizer, \n",
    "              train_real_image_dataset, val_real_image_dataset, \n",
    "              train_input_text_dataset, val_input_text_dataset, \n",
    "              batch_size=64, learning_rate=2e-4, betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    # device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    print(device)\n",
    "    generator = generator.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    # recognizer.to(device)\n",
    "    \n",
    "    train_real_image_loader = DataLoader(train_real_image_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_real_image_loader = DataLoader(val_real_image_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_input_text_loader = DataLoader(train_input_text_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_input_text_loader = DataLoader(val_input_text_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=betas)\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=betas)\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=betas)\n",
    "    \n",
    "    adversarial_loss_function = nn.BCELoss()  # discriminator already has sigmoid\n",
    "    # recognizer_loss_function = nn.CTCLoss()  # TODO ensure recognizer's output fits the input of this loss function\n",
    "    saving_filenames = {\n",
    "        \"encoder\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lr{learning_rate}_betas{betas}_encoder\"),\n",
    "        \"generator\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lr{learning_rate}_betas{betas}_generator\"),\n",
    "        \"discriminator\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lr{learning_rate}_betas{betas}_discriminator\"), \n",
    "        \"losses\": os.path.join(\"main_model\", \"model_training_information\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lr{learning_rate}_betas{betas}_losses\"),\n",
    "        \"accuracies\": os.path.join(\"main_model\", \"model_training_information\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lr{learning_rate}_betas{betas}_accuracies\")\n",
    "        }\n",
    "\n",
    "    best_generator_and_encoder_val_loss = float('inf')\n",
    "    best_discriminator_val_loss = float('inf')\n",
    "    best_generator_and_encoder_val_error = 0\n",
    "    best_discriminator_val_accuracy = 0\n",
    "    saved_generator_and_encoder_models_epochs = []\n",
    "    saved_discriminator_models_epochs = []\n",
    "\n",
    "    generator_and_encoder_train_losses = []\n",
    "    discriminator_train_losses = []\n",
    "    generator_and_encoder_train_accuracies = []\n",
    "    discriminator_train_accuracies = []\n",
    "    generator_and_encoder_val_losses = []\n",
    "    discriminator_val_losses = []\n",
    "    generator_and_encoder_val_accuracies = []\n",
    "    discriminator_val_accuracies = []\n",
    "\n",
    "    # FID: https://torchmetrics.readthedocs.io/en/stable/image/frechet_inception_distance.html\n",
    "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=False)\n",
    "    image_count = 0\n",
    "    for real_image_batch in train_real_image_loader:\n",
    "        image_count += len(real_image_batch)\n",
    "        print(\"fid load image_count\", image_count)\n",
    "        fid.update((real_image_batch.cpu()*255).type(torch.uint8).repeat(1, 3, 1, 1), real=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        generator_and_encoder_train_loss = 0\n",
    "        discriminator_train_loss = 0\n",
    "        discriminator_train_accuracy = 0\n",
    "\n",
    "        for i, (real_image_batch, input_text_batch) in enumerate(zip(train_real_image_loader, train_input_text_loader)):\n",
    "            print(\"epoch\", epoch, \"batch\", i)\n",
    "            print(\"real_image_batch.shape\", real_image_batch.shape)\n",
    "            print(\"input_text_batch.shape\", input_text_batch.shape)\n",
    "\n",
    "            real_image_batch = real_image_batch.to(device)\n",
    "            input_text_batch = input_text_batch.to(device)\n",
    "\n",
    "            # generate noise of N x noise_dim\n",
    "            noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "\n",
    "            # Use encoder and generator to generate fake images\n",
    "            text_embedding = encoder(input_text_batch)\n",
    "            print(noise.shape, text_embedding.shape)\n",
    "            fake_image_batch = generator(noise, text_embedding)\n",
    "\n",
    "            # display random image from the batch\n",
    "            plt.imshow(fake_image_batch.cpu().detach().numpy()[0].squeeze(0), cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            # train discriminator\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_output_for_real_images = discriminator(real_image_batch)\n",
    "            discriminator_output_for_fake_images = discriminator(fake_image_batch.detach())  # added detatch to prevent gradients from flowing back to generator\n",
    "            label_for_real_images = torch.ones_like(discriminator_output_for_real_images).to(device)\n",
    "            label_for_fake_images = torch.zeros_like(discriminator_output_for_fake_images).to(device)\n",
    "            real_images_loss = adversarial_loss_function(discriminator_output_for_real_images, label_for_real_images)\n",
    "            fake_images_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_fake_images)\n",
    "            discriminator_loss = real_images_loss + fake_images_loss\n",
    "            discriminator_loss.backward()  # retain_graph=True because we will use the same discriminator for the generator\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=0.01)\n",
    "            \n",
    "            discriminator_optimizer.step()\n",
    "            discriminator_train_loss += discriminator_loss.item()\n",
    "            discriminator_train_accuracy += (torch.sum(discriminator_output_for_real_images > 0.5) + torch.sum(discriminator_output_for_fake_images < 0.5)) / (2 * batch_size)\n",
    "            print(discriminator_output_for_fake_images)\n",
    "            print(discriminator_output_for_real_images)\n",
    "\n",
    "            # train generator\n",
    "            generator_optimizer.zero_grad()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            # # generate noise of N x noise_dim\n",
    "            # noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "            # # Use encoder and generator to generate fake images\n",
    "            # text_embedding = encoder(input_text_batch)\n",
    "            # fake_image_batch = generator(noise, text_embedding)\n",
    "            # # display random image from the batch\n",
    "            # plt.imshow(fake_image_batch.cpu().detach().numpy()[0].squeeze(0), cmap='gray')\n",
    "            # plt.show()\n",
    "            discriminator_output_for_fake_images = discriminator(fake_image_batch)\n",
    "            adversarial_loss = adversarial_loss_function(discriminator_output_for_fake_images, torch.ones_like(discriminator_output_for_real_images).to(device))  # note that we want the fake images to be classified as real\n",
    "            print(torch.sum(discriminator_output_for_fake_images > 0.5))\n",
    "            print(adversarial_loss)\n",
    "            print(discriminator_output_for_fake_images)\n",
    "            # recognizer_outputs = recognizer(fake_image_batch)\n",
    "            # recognizer_loss = recognizer_loss_function(recognizer_outputs, input_text_batch)\n",
    "            \n",
    "            # balance the losses from different sources, according to https://arxiv.org/pdf/1903.00277.pdf\n",
    "            adversarial_loss_mean, adversarial_loss_std = torch.mean(adversarial_loss), torch.std(adversarial_loss)\n",
    "            # recognizer_loss_mean, recognizer_loss_std = torch.mean(recognizer_loss), torch.std(recognizer_loss)\n",
    "            # recognizer_loss = loss_balancing_alpha * (adversarial_loss_std / recognizer_loss_std) * (recognizer_loss - recognizer_loss_mean) + adversarial_loss_mean\n",
    "            \n",
    "            generator_loss = adversarial_loss #+ recognizer_loss\n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "            generator_and_encoder_train_loss += generator_loss.item()\n",
    "            \n",
    "            # calculate FID\n",
    "            fid.update((fake_image_batch.cpu()*255).type(torch.uint8).repeat(1, 3, 1, 1), real=False)\n",
    "\n",
    "            # display_images.append(fake_image_batch[random.randint(0, len(fake_image_batch) - 1)].detach().numpy())\n",
    "\n",
    "        # plot the collection of display_images, all are greyscale\n",
    "        # display_images = np.array(display_images)\n",
    "        # display_images = np.transpose(display_images, (0, 2, 3, 1))\n",
    "        # display_images = np.squeeze(display_images)\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        # for i in range(25):\n",
    "        #     plt.subplot(5, 5, i + 1)\n",
    "        #     plt.imshow(display_images[i], cmap='gray')\n",
    "        #     plt.axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "\n",
    "\n",
    "        # Tally up losses and accuracies\n",
    "        generator_and_encoder_train_loss /= len(train_real_image_loader)\n",
    "        discriminator_train_loss /= len(train_real_image_loader)\n",
    "        generator_and_encoder_train_error = fid.compute().item()\n",
    "        fid.reset()\n",
    "        discriminator_train_accuracy /= len(train_real_image_loader)\n",
    "        generator_and_encoder_train_losses.append(generator_and_encoder_train_loss)\n",
    "        discriminator_train_losses.append(discriminator_train_loss)\n",
    "        generator_and_encoder_train_accuracies.append(generator_and_encoder_train_error)\n",
    "        discriminator_train_accuracies.append(discriminator_train_accuracy)\n",
    "\n",
    "        generator_and_encoder_val_loss, discriminator_val_loss, generator_and_encoder_val_error, discriminator_val_accuracy = calculate_gan_loss_and_accuracies(generator, encoder, discriminator, recognizer, val_real_image_loader, val_input_text_loader, batch_size=batch_size)\n",
    "        generator_and_encoder_val_losses.append(generator_and_encoder_val_loss)\n",
    "        discriminator_val_losses.append(discriminator_val_loss)\n",
    "        generator_and_encoder_val_accuracies.append(generator_and_encoder_val_error)\n",
    "        discriminator_val_accuracies.append(discriminator_val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\n\\tGenerator and encoder train loss: {generator_and_encoder_train_loss}\\n\\tDiscriminator train loss: {discriminator_train_loss}\\n\\tGenerator and encoder train accuracy: {generator_and_encoder_train_error}\\n\\tDiscriminator train accuracy: {discriminator_train_accuracy}\\n\\tGenerator and encoder val loss: {generator_and_encoder_val_loss}\\n\\tDiscriminator val loss: {discriminator_val_loss}\\n\\tGenerator and encoder val accuracy: {generator_and_encoder_val_error}\\n\\tDiscriminator val accuracy: {discriminator_val_accuracy}\")\n",
    "\n",
    "        # Save models, only if they are better than the previous best\n",
    "        if generator_and_encoder_val_loss < best_generator_and_encoder_val_loss:\n",
    "            best_generator_and_encoder_val_loss = generator_and_encoder_val_loss\n",
    "            saved_generator_and_encoder_models_epochs.append(epoch)\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if discriminator_val_loss < best_discriminator_val_loss:\n",
    "            best_discriminator_val_loss = discriminator_val_loss\n",
    "            saved_discriminator_models_epochs.append(epoch)\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if generator_and_encoder_val_error < best_generator_and_encoder_val_error:\n",
    "            best_generator_and_encoder_val_error = generator_and_encoder_val_error\n",
    "            saved_generator_and_encoder_models_epochs.append(epoch) if epoch not in saved_generator_and_encoder_models_epochs else None\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if discriminator_val_accuracy > best_discriminator_val_accuracy:\n",
    "            best_discriminator_val_accuracy = discriminator_val_accuracy\n",
    "            saved_discriminator_models_epochs.append(epoch) if epoch not in saved_discriminator_models_epochs else None\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "\n",
    "        # plot title\n",
    "        plt.title(\"generator and encoder losses\")\n",
    "        # plot x axis label\n",
    "        plt.xlabel(\"epoch\")\n",
    "        # plot y axis label\n",
    "        plt.ylabel(\"loss\")\n",
    "        # plot the epoch vs the loss\n",
    "        plt.plot(range(epoch+1), generator_and_encoder_train_losses, label=\"train\")\n",
    "        plt.plot(range(epoch+1), generator_and_encoder_val_losses, label=\"val\")\n",
    "        # plot the saved models as dots\n",
    "        plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_losses[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "        # legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        # save the plot\n",
    "        plt.close()\n",
    "\n",
    "        # plot title\n",
    "        plt.title(\"discriminator losses\")\n",
    "        # plot x axis label\n",
    "        plt.xlabel(\"epoch\")\n",
    "        # plot y axis label\n",
    "        plt.ylabel(\"loss\")\n",
    "        # plot the epoch vs the loss\n",
    "        plt.plot(range(epoch+1), discriminator_train_losses, label=\"train\")\n",
    "        plt.plot(range(epoch+1), discriminator_val_losses, label=\"val\")\n",
    "        # plot the saved models as dots\n",
    "        plt.scatter(saved_discriminator_models_epochs, [discriminator_val_losses[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "        # legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        # close the plot\n",
    "        plt.close()\n",
    "\n",
    "        # plot title\n",
    "        plt.title(\"generator and encoder errors\")\n",
    "        # plot x axis label\n",
    "        plt.xlabel(\"epoch\")\n",
    "        # plot y axis label\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        # plot the epoch vs the accuracy\n",
    "        plt.plot(range(epoch+1), generator_and_encoder_train_accuracies, label=\"train\")\n",
    "        plt.plot(range(epoch+1), generator_and_encoder_val_accuracies, label=\"val\")\n",
    "        # plot the saved models as dots\n",
    "        plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_accuracies[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "        # legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        # close the plot\n",
    "        plt.close()\n",
    "\n",
    "        # plot title\n",
    "        plt.title(\"discriminator accuracies\")\n",
    "        # plot x axis label\n",
    "        plt.xlabel(\"epoch\")\n",
    "        # plot y axis label\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        # plot the epoch vs the accuracy\n",
    "        plt.plot(range(epoch+1), discriminator_train_accuracies, label=\"train\")\n",
    "        plt.plot(range(epoch+1), discriminator_val_accuracies, label=\"val\")\n",
    "        # plot the saved models as dots\n",
    "        plt.scatter(saved_discriminator_models_epochs, [discriminator_val_accuracies[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "        # legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        # close the plot\n",
    "        plt.close()\n",
    "\n",
    "    # Plot the losses and accuracies, and save them (filename should be unique, use datetime as a prefix)\n",
    "    # The plot should include which epoch's model we saved\n",
    "    # save the plot, and csv of the losses and accuracies, and which epoch's model we saved\n",
    "    \n",
    "    # plot title\n",
    "    plt.title(\"generator and encoder losses\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"loss\")\n",
    "    # plot the epoch vs the loss\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_train_losses, label=\"train\")\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_val_losses, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_losses[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['losses']}_generator_and_encoder_loss.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"discriminator losses\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"loss\")\n",
    "    # plot the epoch vs the loss\n",
    "    plt.plot(range(num_epochs), discriminator_train_losses, label=\"train\")\n",
    "    plt.plot(range(num_epochs), discriminator_val_losses, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_discriminator_models_epochs, [discriminator_val_losses[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['losses']}_discriminator_loss.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"generator and encoder errors\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    # plot the epoch vs the accuracy\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_train_accuracies, label=\"train\")\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_val_accuracies, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_accuracies[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['accuracies']}_generator_and_encoder_error.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"discriminator accuracies\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    # plot the epoch vs the accuracy\n",
    "    plt.plot(range(num_epochs), discriminator_train_accuracies, label=\"train\")\n",
    "    plt.plot(range(num_epochs), discriminator_val_accuracies, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_discriminator_models_epochs, [discriminator_val_accuracies[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['accuracies']}_discriminator_accuracy.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # save the losses and accuracies as csvs\n",
    "    with open(f\"{saving_filenames['losses']}.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"generator and encoder train loss\", \"discriminator train loss\", \"generator and encoder val loss\", \"discriminator val loss\"])\n",
    "        for i in range(num_epochs):\n",
    "            writer.writerow([i, generator_and_encoder_train_losses[i], discriminator_train_losses[i], generator_and_encoder_val_losses[i], discriminator_val_losses[i]])\n",
    "    with open(f\"{saving_filenames['accuracies']}.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"generator and encoder train error\", \"discriminator train accuracy\", \"generator and encoder val error\", \"discriminator val accuracy\"])\n",
    "        for i in range(num_epochs):\n",
    "            writer.writerow([i, generator_and_encoder_train_accuracies[i], discriminator_train_accuracies[i], generator_and_encoder_val_accuracies[i], discriminator_val_accuracies[i]])\n",
    "\n",
    "\n",
    "def load_model(model, model_filename):\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_models_of_same_batch(generator, encoder, discriminator, filename_prefix, epoch_number):\n",
    "    \"\"\"\n",
    "    Load the generator, encoder, and discriminator from files.\n",
    "    \"\"\"\n",
    "    generator = load_model(generator, f\"{filename_prefix}_generator_epoch{epoch_number}.pt\")\n",
    "    encoder = load_model(encoder, f\"{filename_prefix}_encoder_epoch{epoch_number}.pt\")\n",
    "    discriminator = load_model(discriminator, f\"{filename_prefix}_discriminator_epoch{epoch_number}.pt\")\n",
    "    return generator, encoder, discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (N, C, H, W), with condition vector of shape (N, num_conditions)\n",
    "    Output: (N, C, H, W)\n",
    "\n",
    "    Conditional Batch Normalization\n",
    "    Idea obtained from https://arxiv.org/pdf/1809.11096.pdf\n",
    "    This is a network layer that applies batch normalization to the input tensor, and conditions it on a condition vector\n",
    "    For the Generator, this allows the network to learn to generate images conditioned on the class label and the noise vector\n",
    "\n",
    "    This network takes in a condition vector of length num_conditions, and applies batch normalization to the input tensor. \n",
    "    Then it computes 2 affine parameters (scale and bias) for each channel of the input tensor, conditioned on the condition vector through a linear layer.\n",
    "    The affine parameters are then applied to the input tensor, and the output is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_conditions):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        num_conditions: length of the condition vector\n",
    "        \"\"\"\n",
    "        super(ConditionalBatchNorm2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # batch normalize the input, without using affine parameters\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels, affine=False)\n",
    "\n",
    "        # set up affine parameters conditioned on the condition vector\n",
    "        self.embed_conditions = nn.Sequential(\n",
    "            # 512 hidden units are used by https://arxiv.org/pdf/1903.00277.pdf\n",
    "            spectral_norm(nn.Linear(num_conditions, 512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            spectral_norm(nn.Linear(512, in_channels * 2))\n",
    "        )\n",
    "        # # https://arxiv.org/pdf/1809.11096.pdf\n",
    "        # # initialize affine parameters to be all zeros for bias and ones for scale\n",
    "        # self.embed_conditions[-1].weight.data.zero_()\n",
    "        # self.embed_conditions[-1].bias.data[:in_channels].zero_()  # bias is the second half of the affine parameters\n",
    "        # self.embed_conditions[-1].bias.data[in_channels:].fill_(1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        conditions: condition vector of shape (N, num_conditions)\n",
    "        \"\"\"\n",
    "        # apply batch normalization, out still has shape (N, C, H, W)\n",
    "        out = self.batch_norm(x)\n",
    "\n",
    "        # compute affine parameters\n",
    "        params = self.embed_conditions(conditions)\n",
    "        # params has shape (N, 2 * C), we split the channel dimension in half into 2 tensors of shape (N, C)\n",
    "        scale, bias = params.chunk(2, dim=1)\n",
    "\n",
    "        # Apply spectral normalization to the scale and bias\n",
    "        scale = scale.view(-1, self.in_channels, 1, 1)\n",
    "        bias = bias.view(-1, self.in_channels, 1, 1)\n",
    "\n",
    "        # apply scale and bias. every channel's values are scaled and biased by the channel's own scale and bias value\n",
    "        out = scale.view(-1, self.in_channels, 1, 1) * out + bias.view(-1, self.in_channels, 1, 1)\n",
    "\n",
    "        # out has shape (N, C, H, W)\n",
    "        return out\n",
    "    \n",
    "class ResBlockUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (N, in_channels, H, W), with condition vector of shape (N, num_conditions)\n",
    "    Output: (N, out_channels, H * 2, W * 2)\n",
    "\n",
    "    Residual Block for Upsampling\n",
    "    Idea obtained from https://arxiv.org/pdf/1903.00277.pdf\n",
    "    This is a network layer that upsamples the input tensor by a factor of 2, and conditions it on a condition vector\n",
    "    For the Generator, this allows the network to learn to generate images conditioned on the class label and the noise vector\n",
    "\n",
    "    This network takes in a condition vector of length num_conditions, and upsamples the input tensor by a factor of 2, accounting for the condition vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_conditions):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "        num_conditions: length of the condition vector\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlockUp, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.batch_norm1 = ConditionalBatchNorm2d(in_channels, num_conditions)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "        self.batch_norm2 = ConditionalBatchNorm2d(out_channels, num_conditions)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        conditions: condition vector of shape (N, num_conditions)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = self.upsample(x)\n",
    "        # depending on if this res_block_up changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x, conditions)\n",
    "        out2 = self.relu(out2)\n",
    "        out2 = self.upsample(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2, conditions)\n",
    "        out2 = self.relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, 2 * H, 2 * W)\n",
    "        \n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self attention Layer\n",
    "    \n",
    "    This code is obtained from https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py\n",
    "    \"\"\"\n",
    "    def __init__(self,in_dim):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockDown(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlockDown, self).__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.average_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = x\n",
    "        # depending on if this res_block_down changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        out1 = self.average_pool(out1)\n",
    "        \n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        out2 = self.average_pool(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, H / 2, W / 2)\n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = x\n",
    "        # depending on if this res_block_down changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        \n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, H, W)\n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class GlobalSumPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "\n",
    "        returns a tensor of shape (N, C)\n",
    "        \"\"\"\n",
    "        return torch.sum(x, dim=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    Input with a vector representation of an ascii text\n",
    "    Output a vector embedding of the text\n",
    "    Purpose is to produce an embedding of the text that includes the relationship between the characters\n",
    "    Description of the encoder comes from https://arxiv.org/pdf/1903.00277.pdf.\n",
    "    Although we may end up modifying the dimensions of the hidden state and the embedding vector, \n",
    "    to fit our needs of processing longer texts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_embedding_dim=128, hidden_dim=128, num_layers=4, num_chars=73):\n",
    "        \"\"\"\n",
    "        embedding_dim: dimension of the embedding vector\n",
    "        hidden_dim: dimension of the hidden state of the LSTM\n",
    "        num_layers: number of layers in the LSTM\n",
    "        num_chars: number of characters in the vocabulary\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_chars = num_chars\n",
    "\n",
    "        # TODO: embedding can be from a pretrained model\n",
    "        self.embedding = nn.Embedding(num_chars, char_embedding_dim)\n",
    "        # Using bidirectional LSTM. Batch first so that the input is of shape (N, L, C)\n",
    "        self.lstm = nn.LSTM(char_embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, L), where L is the length of the text. each element is an integer representing a character (1 to 72)\n",
    "        \"\"\"\n",
    "        # Technically, L is the MAXIMUM length of the text in this batch (cuz padding)\n",
    "        \n",
    "        # First embed each character\n",
    "        x = self.embedding(x)  # output should be (N, L, char_embedding_dim)\n",
    "\n",
    "        # Run the LSTM, we will only use the hidden state, the sequence output is not needed\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # TODO: this can be modified to specify how to reshape our output\n",
    "        # out = out[:, -1, :]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        # out, _ = torch.mean(out, dim=1)\n",
    "        \n",
    "        # out now have shape (N, hidden_dim * 2)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Transposed CNN\n",
    "    Input with a vector embedding of the text and a noise vector\n",
    "    Output a 128 x 2048 grayscale image\n",
    "    Purpose is to produce an image that is a representation of the text, with the noise vector adding some variation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_dim=128, embedding_dim=256):\n",
    "        \"\"\" \n",
    "        noise_dim: dimension of the noise vector, should be divisible by 8\n",
    "        embedding_dim: dimension of the embedding vector\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # We are likely going to need to use 7 ResBlockUp layers to get the image to the desired size of 128 x 2048\n",
    "        # We will upscale to this from a 1 x 16 tensor\n",
    "        # 7 ResBlockUp also mean our noise vector will be split into 8 parts. \n",
    "        self.noise_dim = noise_dim\n",
    "        self.noise_chunk_size = noise_dim // 8\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Linear(self.noise_chunk_size, 256 * 1 * 16)\n",
    "        self.res_block_up1 = ResBlockUp(256, 256, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up2 = ResBlockUp(256, 128, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up3 = ResBlockUp(128, 128, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up4 = ResBlockUp(128, 64, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.self_attention = SelfAttention(64)\n",
    "        self.res_block_up5 = ResBlockUp(64, 32, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up6 = ResBlockUp(32, 16, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up7 = ResBlockUp(16, 16, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = spectral_norm(nn.Conv2d(16, 1, kernel_size=3, padding=1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, noise, embedding):\n",
    "        \"\"\"\n",
    "        noise: noise vector of shape (N, noise_dim)\n",
    "        embedding: embedding of the text of shape (N, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # split the noise vector into 8 parts\n",
    "        noise_chunks = torch.split(noise, self.noise_chunk_size, dim=1)\n",
    "\n",
    "        # first input to the network is the first noise chunk\n",
    "        x = noise_chunks[0]\n",
    "        # pass the concatenated tensor through the fully connected layer\n",
    "        x = self.fc(x)  # output is (N, 256 * 1 * 16)\n",
    "        # reshape the tensor to have the desired shape\n",
    "        x = x.view(-1, 256, 1, 16)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the ResBlockUp layers\n",
    "        x = self.res_block_up1(x, torch.cat((embedding, noise_chunks[1]), dim=1))  # (N, 256, 2, 32)\n",
    "        x = self.res_block_up2(x, torch.cat((embedding, noise_chunks[2]), dim=1))  # (N, 128, 4, 64)\n",
    "        x = self.res_block_up3(x, torch.cat((embedding, noise_chunks[3]), dim=1))  # (N, 128, 8, 128)\n",
    "        x = self.res_block_up4(x, torch.cat((embedding, noise_chunks[4]), dim=1))  # (N, 64, 16, 256)\n",
    "        x = self.self_attention(x)  # (N, 64, 16, 256)\n",
    "        x = self.res_block_up5(x, torch.cat((embedding, noise_chunks[5]), dim=1))  # (N, 32, 32, 512)\n",
    "        x = self.res_block_up6(x, torch.cat((embedding, noise_chunks[6]), dim=1))  # (N, 16, 64, 1024)\n",
    "        x = self.res_block_up7(x, torch.cat((embedding, noise_chunks[7]), dim=1))  # (N, 16, 128, 2048)\n",
    "        # pass the tensor through the batch norm layer\n",
    "        x = self.batch_norm(x)  # (N, 16, 128, 2048)\n",
    "        # pass the tensor through the relu layer\n",
    "        x = self.relu(x)  # (N, 16, 128, 2048)\n",
    "        # pass the tensor through the convolution layer\n",
    "        x = self.conv(x)  # (N, 1, 128, 2048)\n",
    "        # pass the tensor through the sigmoid\n",
    "        x = self.sigmoid(x)  # (N, 1, 128, 2048)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN\n",
    "    Input with a 128 x 2048 grayscale image\n",
    "    Output a probability that the image is real and not generated\n",
    "    Purpose is to determine if the image is real or generated, to encourage the generator to produce realistic images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        noise_dim: dimension of the noise vector, should be divisible by 8\n",
    "        embedding_dim: dimension of the embedding vector\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.res_block_down1 = ResBlockDown(1, 16)\n",
    "        self.res_block_down2 = ResBlockDown(16, 16)\n",
    "        self.res_block_down3 = ResBlockDown(16, 32)\n",
    "        self.self_attention = SelfAttention(32)\n",
    "        self.res_block_down4 = ResBlockDown(32, 64)\n",
    "        self.res_block_down5 = ResBlockDown(64, 128)\n",
    "        self.res_block_down6 = ResBlockDown(128, 128)\n",
    "        self.res_block_down7 = ResBlockDown(128, 256)\n",
    "        self.res_block = ResBlock(256, 256)\n",
    "        self.global_sum_pooling = GlobalSumPooling()\n",
    "        self.fc = spectral_norm(nn.Linear(256, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        image: image tensor of shape (N, 1, 128, 2048)\n",
    "        \"\"\"\n",
    "\n",
    "        # pass the tensor through the ResBlockDown layers\n",
    "        x = self.res_block_down1(image)  # (N, 16, 64, 1024)\n",
    "        x = self.res_block_down2(x)  # (N, 16, 32, 512)\n",
    "        x = self.res_block_down3(x)   # (N, 32, 16, 256)\n",
    "        x = self.self_attention(x)  # (N, 32, 16, 256)\n",
    "        x = self.res_block_down4(x)  # (N, 64, 8, 128)\n",
    "        x = self.res_block_down5(x)  # (N, 128, 4, 64)\n",
    "        x = self.res_block_down6(x)  # (N, 128, 2, 32)\n",
    "        x = self.res_block_down7(x)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the ResBlock layer\n",
    "        x = self.res_block(x)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the global sum pooling layer\n",
    "        x = self.global_sum_pooling(x)  # (N, 256)\n",
    "        # pass the tensor through the fully connected layer\n",
    "        x = self.fc(x)  # (N, 1)\n",
    "        # pass the tensor through the sigmoid\n",
    "        x = self.sigmoid(x)  # (N, 1)\n",
    "\n",
    "        return x\n",
    "    pass\n",
    "\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    Input with a 128 x 2048 grayscale image\n",
    "    Output a vector representation of the text\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: http://www.tbluche.com/files/icdar17_gnn.pdf use \"big architecture\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "fid load image_count 32\n",
      "fid load image_count 64\n",
      "fid load image_count 96\n",
      "fid load image_count 128\n",
      "fid load image_count 160\n",
      "fid load image_count 192\n",
      "fid load image_count 224\n",
      "fid load image_count 256\n",
      "fid load image_count 288\n",
      "fid load image_count 320\n",
      "fid load image_count 352\n",
      "fid load image_count 384\n",
      "fid load image_count 416\n",
      "fid load image_count 448\n",
      "fid load image_count 480\n",
      "fid load image_count 512\n",
      "fid load image_count 544\n",
      "fid load image_count 576\n",
      "fid load image_count 608\n",
      "fid load image_count 640\n",
      "fid load image_count 672\n",
      "fid load image_count 704\n",
      "fid load image_count 736\n",
      "fid load image_count 768\n",
      "fid load image_count 800\n",
      "epoch 0 batch 0\n",
      "real_image_batch.shape torch.Size([32, 1, 128, 2048])\n",
      "input_text_batch.shape torch.Size([32, 82])\n",
      "torch.Size([32, 128]) torch.Size([32, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9981],\n",
      "        [0.9461],\n",
      "        [0.9802],\n",
      "        [0.9993],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [0.9967],\n",
      "        [0.9991],\n",
      "        [0.9995],\n",
      "        [0.9997],\n",
      "        [1.0000],\n",
      "        [0.9910],\n",
      "        [0.9950],\n",
      "        [0.9377],\n",
      "        [0.9974],\n",
      "        [0.9983],\n",
      "        [1.0000],\n",
      "        [0.9999],\n",
      "        [0.9969],\n",
      "        [0.9996],\n",
      "        [0.9700],\n",
      "        [0.9959],\n",
      "        [0.9493],\n",
      "        [0.9504],\n",
      "        [1.0000],\n",
      "        [0.9927],\n",
      "        [0.9996],\n",
      "        [0.9994],\n",
      "        [0.9987],\n",
      "        [1.0000],\n",
      "        [0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [0.9986],\n",
      "        [0.9972],\n",
      "        [0.9998],\n",
      "        [0.9999],\n",
      "        [1.0000],\n",
      "        [0.9999],\n",
      "        [0.9999],\n",
      "        [0.9998],\n",
      "        [0.9919],\n",
      "        [0.9998],\n",
      "        [0.9998],\n",
      "        [0.9999],\n",
      "        [0.9989],\n",
      "        [1.0000],\n",
      "        [0.9999],\n",
      "        [0.9999],\n",
      "        [0.9998],\n",
      "        [1.0000],\n",
      "        [0.9897],\n",
      "        [0.9984],\n",
      "        [0.9999],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [0.9996],\n",
      "        [0.9973],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9461],\n",
      "        [0.9978],\n",
      "        [1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0)\n",
      "tensor(64.7831, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[1.0877e-26],\n",
      "        [3.9941e-25],\n",
      "        [6.9072e-28],\n",
      "        [3.6240e-30],\n",
      "        [5.4782e-31],\n",
      "        [2.7997e-37],\n",
      "        [0.0000e+00],\n",
      "        [1.7418e-28],\n",
      "        [9.4994e-28],\n",
      "        [5.8388e-30],\n",
      "        [3.3537e-30],\n",
      "        [2.2939e-28],\n",
      "        [1.0253e-25],\n",
      "        [3.9947e-29],\n",
      "        [1.1243e-24],\n",
      "        [2.3606e-26],\n",
      "        [1.7226e-29],\n",
      "        [3.1868e-34],\n",
      "        [2.6856e-31],\n",
      "        [1.1856e-23],\n",
      "        [9.6261e-29],\n",
      "        [3.5833e-28],\n",
      "        [1.9492e-28],\n",
      "        [1.6006e-24],\n",
      "        [3.6953e-25],\n",
      "        [1.1761e-31],\n",
      "        [3.7708e-25],\n",
      "        [6.0439e-28],\n",
      "        [3.6025e-26],\n",
      "        [9.9420e-28],\n",
      "        [1.9285e-31],\n",
      "        [7.4579e-27]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 0 batch 1\n",
      "real_image_batch.shape torch.Size([32, 1, 128, 2048])\n",
      "input_text_batch.shape torch.Size([32, 82])\n",
      "torch.Size([32, 128]) torch.Size([32, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5086e-23],\n",
      "        [5.3581e-29],\n",
      "        [4.2345e-30],\n",
      "        [1.3716e-32],\n",
      "        [7.9170e-27],\n",
      "        [2.8619e-25],\n",
      "        [2.0070e-27],\n",
      "        [1.3077e-27],\n",
      "        [3.9269e-24],\n",
      "        [9.8466e-25],\n",
      "        [3.0366e-26],\n",
      "        [7.9864e-26],\n",
      "        [1.3802e-23],\n",
      "        [6.1531e-36],\n",
      "        [6.8478e-26],\n",
      "        [6.4170e-22],\n",
      "        [5.9858e-26],\n",
      "        [2.1153e-22],\n",
      "        [6.4338e-28],\n",
      "        [1.5476e-24],\n",
      "        [3.4880e-29],\n",
      "        [1.5030e-23],\n",
      "        [3.0465e-25],\n",
      "        [1.3930e-38],\n",
      "        [6.1209e-30],\n",
      "        [2.2029e-23],\n",
      "        [1.9556e-23],\n",
      "        [5.9078e-23],\n",
      "        [4.0185e-21],\n",
      "        [6.4548e-34],\n",
      "        [4.3250e-31],\n",
      "        [3.9411e-28]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[4.0998e-24],\n",
      "        [1.3843e-22],\n",
      "        [2.1901e-33],\n",
      "        [6.1506e-23],\n",
      "        [2.4087e-21],\n",
      "        [2.7035e-23],\n",
      "        [3.5144e-28],\n",
      "        [1.5678e-22],\n",
      "        [0.0000e+00],\n",
      "        [1.1465e-30],\n",
      "        [6.7328e-23],\n",
      "        [2.9599e-26],\n",
      "        [3.5884e-23],\n",
      "        [1.6453e-30],\n",
      "        [9.5090e-23],\n",
      "        [2.9908e-26],\n",
      "        [3.9384e-24],\n",
      "        [0.0000e+00],\n",
      "        [3.5860e-22],\n",
      "        [2.7874e-29],\n",
      "        [1.6753e-26],\n",
      "        [3.9204e-22],\n",
      "        [3.6533e-25],\n",
      "        [6.9844e-26],\n",
      "        [1.1956e-27],\n",
      "        [5.3455e-27],\n",
      "        [8.1821e-23],\n",
      "        [1.9621e-23],\n",
      "        [1.3070e-25],\n",
      "        [1.7166e-25],\n",
      "        [4.6189e-25],\n",
      "        [1.8664e-24]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0)\n",
      "tensor(59.5911, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[5.8966e-23],\n",
      "        [1.2250e-28],\n",
      "        [1.0741e-29],\n",
      "        [3.3156e-32],\n",
      "        [1.7236e-26],\n",
      "        [5.1682e-25],\n",
      "        [4.2869e-27],\n",
      "        [2.9498e-27],\n",
      "        [7.6906e-24],\n",
      "        [2.1651e-24],\n",
      "        [6.5743e-26],\n",
      "        [1.5978e-25],\n",
      "        [2.6123e-23],\n",
      "        [1.3938e-35],\n",
      "        [1.3652e-25],\n",
      "        [1.1177e-21],\n",
      "        [1.1796e-25],\n",
      "        [4.0043e-22],\n",
      "        [1.3947e-27],\n",
      "        [2.9772e-24],\n",
      "        [8.0679e-29],\n",
      "        [2.7118e-23],\n",
      "        [6.3154e-25],\n",
      "        [3.3378e-38],\n",
      "        [1.2707e-29],\n",
      "        [4.2744e-23],\n",
      "        [3.8417e-23],\n",
      "        [1.1365e-22],\n",
      "        [6.7400e-21],\n",
      "        [1.4532e-33],\n",
      "        [1.2042e-30],\n",
      "        [9.6511e-28]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 0 batch 2\n",
      "real_image_batch.shape torch.Size([32, 1, 128, 2048])\n",
      "input_text_batch.shape torch.Size([32, 82])\n",
      "torch.Size([32, 128]) torch.Size([32, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.8962e-29],\n",
      "        [4.0870e-22],\n",
      "        [7.9274e-32],\n",
      "        [2.7780e-24],\n",
      "        [3.9836e-34],\n",
      "        [1.1689e-26],\n",
      "        [3.8226e-25],\n",
      "        [6.2380e-22],\n",
      "        [5.1027e-26],\n",
      "        [7.1097e-32],\n",
      "        [3.9735e-27],\n",
      "        [3.3741e-28],\n",
      "        [1.2787e-24],\n",
      "        [1.7020e-23],\n",
      "        [7.8269e-23],\n",
      "        [2.9842e-24],\n",
      "        [1.5025e-26],\n",
      "        [1.4488e-24],\n",
      "        [5.2551e-31],\n",
      "        [3.5644e-26],\n",
      "        [8.3029e-24],\n",
      "        [1.1208e-29],\n",
      "        [3.4053e-24],\n",
      "        [1.2232e-22],\n",
      "        [1.4433e-25],\n",
      "        [1.2489e-28],\n",
      "        [5.2377e-25],\n",
      "        [3.4799e-35],\n",
      "        [3.6600e-25],\n",
      "        [1.5305e-25],\n",
      "        [2.3011e-23],\n",
      "        [4.7502e-23]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[9.8982e-25],\n",
      "        [8.6196e-28],\n",
      "        [3.0094e-22],\n",
      "        [7.9216e-22],\n",
      "        [7.9224e-31],\n",
      "        [1.7681e-26],\n",
      "        [2.0055e-27],\n",
      "        [1.5523e-31],\n",
      "        [1.1117e-21],\n",
      "        [7.1816e-24],\n",
      "        [4.2195e-22],\n",
      "        [1.0782e-26],\n",
      "        [5.3053e-22],\n",
      "        [0.0000e+00],\n",
      "        [1.7997e-23],\n",
      "        [5.2298e-26],\n",
      "        [2.3241e-22],\n",
      "        [6.5093e-26],\n",
      "        [6.8184e-23],\n",
      "        [2.5258e-24],\n",
      "        [3.1682e-22],\n",
      "        [1.1829e-25],\n",
      "        [8.7210e-29],\n",
      "        [3.5209e-26],\n",
      "        [1.6641e-22],\n",
      "        [4.4501e-23],\n",
      "        [1.3411e-24],\n",
      "        [7.3068e-22],\n",
      "        [6.7967e-30],\n",
      "        [2.9430e-23],\n",
      "        [1.2996e-26],\n",
      "        [1.7125e-22]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0)\n",
      "tensor(58.8001, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[3.9643e-29],\n",
      "        [4.3143e-22],\n",
      "        [1.1814e-31],\n",
      "        [3.4940e-24],\n",
      "        [5.6747e-34],\n",
      "        [1.6335e-26],\n",
      "        [4.7548e-25],\n",
      "        [7.2298e-22],\n",
      "        [6.2551e-26],\n",
      "        [9.2465e-32],\n",
      "        [4.9149e-27],\n",
      "        [4.0440e-28],\n",
      "        [1.7048e-24],\n",
      "        [1.9420e-23],\n",
      "        [9.8444e-23],\n",
      "        [4.0751e-24],\n",
      "        [2.0138e-26],\n",
      "        [1.8350e-24],\n",
      "        [7.3604e-31],\n",
      "        [4.5365e-26],\n",
      "        [1.0057e-23],\n",
      "        [1.5446e-29],\n",
      "        [4.3475e-24],\n",
      "        [1.4379e-22],\n",
      "        [1.9682e-25],\n",
      "        [1.7528e-28],\n",
      "        [6.6413e-25],\n",
      "        [4.8018e-35],\n",
      "        [4.7728e-25],\n",
      "        [1.9837e-25],\n",
      "        [2.8537e-23],\n",
      "        [5.5238e-23]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 0 batch 3\n",
      "real_image_batch.shape torch.Size([32, 1, 128, 2048])\n",
      "input_text_batch.shape torch.Size([32, 82])\n",
      "torch.Size([32, 128]) torch.Size([32, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "encoder = Encoder()\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train_gan( num_epochs=10, batch_size=32, encoder=encoder, generator=generator ,discriminator=discriminator, train_real_image_dataset=line_image_dataset_train, val_real_image_dataset=line_image_dataset_val, train_input_text_dataset=line_transcription_dataset_train, val_input_text_dataset=line_transcription_dataset_val, recognizer=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnacondaPyCharm3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
