{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model for Handwritten Text Synthesis GAN\n",
    "\n",
    "This model will consist of 4 major networks, following the general architecture of an GAN.\n",
    "\n",
    "1. Encoder: Produces an embedding that will be concatenated with the noise vector.\n",
    "2. Generator: Taking noise vector as input and the text embedding to produce an 128x2048 image.\n",
    "3. Discriminator: Trained alternating with generator input and ground-truth input, binary classification real or fake.\n",
    "4. Recognizer: Taking image as input, produce a vector representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/aps360/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch_fidelity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, Subset, random_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.text import CharErrorRate\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, ToPILImage, ToTensor\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Run once only to format data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_HEIGHT = 32\n",
    "SCALE_WIDTH = SCALE_HEIGHT*16\n",
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying. This new `.txt` file contains all info necessary\n",
    "    for the functionality of this project.\n",
    "    \"\"\"\n",
    "\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "\n",
    "        # First write the headers at the top of the file\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actual items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace, we join string till the end\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            # Punctuation include , ! ? ' \" , : ; -\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            num_samples += 1\n",
    "\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            \n",
    "            # Read image, gets its dimensions, perform processing operations, and other stuff\n",
    "            tmp_image = cv.imread(os.path.join(image_path_head, image_path_tail), cv.IMREAD_GRAYSCALE)  # Removes the channel dimension\n",
    "            height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            # Condition here to speed up overall processing time\n",
    "            if width * (SCALE_HEIGHT/height) >= SCALE_WIDTH:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image(cv_image, graylevel):\n",
    "    \"\"\"\n",
    "    Takes in a grayscale image that OpenCV read of shape (H, W) of type uint8\n",
    "    Returns a PyTorch tensor of shape (1, 32, W'), where W' is the scaled width\n",
    "    This tensor is padded and effectively thresholded\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaling factor\n",
    "    height, width = cv_image.shape\n",
    "    scale = SCALE_HEIGHT/height\n",
    "    scaled_width = int(width*scale)\n",
    "\n",
    "    # Trick here is to apply threshold before resize and padding\n",
    "    # This allows OpenCV resizing to create a cleaner output image\n",
    "    # 2nd return value is the thresholded image\n",
    "    output = cv.threshold(cv_image, graylevel, 255, cv.THRESH_BINARY)[1]\n",
    "\n",
    "    # INTER_AREA recommended for sizing down\n",
    "    output = cv.resize(output, (scaled_width, SCALE_HEIGHT), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    # Turn it back to a tensor and map to [0, 1]\n",
    "    output = torch.from_numpy(output).unsqueeze(0).type(torch.float32)\n",
    "    output = (output-output.min()) / (output.max()-output.min())\n",
    "    \n",
    "    # Add padding\n",
    "    _, _, resized_height = output.shape\n",
    "    padding_to_add = SCALE_WIDTH - resized_height\n",
    "    output = F.pad(output, (0, padding_to_add), value=1.0)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "# preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer â‡” Char Mapping Dict (Run everytime before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "# Reserve 0 for CTC blank\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataframe containing the stuff in `lines_improved.txt`\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # Class properties\n",
    "        self.ty = ty  # Type of dataset (lines, images, or both)\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "\n",
    "        # Temp variables...\n",
    "        length = self.lines_df.shape[0]\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i][\"transcription\"].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "\n",
    "        # ...for the important data\n",
    "        if self.ty in (\"txt\", None):  # Added this condition to speed thigns up if only text\n",
    "            self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i]))) for i in range(length)]\n",
    "        if self.ty in (\"img\", None):\n",
    "            self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Datasets and Subsets of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "100 20\n",
      "images\n",
      "100 20\n",
      "both\n",
      "1000 200\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(100))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(20))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(100))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(20))\n",
    "\n",
    "line_dataset_train = Subset(line_dataset_train, range(1000))\n",
    "line_dataset_val = Subset(line_dataset_val, range(200))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Example Image and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([32, 512])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = line_dataset_train[0]\n",
    "print(image.shape)\n",
    "plt.title(\"\".join([int_to_char[int(val)] for val in label[label.nonzero()]]))\n",
    "print(image.squeeze(0).shape)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "label, \"\".join([int_to_char[int(val)] for val in label[label.nonzero()]])\n",
    "# line_dataset.lines_df.iloc[798]\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Key Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizer Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recog_accuracy(preds, target):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the recognizer with character error rate\n",
    "    which is based on edit distance\n",
    "\n",
    "    Params:\n",
    "        preds: a list of prediction strings\n",
    "        targets: a list of target strings\n",
    "\n",
    "    Returns:\n",
    "        An integer, the character error rate average across\n",
    "        all predictions and targets\n",
    "    \"\"\"\n",
    "\n",
    "    cer = CharErrorRate()\n",
    "    return cer(preds, target)\n",
    "\n",
    "\n",
    "def create_strings_from_tensor(int_tensor):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        int_tensor: A shape (N, 82) tensor where each row corresponds to\n",
    "        a integer mapping of a string. Includes padding\n",
    "    \n",
    "    Returns:\n",
    "        A list of N strings\n",
    "    \"\"\"\n",
    "\n",
    "    strings = []\n",
    "    for string_map in int_tensor:\n",
    "        strings.append(\"\".join([int_to_char[int(i)] for i in string_map[string_map != 0]]))\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataloading Functions\"\"\"\n",
    "\"\"\"Datasaving Functions\"\"\"\n",
    "\n",
    "\"\"\"Plotting Functions\"\"\"\n",
    "\"\"\"Evaluation Functions\"\"\"\n",
    "def calculate_gan_loss_and_accuracies(generator, encoder, discriminator, recognizer, \n",
    "                                  line_loader, fid_set,\n",
    "                                  batch_size=64, adversarial_loss_function=nn.BCELoss(), recognizer_loss_function=nn.NLLLoss(),\n",
    "                                  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    \"\"\"\n",
    "    Calculates the loss for the GAN\n",
    "    \n",
    "    Params:\n",
    "        generator: The generator model\n",
    "        encoder: The encoder model\n",
    "        discriminator: The discriminator model\n",
    "        recognizer: The recognizer model\n",
    "        line_loader: The dataloader for lines and their corresponding texts\n",
    "        batch_size: The batch size to use\n",
    "        adversarial_loss_function: The adversarial loss function to use\n",
    "        recognizer_loss_function: The recognizer loss function to use\n",
    "\n",
    "    Returns:\n",
    "        generator_and_encoder_loss: The loss for the generator and encoder\n",
    "        discriminator_loss: The loss for the discriminator\n",
    "    \"\"\"\n",
    "    fid_set.reset()  # Just to make sure\n",
    "    generator_and_encoder_loss = 0\n",
    "    discriminator_loss = 0\n",
    "    discriminator_accuracy = 0\n",
    "    recognizer_error = 0  # Character error rate\n",
    "    with torch.no_grad():\n",
    "        for i, (line_image_batch, input_text_batch) in enumerate(line_loader):\n",
    "            print(i)\n",
    "            line_image_batch = line_image_batch.to(device)\n",
    "            input_text_batch = input_text_batch.to(device)\n",
    "\n",
    "            # generate noise of N x noise_dim\n",
    "            noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "\n",
    "            # Use encoder and generator to generate fake images\n",
    "            text_embedding = encoder(input_text_batch)\n",
    "            fake_image_batch = generator(noise, text_embedding)\n",
    "\n",
    "            # train discriminator\n",
    "            discriminator_output_for_real_images = discriminator(line_image_batch)\n",
    "            discriminator_output_for_fake_images = discriminator(fake_image_batch)\n",
    "            label_for_real_images = torch.ones_like(discriminator_output_for_real_images).to(device)\n",
    "            label_for_fake_images = torch.zeros_like(discriminator_output_for_fake_images).to(device)\n",
    "            real_images_loss = adversarial_loss_function(discriminator_output_for_real_images, label_for_real_images)\n",
    "            fake_images_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_fake_images)\n",
    "            discriminator_loss = (real_images_loss + fake_images_loss) / 2\n",
    "            discriminator_loss += discriminator_loss.item()\n",
    "            discriminator_accuracy += (torch.sum(discriminator_output_for_real_images >= 0.5).item() + torch.sum(discriminator_output_for_fake_images < 0.5).item()) / (2 * batch_size)\n",
    "\n",
    "            # train generator\n",
    "            \n",
    "            adversarial_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_real_images)\n",
    "            recognizer_outputs = recognizer(fake_image_batch)\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # Takes input torch.Size([N, 73, 82])\n",
    "                F.log_softmax(recognizer_outputs, 1),\n",
    "                input_text_batch  # Shape N by 82\n",
    "            )\n",
    "            \n",
    "            # balance the losses from different sources, according to https://arxiv.org/pdf/1903.00277.pdf\n",
    "            # adversarial_loss_mean, adversarial_loss_std = torch.mean(adversarial_loss), torch.std(adversarial_loss)\n",
    "            # recognizer_loss_mean, recognizer_loss_std = torch.mean(recognizer_loss), torch.std(recognizer_loss)\n",
    "            # recognizer_loss = loss_balancing_alpha * (adversarial_loss_std / recognizer_loss_std) * (recognizer_loss - recognizer_loss_mean) + adversarial_loss_mean\n",
    "            \n",
    "            generator_loss = adversarial_loss + recognizer_loss\n",
    "            generator_and_encoder_loss += generator_loss.item()\n",
    "\n",
    "            # calculate fid\n",
    "            fid_set.update(fake_image_batch.cpu().repeat(1, 3, 1, 1), real=False)\n",
    "\n",
    "            # Calculate recognizer error\n",
    "            preds = create_strings_from_tensor(torch.argmax(recognizer_outputs, dim=1))\n",
    "            target = create_strings_from_tensor(input_text_batch)\n",
    "            print(preds, target)\n",
    "            recognizer_error += calculate_recog_accuracy(preds, target)\n",
    "\n",
    "    generator_and_encoder_error = fid_set.compute().item()\n",
    "    fid_set.reset()\n",
    "    discriminator_accuracy /= len(line_loader)\n",
    "    generator_and_encoder_loss /= len(line_loader)\n",
    "    discriminator_loss /= len(line_loader)\n",
    "    recognizer_error /= len(line_loader)\n",
    "\n",
    "    return generator_and_encoder_loss, discriminator_loss, generator_and_encoder_error, discriminator_accuracy, recognizer_error\n",
    "\n",
    "\"\"\"Training Functions\"\"\"\n",
    "def train_recognizer(recognizer, train_line_loader, val_line_loader, batch_size=64, learning_rate=2e-4, betas=(0, 0.999), num_epochs=30):\n",
    "    # only train on real images\n",
    "    # also save model, plot graphs, save graphs\n",
    "    pass\n",
    "\n",
    "def train_gan(generator, encoder, discriminator, recognizer, \n",
    "              train_line_dataset, val_line_dataset,\n",
    "              batch_size=64, encoder_lr=1e-5, generator_lr=1e-5, discriminator_lr=1e-5,\n",
    "              betas=(0, 0.999), num_epochs=30, loss_balancing_alpha=1,\n",
    "              num_generator_updates_per_discriminator_update=1):\n",
    "    # Note, the generator and discriminator should be spectrally normalized before training\n",
    "    # TODO: load dataloader with batch size batch_size\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    generator = generator.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    recognizer = recognizer.to(device)\n",
    "    \n",
    "    train_line_loader = DataLoader(train_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_line_loader = DataLoader(val_line_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr, betas=betas)\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=generator_lr, betas=betas)\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=discriminator_lr, betas=betas)\n",
    "    \n",
    "    adversarial_loss_function = nn.BCELoss()  # discriminator already has sigmoid\n",
    "    recognizer_loss_function = nn.NLLLoss()\n",
    "    saving_filenames = {\n",
    "        \"encoder\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lrE{encoder_lr}_lrG{generator_lr}_lrD{discriminator_lr}_betas{betas}_encoder\"),\n",
    "        \"generator\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lrE{encoder_lr}_lrG{generator_lr}_lrD{discriminator_lr}_betas{betas}_generator\"),\n",
    "        \"discriminator\": os.path.join(\"main_model\", \"model_snapshots\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lrE{encoder_lr}_lrG{generator_lr}_lrD{discriminator_lr}_betas{betas}_discriminator\"), \n",
    "        \"losses\": os.path.join(\"main_model\", \"model_training_information\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lrE{encoder_lr}_lrG{generator_lr}_lrD{discriminator_lr}_betas{betas}_losses\"),\n",
    "        \"accuracies\": os.path.join(\"main_model\", \"model_training_information\", f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_bs{batch_size}_lrE{encoder_lr}_lrG{generator_lr}_lrD{discriminator_lr}_accuracies\")\n",
    "        }\n",
    "\n",
    "    best_generator_and_encoder_val_loss = float('inf')\n",
    "    best_discriminator_val_loss = float('inf')\n",
    "    best_generator_and_encoder_val_error = 0\n",
    "    best_discriminator_val_accuracy = 0\n",
    "    saved_generator_and_encoder_models_epochs = []\n",
    "    saved_discriminator_models_epochs = []\n",
    "\n",
    "    generator_and_encoder_train_losses = []\n",
    "    discriminator_train_losses = []\n",
    "    generator_and_encoder_train_accuracies = []\n",
    "    discriminator_train_accuracies = []\n",
    "    generator_and_encoder_val_losses = []\n",
    "    discriminator_val_losses = []\n",
    "    generator_and_encoder_val_accuracies = []\n",
    "    discriminator_val_accuracies = []\n",
    "    recognizer_train_errors = []\n",
    "    recognizer_val_errors = []\n",
    "\n",
    "    # FID: https://torchmetrics.readthedocs.io/en/stable/image/frechet_inception_distance.html\n",
    "    print(\"FID on train and validation set (all images)\")\n",
    "    fid_set = FrechetInceptionDistance(feature=2048, reset_real_features=False, normalize=True)\n",
    "    for i, (real_image_batch, _) in enumerate(train_line_loader):\n",
    "        print(i, real_image_batch.shape)\n",
    "        fid_set.update(real_image_batch.cpu().repeat(1, 3, 1, 1), real=True)\n",
    "    for i, (real_image_batch, _) in enumerate(val_line_loader):\n",
    "        print(i, real_image_batch.shape)\n",
    "        fid_set.update(real_image_batch.cpu().repeat(1, 3, 1, 1), real=True)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        display_images = []\n",
    "\n",
    "        generator_and_encoder_train_loss = 0\n",
    "        discriminator_train_loss = 0\n",
    "        discriminator_train_accuracy = 0\n",
    "        recognizer_train_error = 0\n",
    "        \n",
    "        times_discriminator_updated = 0\n",
    "\n",
    "        for i, (real_image_batch, input_text_batch) in enumerate(train_line_loader):\n",
    "            print(\"epoch\", epoch, \"batch\", i)\n",
    "            # print(\"real_image_batch.shape\", real_image_batch.shape)\n",
    "            # print(\"input_text_batch.shape\", input_text_batch.shape)\n",
    "\n",
    "            real_image_batch = real_image_batch.to(device)\n",
    "            input_text_batch = input_text_batch.to(device)\n",
    "\n",
    "            # generate noise of N x noise_dim\n",
    "            noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "\n",
    "            # Use encoder and generator to generate fake images\n",
    "            text_embedding = encoder(input_text_batch)\n",
    "            # print(noise.shape, text_embedding.shape)\n",
    "            fake_image_batch = generator(noise, text_embedding)\n",
    "            # We won't need to pass in real images to recognizer because we use pretrained\n",
    "            # We pretrained the recognizer ourselves\n",
    "\n",
    "            # display random image from the batch\n",
    "            sample_text = input_text_batch[0][input_text_batch[0].nonzero()]\n",
    "            sample_text = \"\".join([int_to_char[int(i)] for i in sample_text])\n",
    "            print(sample_text)\n",
    "            plt.imshow(fake_image_batch.cpu().detach().numpy()[0].squeeze(0), cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            # Only train discriminator every num_generator_updates_per_discriminator_update iterations\n",
    "            if i % num_generator_updates_per_discriminator_update == 0:\n",
    "                times_discriminator_updated += 1\n",
    "                # train discriminator\n",
    "                discriminator_optimizer.zero_grad()\n",
    "                # real_image_batch = real_image_batch.type(torch.float32)\n",
    "                discriminator_output_for_real_images = discriminator(real_image_batch)\n",
    "                discriminator_output_for_fake_images = discriminator(fake_image_batch.detach())  # added detatch to prevent gradients from flowing back to generator\n",
    "                label_for_real_images = torch.ones_like(discriminator_output_for_real_images).to(device)*0.99  # To prevent discriminator from getting too confident\n",
    "                label_for_fake_images = torch.zeros_like(discriminator_output_for_fake_images).to(device)+0.01  # To prevent discriminator from getting too confident\n",
    "                real_images_loss = adversarial_loss_function(discriminator_output_for_real_images, label_for_real_images)\n",
    "                fake_images_loss = adversarial_loss_function(discriminator_output_for_fake_images, label_for_fake_images)\n",
    "                discriminator_loss = real_images_loss + fake_images_loss\n",
    "                discriminator_loss.backward()  # retain_graph=True because we will use the same discriminator for the generator\n",
    "                \n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=0.5)\n",
    "                \n",
    "                discriminator_optimizer.step()\n",
    "                discriminator_train_loss += discriminator_loss.item()\n",
    "                discriminator_train_accuracy += (torch.sum(discriminator_output_for_real_images > 0.5) + torch.sum(discriminator_output_for_fake_images < 0.5)) / (2 * batch_size)\n",
    "\n",
    "            # train generator\n",
    "            generator_optimizer.zero_grad()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            # # generate noise of N x noise_dim\n",
    "            # noise = torch.randn(len(input_text_batch), generator.noise_dim).to(device)\n",
    "            # # Use encoder and generator to generate fake images\n",
    "            # text_embedding = encoder(input_text_batch)\n",
    "            # fake_image_batch = generator(noise, text_embedding)\n",
    "            # # display random image from the batch\n",
    "            # plt.imshow(fake_image_batch.cpu().detach().numpy()[0].squeeze(0), cmap='gray')\n",
    "            # plt.show()\n",
    "            discriminator_output_for_fake_images = discriminator(fake_image_batch)\n",
    "            adversarial_loss = adversarial_loss_function(discriminator_output_for_fake_images, torch.ones_like(discriminator_output_for_fake_images).to(device)*0.99)  # note that we want the fake images to be classified as real, also the *0.99 is to prevent generator from getting too confident\n",
    "            # print(torch.sum(discriminator_output_for_fake_images > 0.5))\n",
    "            # print(adversarial_loss)\n",
    "\n",
    "            # Feed in fake images to the pretrained recognizer for decoding\n",
    "            recognizer_outputs_from_fake = recognizer(fake_image_batch)\n",
    "            # Apply NLLLoss()\n",
    "            recognizer_loss = recognizer_loss_function(\n",
    "                # Takes input torch.Size([N, 73, 82])\n",
    "                F.log_softmax(recognizer_outputs_from_fake, 1),\n",
    "                input_text_batch  # Shape N by 82\n",
    "            )\n",
    "\n",
    "            # Calculate recognizer error\n",
    "            preds = create_strings_from_tensor(torch.argmax(recognizer_outputs_from_fake, dim=1))\n",
    "            target = create_strings_from_tensor(input_text_batch)\n",
    "            recognizer_train_error += calculate_recog_accuracy(preds, target)\n",
    "            \n",
    "            # balance the losses from different sources, according to https://arxiv.org/pdf/1903.00277.pdf\n",
    "            # adversarial_loss_mean, adversarial_loss_std = torch.mean(adversarial_loss), torch.std(adversarial_loss)\n",
    "            # recognizer_loss_mean, recognizer_loss_std = torch.mean(recognizer_loss), torch.std(recognizer_loss)\n",
    "            # recognizer_loss = loss_balancing_alpha * (adversarial_loss_std / recognizer_loss_std) * (recognizer_loss - recognizer_loss_mean) + adversarial_loss_mean\n",
    "            # TODO check magnitude of NLLLoss (within the same or at most 1 order of magnitude), try to make them the same\n",
    "            \n",
    "            generator_loss = adversarial_loss + recognizer_loss\n",
    "            generator_loss.backward()\n",
    "            # We do not update recognizer's gradients here\n",
    "            # Recognizer does not train if we don't stop its optimizer, gradients just accumulate\n",
    "            generator_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "            generator_and_encoder_train_loss += generator_loss.item()\n",
    "            \n",
    "            # calculate FID\n",
    "            fid_set.update(fake_image_batch.cpu().repeat(1, 3, 1, 1), real=False)\n",
    "\n",
    "            # display_images.append(fake_image_batch[random.randint(0, len(fake_image_batch) - 1)].detach().numpy())\n",
    "\n",
    "        # plot the collection of display_images, all are greyscale\n",
    "        # display_images = np.array(display_images)\n",
    "        # display_images = np.transpose(display_images, (0, 2, 3, 1))\n",
    "        # display_images = np.squeeze(display_images)\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        # for i in range(25):\n",
    "        #     plt.subplot(5, 5, i + 1)\n",
    "        #     plt.imshow(display_images[i], cmap='gray')\n",
    "        #     plt.axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "\n",
    "\n",
    "        # Tally up losses and accuracies\n",
    "        generator_and_encoder_train_loss /= len(train_line_loader)\n",
    "        discriminator_train_loss /= times_discriminator_updated\n",
    "        generator_and_encoder_train_error = fid_set.compute().item()\n",
    "        discriminator_train_accuracy /= times_discriminator_updated\n",
    "        recognizer_train_error /= (i+1)\n",
    "        fid_set.reset()\n",
    "        # Appending them to the list\n",
    "        generator_and_encoder_train_losses.append(generator_and_encoder_train_loss)\n",
    "        discriminator_train_losses.append(discriminator_train_loss)\n",
    "        generator_and_encoder_train_accuracies.append(generator_and_encoder_train_error)\n",
    "        discriminator_train_accuracies.append(discriminator_train_accuracy.cpu().numpy())\n",
    "        recognizer_train_errors.append(recognizer_train_error)\n",
    "\n",
    "        generator_and_encoder_val_loss, discriminator_val_loss, generator_and_encoder_val_error, discriminator_val_accuracy, recognizer_val_error = calculate_gan_loss_and_accuracies(\n",
    "            generator,\n",
    "            encoder,\n",
    "            discriminator,\n",
    "            recognizer,\n",
    "            val_line_loader,\n",
    "            fid_set, batch_size=batch_size\n",
    "        )\n",
    "        generator_and_encoder_val_losses.append(generator_and_encoder_val_loss)\n",
    "        discriminator_val_losses.append(discriminator_val_loss.cpu().numpy())\n",
    "        generator_and_encoder_val_accuracies.append(generator_and_encoder_val_error)\n",
    "        discriminator_val_accuracies.append(discriminator_val_accuracy)\n",
    "        recognizer_val_errors.append(recognizer_val_error)\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\n\\tGenerator and encoder train loss: {generator_and_encoder_train_loss}\\n\\tDiscriminator train loss: {discriminator_train_loss}\\n\\tGenerator and encoder train accuracy: {generator_and_encoder_train_error}\\n\\tDiscriminator train accuracy: {discriminator_train_accuracy}\\n\\tGenerator and encoder val loss: {generator_and_encoder_val_loss}\\n\\tDiscriminator val loss: {discriminator_val_loss}\\n\\tGenerator and encoder val accuracy: {generator_and_encoder_val_error}\\n\\tDiscriminator val accuracy: {discriminator_val_accuracy}\\n\\tRecognizer train error: {recognizer_train_error}\\n\\tRecognizer val error: {recognizer_val_error}\")\n",
    "\n",
    "        # Save models, only if they are better than the previous best\n",
    "        if generator_and_encoder_val_loss < best_generator_and_encoder_val_loss:\n",
    "            best_generator_and_encoder_val_loss = generator_and_encoder_val_loss\n",
    "            saved_generator_and_encoder_models_epochs.append(epoch)\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if discriminator_val_loss < best_discriminator_val_loss:\n",
    "            best_discriminator_val_loss = discriminator_val_loss\n",
    "            saved_discriminator_models_epochs.append(epoch)\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if generator_and_encoder_val_error < best_generator_and_encoder_val_error:\n",
    "            best_generator_and_encoder_val_error = generator_and_encoder_val_error\n",
    "            saved_generator_and_encoder_models_epochs.append(epoch) if epoch not in saved_generator_and_encoder_models_epochs else None\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "        if discriminator_val_accuracy > best_discriminator_val_accuracy:\n",
    "            best_discriminator_val_accuracy = discriminator_val_accuracy\n",
    "            saved_discriminator_models_epochs.append(epoch) if epoch not in saved_discriminator_models_epochs else None\n",
    "            torch.save(generator.state_dict(), f\"{saving_filenames['generator']}_epoch{epoch}.pt\")\n",
    "            torch.save(encoder.state_dict(), f\"{saving_filenames['encoder']}_epoch{epoch}.pt\")\n",
    "            torch.save(discriminator.state_dict(), f\"{saving_filenames['discriminator']}_epoch{epoch}.pt\")\n",
    "\n",
    "        # # plot title\n",
    "        # plt.title(\"generator and encoder losses\")\n",
    "        # # plot x axis label\n",
    "        # plt.xlabel(\"epoch\")\n",
    "        # # plot y axis label\n",
    "        # plt.ylabel(\"loss\")\n",
    "        # # plot the epoch vs the loss\n",
    "        # plt.plot(range(epoch+1), generator_and_encoder_train_losses, label=\"train\")\n",
    "        # plt.plot(range(epoch+1), generator_and_encoder_val_losses, label=\"val\")\n",
    "        # # plot the saved models as dots\n",
    "        # plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_losses[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "        # # legend\n",
    "        # plt.legend()\n",
    "        # # show the plot\n",
    "        # plt.show()\n",
    "        # # save the plot\n",
    "        # plt.close()\n",
    "\n",
    "        # # plot title\n",
    "        # plt.title(\"discriminator losses\")\n",
    "        # # plot x axis label\n",
    "        # plt.xlabel(\"epoch\")\n",
    "        # # plot y axis label\n",
    "        # plt.ylabel(\"loss\")\n",
    "        # # plot the epoch vs the loss\n",
    "        # plt.plot(range(epoch+1), discriminator_train_losses, label=\"train\")\n",
    "        # plt.plot(range(epoch+1), discriminator_val_losses, label=\"val\")\n",
    "        # # plot the saved models as dots\n",
    "        # plt.scatter(saved_discriminator_models_epochs, [discriminator_val_losses[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "        # # legend\n",
    "        # plt.legend()\n",
    "        # # show the plot\n",
    "        # plt.show()\n",
    "        # # close the plot\n",
    "        # plt.close()\n",
    "\n",
    "        # # plot title\n",
    "        # plt.title(\"generator and encoder errors\")\n",
    "        # # plot x axis label\n",
    "        # plt.xlabel(\"epoch\")\n",
    "        # # plot y axis label\n",
    "        # plt.ylabel(\"accuracy\")\n",
    "        # # plot the epoch vs the accuracy\n",
    "        # plt.plot(range(epoch+1), generator_and_encoder_train_accuracies, label=\"train\")\n",
    "        # plt.plot(range(epoch+1), generator_and_encoder_val_accuracies, label=\"val\")\n",
    "        # # plot the saved models as dots\n",
    "        # plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_accuracies[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "        # # legend\n",
    "        # plt.legend()\n",
    "        # # show the plot\n",
    "        # plt.show()\n",
    "        # # close the plot\n",
    "        # plt.close()\n",
    "\n",
    "        # # plot title\n",
    "        # plt.title(\"discriminator accuracies\")\n",
    "        # # plot x axis label\n",
    "        # plt.xlabel(\"epoch\")\n",
    "        # # plot y axis label\n",
    "        # plt.ylabel(\"accuracy\")\n",
    "        # # plot the epoch vs the accuracy\n",
    "        # plt.plot(range(epoch+1), discriminator_train_accuracies, label=\"train\")\n",
    "        # plt.plot(range(epoch+1), discriminator_val_accuracies, label=\"val\")\n",
    "        # # plot the saved models as dots\n",
    "        # plt.scatter(saved_discriminator_models_epochs, [discriminator_val_accuracies[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "        # # legend\n",
    "        # plt.legend()\n",
    "        # # show the plot\n",
    "        # plt.show()\n",
    "        # # close the plot\n",
    "        # plt.close()\n",
    "\n",
    "    # Plot the losses and accuracies, and save them (filename should be unique, use datetime as a prefix)\n",
    "    # The plot should include which epoch's model we saved\n",
    "    # save the plot, and csv of the losses and accuracies, and which epoch's model we saved\n",
    "    \n",
    "    # plot title\n",
    "    plt.title(\"generator and encoder losses\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"loss\")\n",
    "    # plot the epoch vs the loss\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_train_losses, label=\"train\")\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_val_losses, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_losses[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['losses']}_generator_and_encoder_loss.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"discriminator losses\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"loss\")\n",
    "    # plot the epoch vs the loss\n",
    "    plt.plot(range(num_epochs), discriminator_train_losses, label=\"train\")\n",
    "    plt.plot(range(num_epochs), discriminator_val_losses, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_discriminator_models_epochs, [discriminator_val_losses[i] for i in saved_discriminator_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['losses']}_discriminator_loss.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"generator and encoder errors\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    # plot the epoch vs the accuracy\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_train_accuracies, label=\"train\")\n",
    "    plt.plot(range(num_epochs), generator_and_encoder_val_accuracies, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_generator_and_encoder_models_epochs, [generator_and_encoder_val_accuracies[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['accuracies']}_generator_and_encoder_error.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # plot title\n",
    "    plt.title(\"recognizer errors\")\n",
    "    # plot x axis label\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # plot y axis label\n",
    "    plt.ylabel(\"error\")\n",
    "    # plot the epoch vs the accuracy\n",
    "    plt.plot(range(num_epochs), recognizer_train_errors, label=\"train\")\n",
    "    plt.plot(range(num_epochs), recognizer_val_errors, label=\"val\")\n",
    "    # plot the saved models as dots\n",
    "    plt.scatter(saved_generator_and_encoder_models_epochs, [recognizer_val_errors[i] for i in saved_generator_and_encoder_models_epochs], label=\"saved models\")\n",
    "    # legend\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    plt.savefig(f\"{saving_filenames['accuracies']}_recognizer_error.png\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # save the losses and accuracies as csvs\n",
    "    with open(f\"{saving_filenames['losses']}.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"generator and encoder train loss\", \"discriminator train loss\", \"generator and encoder val loss\", \"discriminator val loss\"])\n",
    "        for i in range(num_epochs):\n",
    "            writer.writerow([i, generator_and_encoder_train_losses[i], discriminator_train_losses[i], generator_and_encoder_val_losses[i], discriminator_val_losses[i]])\n",
    "    with open(f\"{saving_filenames['accuracies']}.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"generator and encoder train error\", \"discriminator train accuracy\", \"recognizer train error\", \"generator and encoder val error\", \"discriminator val accuracy\", \"recognizer val error\"])\n",
    "        for i in range(num_epochs):\n",
    "            writer.writerow([i, generator_and_encoder_train_accuracies[i], discriminator_train_accuracies[i], recognizer_train_errors[i], generator_and_encoder_val_accuracies[i], discriminator_val_accuracies[i], recognizer_val_errors[i]])\n",
    "\n",
    "\n",
    "def load_model(model, model_filename):\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_models_of_same_batch(generator, encoder, discriminator, filename_prefix, epoch_number):\n",
    "    \"\"\"\n",
    "    Load the generator, encoder, and discriminator from files.\n",
    "    \"\"\"\n",
    "    generator = load_model(generator, f\"{filename_prefix}_generator_epoch{epoch_number}.pt\")\n",
    "    encoder = load_model(encoder, f\"{filename_prefix}_encoder_epoch{epoch_number}.pt\")\n",
    "    discriminator = load_model(discriminator, f\"{filename_prefix}_discriminator_epoch{epoch_number}.pt\")\n",
    "    return generator, encoder, discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (N, C, H, W), with condition vector of shape (N, num_conditions)\n",
    "    Output: (N, C, H, W)\n",
    "\n",
    "    Conditional Batch Normalization\n",
    "    Idea obtained from https://arxiv.org/pdf/1809.11096.pdf\n",
    "    This is a network layer that applies batch normalization to the input tensor, and conditions it on a condition vector\n",
    "    For the Generator, this allows the network to learn to generate images conditioned on the class label and the noise vector\n",
    "\n",
    "    This network takes in a condition vector of length num_conditions, and applies batch normalization to the input tensor. \n",
    "    Then it computes 2 affine parameters (scale and bias) for each channel of the input tensor, conditioned on the condition vector through a linear layer.\n",
    "    The affine parameters are then applied to the input tensor, and the output is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_conditions):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        num_conditions: length of the condition vector\n",
    "        \"\"\"\n",
    "        super(ConditionalBatchNorm2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # batch normalize the input, without using affine parameters\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels, affine=False)\n",
    "\n",
    "        # set up affine parameters conditioned on the condition vector\n",
    "        self.embed_conditions = nn.Sequential(\n",
    "            # 512 hidden units are used by https://arxiv.org/pdf/1903.00277.pdf\n",
    "            nn.Linear(num_conditions, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, in_channels * 2)\n",
    "        )\n",
    "        # # https://arxiv.org/pdf/1809.11096.pdf\n",
    "        # # initialize affine parameters to be all zeros for bias and ones for scale\n",
    "        # self.embed_conditions[-1].weight.data.zero_()\n",
    "        # self.embed_conditions[-1].bias.data[:in_channels].zero_()  # bias is the second half of the affine parameters\n",
    "        # self.embed_conditions[-1].bias.data[in_channels:].fill_(1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        conditions: condition vector of shape (N, num_conditions)\n",
    "        \"\"\"\n",
    "        # apply batch normalization, out still has shape (N, C, H, W)\n",
    "        out = self.batch_norm(x)\n",
    "\n",
    "        # compute affine parameters\n",
    "        params = self.embed_conditions(conditions)\n",
    "        # params has shape (N, 2 * C), we split the channel dimension in half into 2 tensors of shape (N, C)\n",
    "        scale, bias = params.chunk(2, dim=1)\n",
    "\n",
    "        # Apply spectral normalization to the scale and bias\n",
    "        scale = scale.view(-1, self.in_channels, 1, 1)\n",
    "        bias = bias.view(-1, self.in_channels, 1, 1)\n",
    "\n",
    "        # apply scale and bias. every channel's values are scaled and biased by the channel's own scale and bias value\n",
    "        out = scale.view(-1, self.in_channels, 1, 1) * out + bias.view(-1, self.in_channels, 1, 1)\n",
    "\n",
    "        # out has shape (N, C, H, W)\n",
    "        return out\n",
    "    \n",
    "class ResBlockUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (N, in_channels, H, W), with condition vector of shape (N, num_conditions)\n",
    "    Output: (N, out_channels, H * 2, W * 2)\n",
    "\n",
    "    Residual Block for Upsampling\n",
    "    Idea obtained from https://arxiv.org/pdf/1903.00277.pdf\n",
    "    This is a network layer that upsamples the input tensor by a factor of 2, and conditions it on a condition vector\n",
    "    For the Generator, this allows the network to learn to generate images conditioned on the class label and the noise vector\n",
    "\n",
    "    This network takes in a condition vector of length num_conditions, and upsamples the input tensor by a factor of 2, accounting for the condition vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_conditions):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "        num_conditions: length of the condition vector\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlockUp, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.batch_norm1 = ConditionalBatchNorm2d(in_channels, num_conditions)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.batch_norm2 = ConditionalBatchNorm2d(out_channels, num_conditions)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        conditions: condition vector of shape (N, num_conditions)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = self.upsample(x)\n",
    "        # depending on if this res_block_up changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x, conditions)\n",
    "        out2 = self.relu(out2)\n",
    "        out2 = self.upsample(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2, conditions)\n",
    "        out2 = self.relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, 2 * H, 2 * W)\n",
    "        \n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self attention Layer\n",
    "    \n",
    "    This code is obtained from https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,in_dim):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1) #\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockDown(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlockDown, self).__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.average_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = x\n",
    "        # depending on if this res_block_down changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        out1 = self.average_pool(out1)\n",
    "        \n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        out2 = self.average_pool(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, H / 2, W / 2)\n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        in_channels: number of channels in the input tensor\n",
    "        out_channels: number of channels in the output tensor\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.conv1x1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        # upsample the input tensor\n",
    "        out1 = x\n",
    "        # depending on if this res_block_down changes the number of channels, we may need to use a 1x1 convolution to change the number of channels\n",
    "        if self.conv1x1 is not None:\n",
    "            out1 = self.conv1x1(out1)\n",
    "        \n",
    "        # second part of the res_block_up\n",
    "        out2 = self.batch_norm1(x)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv1(out2)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.leaky_relu(out2)\n",
    "        out2 = self.conv2(out2)\n",
    "        \n",
    "        # the output has shape (N, out_channels, H, W)\n",
    "        out = out1 + out2\n",
    "        return out\n",
    "    \n",
    "class GlobalSumPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, C, H, W)\n",
    "\n",
    "        returns a tensor of shape (N, C)\n",
    "        \"\"\"\n",
    "        return torch.sum(x, dim=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    Input with a vector representation of an ascii text\n",
    "    Output a vector embedding of the text\n",
    "    Purpose is to produce an embedding of the text that includes the relationship between the characters\n",
    "    Description of the encoder comes from https://arxiv.org/pdf/1903.00277.pdf.\n",
    "    Although we may end up modifying the dimensions of the hidden state and the embedding vector, \n",
    "    to fit our needs of processing longer texts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_embedding_dim=128, hidden_dim=256, num_layers=6, num_chars=73):\n",
    "        \"\"\"\n",
    "        embedding_dim: dimension of the embedding vector\n",
    "        hidden_dim: dimension of the hidden state of the LSTM\n",
    "        num_layers: number of layers in the LSTM\n",
    "        num_chars: number of characters in the vocabulary\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_chars = num_chars\n",
    "\n",
    "        # TODO: embedding can be from a pretrained model\n",
    "        self.embedding = nn.Embedding(num_chars, char_embedding_dim)\n",
    "        # Using bidirectional LSTM. Batch first so that the input is of shape (N, L, C)\n",
    "        self.lstm = nn.LSTM(char_embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (N, L), where L is the length of the text. each element is an integer representing a character (1 to 72)\n",
    "        \"\"\"\n",
    "        # Technically, L is the MAXIMUM length of the text in this batch (cuz padding)\n",
    "        \n",
    "        # First embed each character\n",
    "        x = self.embedding(x)  # output should be (N, L, char_embedding_dim)\n",
    "\n",
    "        # Run the LSTM, we will only use the hidden state, the sequence output is not needed\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # TODO: this can be modified to specify how to reshape our output\n",
    "        # out = out[:, -1, :]\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        # out, _ = torch.mean(out, dim=1)\n",
    "        \n",
    "        # out now have shape (N, hidden_dim * 2)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Transposed CNN\n",
    "    Input with a vector embedding of the text and a noise vector\n",
    "    Output a 128 x 2048 grayscale image\n",
    "    Purpose is to produce an image that is a representation of the text, with the noise vector adding some variation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_dim=96, embedding_dim=512):\n",
    "        \"\"\" \n",
    "        noise_dim: dimension of the noise vector, should be divisible by 6\n",
    "        embedding_dim: dimension of the embedding vector (2 * hidden_dim of the encoder)\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # We are likely going to need to use 5 ResBlockUp layers to get the image to the desired size of 128 x 2048\n",
    "        # We will upscale to this from a 1 x 16 tensor\n",
    "        # 5 ResBlockUp also mean our noise vector will be split into 6 parts. \n",
    "        self.noise_dim = noise_dim\n",
    "        self.noise_chunk_size = noise_dim // 6\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = spectral_norm(nn.Linear(self.noise_chunk_size, 256 * 1 * 16))\n",
    "        self.res_block_up1 = ResBlockUp(256, 256, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up2 = ResBlockUp(256, 128, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up3 = ResBlockUp(128, 64, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.self_attention = SelfAttention(64)\n",
    "        self.res_block_up4 = ResBlockUp(64, 32, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.res_block_up5 = ResBlockUp(32, 16, self.embedding_dim + self.noise_chunk_size)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = spectral_norm(nn.Conv2d(16, 1, kernel_size=3, padding=1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, noise, embedding):\n",
    "        \"\"\"\n",
    "        noise: noise vector of shape (N, noise_dim)\n",
    "        embedding: embedding of the text of shape (N, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # split the noise vector into 8 parts\n",
    "        noise_chunks = torch.split(noise, self.noise_chunk_size, dim=1)\n",
    "\n",
    "        # first input to the network is the first noise chunk\n",
    "        x = noise_chunks[0]\n",
    "        # pass the concatenated tensor through the fully connected layer\n",
    "        x = self.fc(x)  # output is (N, 256 * 1 * 16)\n",
    "        # reshape the tensor to have the desired shape\n",
    "        x = x.view(-1, 256, 1, 16)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the ResBlockUp layers\n",
    "        x = self.res_block_up1(x, torch.cat((embedding, noise_chunks[1]), dim=1))  # (N, 256, 2, 32)\n",
    "        x = self.res_block_up2(x, torch.cat((embedding, noise_chunks[2]), dim=1))  # (N, 128, 4, 64)\n",
    "        x = self.res_block_up3(x, torch.cat((embedding, noise_chunks[3]), dim=1))  # (N, 64, 8, 128)\n",
    "        x = self.self_attention(x)  # (N, 64, 8, 128)\n",
    "        x = self.res_block_up4(x, torch.cat((embedding, noise_chunks[4]), dim=1))  # (N, 32, 16, 256)\n",
    "        x = self.res_block_up5(x, torch.cat((embedding, noise_chunks[5]), dim=1))  # (N, 16, 32, 512)\n",
    "        # pass the tensor through the batch norm layer\n",
    "        x = self.batch_norm(x)  # (N, 16, 32, 512)\n",
    "        # pass the tensor through the relu layer\n",
    "        x = self.relu(x)  # (N, 16, 32, 512)\n",
    "        # pass the tensor through the convolution layer\n",
    "        x = self.conv(x)  # (N, 1, 32, 512)\n",
    "        # pass the tensor through the sigmoid\n",
    "        x = self.sigmoid(x)  # (N, 1, 32, 512)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN\n",
    "    Input with a 128 x 2048 grayscale image\n",
    "    Output a probability that the image is real and not generated\n",
    "    Purpose is to determine if the image is real or generated, to encourage the generator to produce realistic images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        noise_dim: dimension of the noise vector, should be divisible by 8\n",
    "        embedding_dim: dimension of the embedding vector\n",
    "\n",
    "        specifications inspired by https://arxiv.org/pdf/1903.00277.pdf\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.res_block_down1 = ResBlockDown(1, 16)\n",
    "        self.res_block_down2 = ResBlockDown(16, 32)\n",
    "        self.self_attention = SelfAttention(32)\n",
    "        self.res_block_down3 = ResBlockDown(32, 64)\n",
    "        self.res_block_down4 = ResBlockDown(64, 128)\n",
    "        self.res_block_down5 = ResBlockDown(128, 256)\n",
    "        self.res_block = ResBlock(256, 256)\n",
    "        self.global_sum_pooling = GlobalSumPooling()\n",
    "        self.fc = spectral_norm(nn.Linear(256, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        image: image tensor of shape (N, 1, 128, 2048)\n",
    "        \"\"\"\n",
    "\n",
    "        # pass the tensor through the ResBlockDown layers\n",
    "        x = self.res_block_down1(image)  # (N, 16, 16, 256)\n",
    "        x = self.res_block_down2(x)   # (N, 32, 8, 128)\n",
    "        x = self.self_attention(x)  # (N, 32, 8, 128)\n",
    "        x = self.res_block_down3(x)  # (N, 64, 4, 64)\n",
    "        x = self.res_block_down4(x)  # (N, 128, 2, 32)\n",
    "        x = self.res_block_down5(x)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the ResBlock layer\n",
    "        x = self.res_block(x)  # (N, 256, 1, 16)\n",
    "        # pass the tensor through the global sum pooling layer\n",
    "        x = self.global_sum_pooling(x)  # (N, 256)\n",
    "        # pass the tensor through the fully connected layer\n",
    "        x = self.fc(x)  # (N, 1)\n",
    "        # pass the tensor through the sigmoid\n",
    "        x = self.sigmoid(x)  # (N, 1)\n",
    "\n",
    "        return x\n",
    "    pass\n",
    "\n",
    "class Recognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN:\n",
    "    Input with a N x 1 x 32 x 512 image\n",
    "    Output a vector representation of the text size N x 73 x (82*2+1)\n",
    "    Purpose is to recognize the text from the image, to encourage the generator to produce images that are representations of the text\n",
    "    \"\"\"\n",
    "    # TODO dont include dropout when validatino \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"recognizer\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4,2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\n",
    "        #self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4,2))\n",
    "        #self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=3, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(256, 73)\n",
    "        self.dense2 = nn.Linear(248, 82)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = self.bn1(self.lrelu(self.maxpool(self.conv1(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn2(self.lrelu(self.conv2(img)))\n",
    "        #print(img.shape)\n",
    "        img = self.bn3(self.lrelu(self.dropout(self.conv3(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn4(self.lrelu(self.dropout(self.conv4(img))))\n",
    "        #print(img.shape)\n",
    "        img = self.bn5(self.lrelu(self.dropout(self.conv5(img))))\n",
    "        # Collapse \n",
    "        img, _ = torch.max(img, dim=2)\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0, 2, 1)\n",
    "        #print(img.shape)\n",
    "        img, _ = self.lstm(img)\n",
    "        #print(img.shape)\n",
    "        img = self.lrelu(self.dense(img))\n",
    "        #print(img.shape)\n",
    "        img = img.permute(0,2,1)\n",
    "        img = self.dense2(img)\n",
    "        #print(img.shape)\n",
    "        #print(img.shape)\n",
    "        return img\n",
    "        # img = torch.stack()\n",
    "        # img = self.dense(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Evaluating Pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "encoder = Encoder()\n",
    "recognizer = Recognizer()\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_21-45-53_bs32_lr1e-05_betas(0, 0.999)\", epoch_number=9)\n",
    "# with torch.no_grad():\n",
    "#     for i in range(10):\n",
    "#         noise = torch.randn(1, 128)\n",
    "#         a = line_transcription_dataset_train[0]\n",
    "#         embedding = encoder(a.unsqueeze(0))\n",
    "#         print(embedding.shape)\n",
    "#         image = generator(noise, embedding)\n",
    "#         plt.imshow(image.squeeze(0).squeeze(0).detach().numpy(), cmap='gray')\n",
    "#         plt.savefig(f'test{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Actual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "FID on train and validation set (all images)\n",
      "0 torch.Size([64, 1, 32, 512])\n",
      "1 torch.Size([64, 1, 32, 512])\n",
      "2 torch.Size([64, 1, 32, 512])\n",
      "3 torch.Size([64, 1, 32, 512])\n",
      "4 torch.Size([64, 1, 32, 512])\n",
      "5 torch.Size([64, 1, 32, 512])\n",
      "6 torch.Size([64, 1, 32, 512])\n",
      "7 torch.Size([64, 1, 32, 512])\n",
      "8 torch.Size([64, 1, 32, 512])\n",
      "9 torch.Size([64, 1, 32, 512])\n",
      "10 torch.Size([64, 1, 32, 512])\n",
      "11 torch.Size([64, 1, 32, 512])\n",
      "12 torch.Size([64, 1, 32, 512])\n",
      "13 torch.Size([64, 1, 32, 512])\n",
      "14 torch.Size([64, 1, 32, 512])\n",
      "15 torch.Size([40, 1, 32, 512])\n",
      "0 torch.Size([64, 1, 32, 512])\n",
      "1 torch.Size([64, 1, 32, 512])\n",
      "2 torch.Size([64, 1, 32, 512])\n",
      "3 torch.Size([8, 1, 32, 512])\n",
      "epoch 0 batch 0\n",
      "equally familiar hoops to mild laughter .\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m recognizer \u001b[39m=\u001b[39m load_model(recognizer, recog_path)\n\u001b[1;32m     10\u001b[0m \u001b[39m# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_gan(encoder_lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, generator_lr\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m, discriminator_lr\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m,\n\u001b[1;32m     14\u001b[0m           num_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, \n\u001b[1;32m     15\u001b[0m           encoder\u001b[39m=\u001b[39mencoder, generator\u001b[39m=\u001b[39mgenerator ,discriminator\u001b[39m=\u001b[39mdiscriminator,\n\u001b[1;32m     16\u001b[0m           train_line_dataset\u001b[39m=\u001b[39mline_dataset_train, val_line_dataset\u001b[39m=\u001b[39mline_dataset_val, \n\u001b[1;32m     17\u001b[0m           recognizer\u001b[39m=\u001b[39mrecognizer, num_generator_updates_per_discriminator_update\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 207\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, encoder, discriminator, recognizer, train_line_dataset, val_line_dataset, batch_size, encoder_lr, generator_lr, discriminator_lr, betas, num_epochs, loss_balancing_alpha, num_generator_updates_per_discriminator_update)\u001b[0m\n\u001b[1;32m    205\u001b[0m fake_images_loss \u001b[39m=\u001b[39m adversarial_loss_function(discriminator_output_for_fake_images, label_for_fake_images)\n\u001b[1;32m    206\u001b[0m discriminator_loss \u001b[39m=\u001b[39m real_images_loss \u001b[39m+\u001b[39m fake_images_loss\n\u001b[0;32m--> 207\u001b[0m discriminator_loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# retain_graph=True because we will use the same discriminator for the generator\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m# Gradient clipping to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m    210\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(discriminator\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/aps360/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aps360/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "encoder = Encoder()\n",
    "recognizer = Recognizer()\n",
    "\n",
    "# Load in the recognizer\n",
    "recog_path = \"./model_recognizer_bs32_lr0.001_epoch499\"\n",
    "recognizer = load_model(recognizer, recog_path)\n",
    "\n",
    "# generator = load_model(generator, \"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)_generator_epoch9.pt\")\n",
    "# generator, encoder, discriminator = load_models_of_same_batch(generator, encoder, discriminator, filename_prefix=\"main_model/model_snapshots/2023-07-13_18-06-43_bs4_lr0.0002_betas(0, 0.999)\", epoch_number=9)\n",
    "\n",
    "train_gan(encoder_lr=0.0001, generator_lr=1e-6, discriminator_lr=1e-6,\n",
    "          num_epochs=50, batch_size=64, \n",
    "          encoder=encoder, generator=generator ,discriminator=discriminator,\n",
    "          train_line_dataset=line_dataset_train, val_line_dataset=line_dataset_val, \n",
    "          recognizer=recognizer, num_generator_updates_per_discriminator_update=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Hyperparameters to Tune\n",
    "- Dimension of text embedding, we can start with 128, 256, or 512 and increase it later on.\n",
    "- Dataset of training. If the model does not converge, it is likely we will have to manually select example images that have similar writing style.\n",
    "- Learning rate\n",
    "- Balancing the effect of recognizer and discriminator\n",
    "\n",
    "- Generator Networks:\n",
    "  - ResNetUp\n",
    "    - Should the bias be False? Or can it be True?\n",
    "      - conv1 probably don't, since it is batch-normalized right after\n",
    "      - but what about conv2?\n",
    "  - Conditional Batch Norm\n",
    "  - Number of filters in each resnet block\n",
    "\n",
    "LSTM hidden layers should increase, hidden size should increase. \n",
    "- because our text is longer. \n",
    "\n",
    "- Discriminator Networks:\n",
    "  - ResNetDown\n",
    "    - Still if bias should be False?\n",
    "    - LeakyReLU slope\n",
    "  - ResNet\n",
    "    - bias?\n",
    "    - leakyReLU slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DEPRECATED] Old Recognizer Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training Functions\"\"\"\n",
    "def load_data(line_dataset, batch_size):\n",
    "    \n",
    "    indices = [i for i in range(len(line_dataset))]\n",
    "    \n",
    "    train_indices = indices[0:round(0.8*len(indices))][:1000]\n",
    "    val_indices = indices[round(0.8*len(indices)):len(indices)][:300]\n",
    "    \n",
    "    train_data = [(line_dataset[i][0].unsqueeze(0), line_dataset[i][1]) for i in train_indices]\n",
    "    val_data = [(line_dataset[j][0].unsqueeze(0), line_dataset[j][1]) for j in val_indices]\n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(val_data, batch_size,\n",
    "                                               num_workers=0, sampler=val_sampler)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    train_loader = DataLoader(train_data, batch_size,\n",
    "                                               num_workers=1, sampler=train_sampler)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_data\n",
    "    \n",
    "\n",
    "def train_recognizer(recognizer, line_dataset, batch_size, learning_rate=2e-5,  num_epochs=30):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    criterion = nn.CTCLoss()\n",
    "    optimizer = torch.optim.Adam(recognizer.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    train_loader, val_loader = load_data(line_dataset, batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "            image, label = data\n",
    "            image = image.type(torch.FloatTensor)\n",
    "            torch.transpose(image, 0, 1)\n",
    "            #torch.transpose(label, 0, 1)\n",
    "            \n",
    "            print(i/len(train_loader))\n",
    "            \n",
    "            embedding = recognizer(image)\n",
    "            loss = criterion(embedding, label.float(), (batch_size, ), (batch_size, ))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the statistics\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        val_loss[epoch] = evaluate_recognizer(recognizer, val_loader, criterion, batch_size)\n",
    "        \n",
    "        print((\"Epoch {}: Train loss: {}\"+\n",
    "          \" Validation loss: {}\").format(\n",
    "                  epoch + 1,\n",
    "                  train_loss[epoch],\n",
    "                  val_loss[epoch]))\n",
    "\n",
    "        model_path = get_model_name(recognizer.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(recognizer.state_dict(), model_path)\n",
    "\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
    "    \n",
    "\n",
    "def evaluate_recognizer(recognizer, val_data, criterion, batch_size):\n",
    "    total_loss = 0.0\n",
    "    for i, data in enumerate(val_data, 0):\n",
    "        print(i/len(val_data))\n",
    "        image, label = data\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        embedding = recognizer(image)\n",
    "        label = label.unsqueeze(0)\n",
    "        loss = criterion(embedding, label, (batch_size,), (batch_size,))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    return loss\n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def plot_training_curve(path):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "\n",
    "    Args:\n",
    "        path: The base path of the csv files produced during training\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    n = len(train_loss) # number of epochs\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnacondaPyCharm3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
