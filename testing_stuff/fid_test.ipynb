{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2 as cv\n",
    "\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, Subset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Grayscale, Resize, InterpolationMode\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch_fidelity\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 11073\n",
      "Valid samples: 7135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_lines(data_root):\n",
    "    \"\"\"\n",
    "    Creates a new `.txt` file `lines_improved.txt` that will be used\n",
    "    for querying\n",
    "    \"\"\"\n",
    "    original_path = os.path.join(data_root, \"lines.txt\")\n",
    "    improved_path = os.path.join(data_root, \"lines_improved.txt\")\n",
    "    fi = open(improved_path, \"w\")\n",
    "\n",
    "    # Some variables for tracking\n",
    "    num_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    # Loop through \"lines.txt\"\n",
    "    with open(original_path, \"r\") as fo:\n",
    "        # TODO replace imag_path with the .pt file later on\n",
    "        headers = [\"image_id\", \"image_path\", \"image_pt_path\", \"graylevel\", \"original_height\", \"original_width\", \"transcription\", \"transcription_len\"]\n",
    "        fi.writelines(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "        # Skip the intro stuff\n",
    "        for line in fo.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Valid lines, not the intro_text\n",
    "            line_items = line.strip().split(\" \")  # `strip()` to remove newlines\n",
    "\n",
    "            # The actua items (we extract the important ones)\n",
    "            image_id = line_items[0]\n",
    "            status = line_items[1]\n",
    "            graylevel = int(line_items[2])\n",
    "            transcription = \" \".join(line_items[8:])  # Some data has whitespace\n",
    "\n",
    "            # Skip error images\n",
    "            if status == \"err\":\n",
    "                continue\n",
    "        \n",
    "            # Alphanumeric + common punctuation regex\n",
    "            # Returns None if no match\n",
    "            # 26 + 26 + 10 + 9 + 1 = 72\n",
    "            # Spaces might be included as well\n",
    "            if re.fullmatch(\"[a-zA-Z0-9.!?'\\\",:;| -]*\", transcription) is None:\n",
    "                continue\n",
    "\n",
    "            # Now we have valid transcription\n",
    "            # We get the `.png` image path\n",
    "            inp = image_id.split(\"-\")  # `inp` stands for image name parts\n",
    "            image_path_head = os.path.join(data_root, \"lines\", inp[0], f\"{inp[0]}-{inp[1]}\")\n",
    "            image_path_tail = f\"{image_id}.png\"\n",
    "            image_path = os.path.join(image_path_head, image_path_tail)\n",
    "            num_samples += 1\n",
    "            tmp_image = read_image(os.path.join(image_path_head, image_path_tail))  # Temporarily reading to get dimensions\n",
    "            _, height, width = tmp_image.shape\n",
    "\n",
    "            # Scaling calculations\n",
    "            # If width * scale >= desired length (>= to be safe)\n",
    "            if width * (32/height) >= 512:\n",
    "                continue\n",
    "\n",
    "            resized_tensor = process_image_tensor(tmp_image, graylevel)\n",
    "            image_pt_path = os.path.join(image_path_head, f\"{image_id}.pt\")\n",
    "            torch.save(resized_tensor, image_pt_path)\n",
    "\n",
    "            # A fully valid image\n",
    "            # Separate by underscores because `transcription` has spaces so we can't split by spaces\n",
    "            fi.writelines(f\"{image_id}\\t{image_path}\\t{image_pt_path}\\t{graylevel}\\t{height}\\t{width}\\t{transcription}\\t{len(transcription)}\\n\")\n",
    "            valid_samples += 1\n",
    "        \n",
    "        fi.close()\n",
    "    \n",
    "    print(\"# samples:\", num_samples)\n",
    "    print(\"Valid samples:\", valid_samples)\n",
    "\n",
    "\n",
    "def process_image_tensor(image_tensor, graylevel):\n",
    "    \"\"\"\n",
    "    Returns a copy of image_tensor, processed (doesn't update the original variable)\n",
    "    \"\"\"\n",
    "\n",
    "    # Grayscale the image - if the image is not already in grayscale\n",
    "    grayscale_transform = Grayscale()\n",
    "    image_tensor = grayscale_transform(image_tensor)\n",
    "\n",
    "    # Resize it\n",
    "    resize_transform = Resize(32, InterpolationMode.NEAREST_EXACT)\n",
    "    image_tensor = resize_transform(image_tensor)\n",
    "\n",
    "    # Threshold it\n",
    "    # Threshold first because threshold was specifically specified for the original\n",
    "    image_tensor = image_tensor >= graylevel\n",
    "\n",
    "    # Add padding\n",
    "    _,_, resized_height = image_tensor.shape\n",
    "    padding_to_add = 512 - resized_height\n",
    "    image_tensor = F.pad(image_tensor, (0, padding_to_add), value=1)\n",
    "\n",
    "    # Convert to uint8\n",
    "    image_tensor = image_tensor.type(torch.float32)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "# Uncomment this if your data isn't processed yet\n",
    "preprocess_lines(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ascii code\n",
    "valid = [\n",
    "    ' ', '!', '\"', \"'\", ',', '-', '.',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', ';', '?', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "# Enumerate from 1 to save space for padding\n",
    "char_to_int = {v: i for i, v in enumerate(valid, 1)}\n",
    "int_to_char = {i: v for i, v in enumerate(valid, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines_improved_dir, ty=None):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            lines_improved_dir: path to the `lines_improved.txt` file\n",
    "            ty: type of the dataset \"txt\", \"img\" for text dataset or image dataset.\n",
    "        \"\"\"\n",
    "        self.lines_df = pd.read_csv(lines_improved_dir, sep=\"\\t\", quoting=csv.QUOTE_NONE)  # List containing the stuff in `lines.txt`\n",
    "        length = self.lines_df.shape[0]\n",
    "        self.max_transcription_len = max(self.lines_df[\"transcription_len\"])\n",
    "        self.ty = ty\n",
    "        line_datas = self.lines_df.iloc\n",
    "        ret_texts = [line_datas[i]['transcription'].replace('|', ' ') for i in range(length)]\n",
    "        ret_ctois = [torch.tensor([char_to_int[char] for char in ret_texts[i]]) for i in range(length)]\n",
    "        self.ret_ctoi_paddeds = [F.pad(ret_ctois[i], pad=(0, self.max_transcription_len-len(ret_ctois[i]))) for i in range(length)]\n",
    "        self.ret_images = [torch.load(line_datas[i][\"image_pt_path\"]) for i in range(length)]\n",
    "    def __len__(self):\n",
    "        return len(self.lines_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # line_data = self.lines_df.iloc[index]\n",
    "\n",
    "        # ret_text = line_data['transcription'].replace('|', ' ')\n",
    "        # # Get the numerical mapping\n",
    "        # ret_ctoi = torch.tensor([char_to_int[char] for char in ret_text])\n",
    "        # # Padding to the left amount to make it reach `max_transcription_len`\n",
    "        # # TODO possibility to remove padding in the future and let a dataloader handle it\n",
    "        # ret_ctoi_padded = F.pad(ret_ctoi, pad=(0, self.max_transcription_len-len(ret_ctoi)))\n",
    "        # ret_image = torch.load(line_data[\"image_pt_path\"])\n",
    "\n",
    "        # Different type of individual loaders\n",
    "        if self.ty == \"txt\":\n",
    "            return self.ret_ctoi_paddeds[index]\n",
    "        elif self.ty == \"img\":\n",
    "            return self.ret_images[index]\n",
    "        else:\n",
    "            return self.ret_images[index], self.ret_ctoi_paddeds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "100 20\n",
      "images\n",
      "1000 20\n",
      "both\n",
      "5708 1427\n"
     ]
    }
   ],
   "source": [
    "line_transcription_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"txt\")\n",
    "line_image_dataset = LineDataset(\"./data/lines_improved.txt\", ty=\"img\")\n",
    "line_dataset = LineDataset(\"./data/lines_improved.txt\")\n",
    "\n",
    "# Don't change this, we want to maintain consistent split\n",
    "torch.manual_seed(12345678)  # DO NOT REMOVE THIS LINE\n",
    "line_transcription_dataset_train, line_transcription_dataset_val = random_split(line_transcription_dataset, [0.8, 0.2])\n",
    "line_image_dataset_train, line_image_dataset_val = random_split(line_image_dataset, [0.8, 0.2])\n",
    "line_dataset_train, line_dataset_val = random_split(line_dataset, [0.8, 0.2])\n",
    "\n",
    "# To train on a small dataset\n",
    "line_transcription_dataset_train = Subset(line_transcription_dataset_train, range(100))\n",
    "line_transcription_dataset_val = Subset(line_transcription_dataset_val, range(20))\n",
    "\n",
    "line_image_dataset_train = Subset(line_image_dataset_train, range(1000))\n",
    "line_image_dataset_val = Subset(line_image_dataset_val, range(20))\n",
    "\n",
    "# line_transcription_dataset_train, line_transcription_dataset_val, _ = random_split(line_transcription_dataset, [0.005, 0.005, 0.99])\n",
    "# line_image_dataset_train, line_image_dataset_val, _ = random_split(line_image_dataset, [0.005, 0.005, 0.99])\n",
    "# line_dataset_train, line_dataset_val = random_split(line_dataset, [0.0025, 0.9975])\n",
    "\n",
    "print(\"lines\")\n",
    "print(len(line_transcription_dataset_train), len(line_transcription_dataset_val))\n",
    "print(\"images\")\n",
    "print(len(line_image_dataset_train), len(line_image_dataset_val))\n",
    "print(\"both\")\n",
    "print(len(line_dataset_train), len(line_dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(line_image_dataset_train)\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "itransform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe30cff0640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "img = itransform(next(a).squeeze(0))\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fid = FrechetInceptionDistance(feature=2048)\n",
    "fid.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_image_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "torch.Size([20, 1, 32, 512])\n",
      "tensor(157.6595, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loader1 = DataLoader(line_image_dataset_train, batch_size=20, shuffle=True)\n",
    "test_loader2 = DataLoader(line_image_dataset_val, batch_size=20, shuffle=True)\n",
    "# ii = next(iter(test_loader2))\n",
    "ii = torch.load(\"./h\")\n",
    "# print(type(ii))\n",
    "# print(ii[0])\n",
    "a = 0\n",
    "# print(ii.shape)\n",
    "for iii, i in enumerate(test_loader1, start=1):\n",
    "    # print(torch.isinf((i.cpu()*255).to(torch.uint8).repeat(1, 3, 1, 1)).any())\n",
    "    print(i.shape)\n",
    "    fid.update((i*255).to(torch.uint8).repeat(1, 3, 1, 1).to(device), real=True)\n",
    "    # fid.update((ii.cpu()*255).to(torch.uint8).repeat(1, 3, 1, 1).to(device), real=False)\n",
    "    fid.update((ii).to(torch.uint8).repeat(1, 3, 1, 1).to(device), real=False)\n",
    "        \n",
    "    f = fid.compute()\n",
    "    a += f\n",
    "    fid.reset()\n",
    "print(a/iii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps360_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
